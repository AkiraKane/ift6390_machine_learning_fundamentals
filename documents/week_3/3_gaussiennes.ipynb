{
 "metadata": {
  "name": "",
  "signature": "sha256:dac4ab0c3bdba4d5b863dc8b06b1e4abd6d347f7ee18112feda34c5df286f006"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gaussiennes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aujourd'hui, nous allons tenter de repr\u00e9senter la distribution des donn\u00e9es de l'ensemble `iris` avec des gaussiennes. \n",
      "\n",
      "Les param\u00e8tres $\\mu$ et $\\sigma^2$ des gaussiennes seront estim\u00e9s en maximisant leur log-vraisemblance suivant l'\u00e9quation:\n",
      "\n",
      "$$\\log\\prod_i^n p(X=x_i)$$\n",
      "\n",
      "Voici le [d\u00e9tail du calcul](https://studium.umontreal.ca/mod/resource/view.php?id=416973) dans le premier cas qui nous int\u00e9resse, soit une gaussienne isotropique.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Impl\u00e9mentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous allons commencer par impl\u00e9menter une gaussienne isotropique, puis ensuite diagonale et finalement avec matrice de covariance. Vous serez \u00e0 m\u00eame de constater la diff\u00e9rence de capacit\u00e9 d'une gaussienne \u00e0 l'autre sur l'ensemble `iris`.\n",
      "\n",
      "Bref, voici les deux \u00e9quations qu'il faut retenir pour l'impl\u00e9mentation des param\u00e8tres de la gaussienne isotropique\n",
      "$$\\vec{\\mu} = \\frac{1}{n}\\sum_{i=0}^n \\vec{x_{i}}$$\n",
      "$$\\sigma^2 = \\frac{1}{nd}\\sum_{i=0}^n \\sum_{j=0}^d (x_{i,j}-\\mu_j)^2$$\n",
      "\n",
      "La log probabilit\u00e9 se calcule suivant:\n",
      "\n",
      "$$\\log p(x) = \\left(- \\frac{d}{2}\\log 2\\pi - d\\log\\sigma\\right) - \\left(\\frac{1}{2\\sigma^2}\\sum_{i=0}^n(x_i-\\mu)^2\\right)$$\n",
      "\n",
      "Prenez le temps de bien penser comment impl\u00e9menter ces \u00e9quations avec `numpy`. Les fonctions `sum`, `log` et `prod` vous seront utiles. Vous devez aussi t\u00e9l\u00e9charger le fichier [utilitaires3.py](http://www.iro.umontreal.ca/~memisevr/teaching/ift3395_2014/demos/utilitaires3.py) et le d\u00e9poser dans le m\u00eame dossier que vos fichier `ipython` (`.ipynb`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ce code ne fait que charger ce qui sera utile pour le reste de la d\u00e9mo\n",
      "%pylab inline\n",
      "import numpy as np\n",
      "import utilitaires3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le squelette des fonctions a d\u00e9j\u00e0 \u00e9t\u00e9 impl\u00e9ment\u00e9 pour vous ici. Il ne vous reste qu'\u00e0 impl\u00e9menter le calcul de $\\mu$ et $\\sigma^2$ puis du `d\u00e9nominateur` et finalement la log probabilit\u00e9 (`log_prob`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gaussienne_isotropique(train_data):\n",
      "    # calculez la moyenne\n",
      "    mu = ?\n",
      "    # calculez la variance\n",
      "    sigma_sq = ?\n",
      "    \n",
      "    return mu, sigma_sq\n",
      "    \n",
      "def iso_log_probabilite(mu, sigma_sq, test_data):\n",
      "    # calculez la log probabilit\u00e9\n",
      "    denominateur = ?\n",
      "    \n",
      "    log_prob = denominateur - ?\n",
      "    \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le code suivant va utilser les deux fonctions que vous avez compl\u00e9t\u00e9es et afficher la distribution apprises sur l'ensemble `iris`. Vous pouvez changer la classe en changeant la valeur de `train_class` pour 1,2 ou 3. \n",
      "\n",
      "Est-ce que la distribution de la gaussienne vous semble bien repr\u00e9senter la distribution des donn\u00e9es?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# On fourni ici un exemple ou on ne decoupe pas les donnes en ensemble de train et test\n",
      "# a vous de le faire comme dans la demo 3\n",
      "iris=np.loadtxt('iris.txt')\n",
      "np.random.seed(3395)\n",
      "train_cols = [0,1]\n",
      "train_class = 1\n",
      "\n",
      "\n",
      "# selection seulement la classe train_class\n",
      "iris = iris[iris[:,-1]==train_class]\n",
      "n_train = int(iris.shape[0]*0.90)\n",
      "\n",
      "inds = range(iris.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "\n",
      "train_data = iris[train_inds]\n",
      "test_data = iris[test_inds]\n",
      "train_data = train_data[:, train_cols]\n",
      "test_data = test_data[:, train_cols]\n",
      "\n",
      "mu, sigma_sq = gaussienne_isotropique(train_data)\n",
      "\n",
      "print \"gaussienne isotropique\"\n",
      "utilitaires3.distribution(train_data, test_data, lambda data: iso_log_probabilite(mu, sigma_sq, data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Au cas o\u00f9 vous ne l'aviez pas devin\u00e9, la r\u00e9ponse \u00e9tait non! C'est pourquoi nous allons passer \u00e0 l'impl\u00e9mentation d'une gaussienne diagonale. Le calcul de la moyenne reste la m\u00eame, mais la variance se calcule maintenant selon l'\u00e9quation suivante:\n",
      "\n",
      "$$\\vec{\\sigma}^2 = \\frac{1}{n}\\sum_{i=0}^n (\\vec{x_i}-\\vec{\\mu})^2$$\n",
      "\n",
      "La log probabilit\u00e9 se calcule suivant\n",
      "\n",
      "$$\\log p(x) = \\left(- \\frac{d}{2}\\log 2\\pi - \\log(\\prod_i\\vec{\\sigma_i})\\right) - \\left(\\frac{1}{2\\vec{\\sigma}^2}\\sum_{i=0}^n(x_i-\\mu)^2\\right)$$\n",
      "\n",
      "\u00c0 noter que $\\vec{\\sigma}^2$ est maintenant un vecteur. Il ne vous reste qu'\u00e0 completer les deux fonctions suivantes comme vous venez de le faire pr\u00e9c\u00e9demment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gaussienne_diagonale(train_data):\n",
      "    # calculez la moyenne\n",
      "    mu = np.mean(train_data, axis=0)\n",
      "    # calculez la variance\n",
      "    sigma_sq = ?\n",
      "\n",
      "    return mu, sigma_sq\n",
      "    \n",
      "def diag_log_probabilite(mu, sigma_sq, test_data):\n",
      "    # calculez la log probabilite\n",
      "    denominateur = ?\n",
      "    \n",
      "    log_prob = denominateur - ?\n",
      "    \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vous pouvez maintenant comparer les r\u00e9sultats des gaussiennes isotropiques et diagonales. N'oubliez pas de changer la classe pour voir diff\u00e9rents exemples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# On fourni ici un exemple ou on ne decoupe pas les donnes en ensemble de train et test\n",
      "# a vous de le faire comme dans la demo 3\n",
      "iris=np.loadtxt('iris.txt')\n",
      "np.random.seed(3395)\n",
      "train_cols = [0,1]\n",
      "train_class = 1\n",
      "\n",
      "# selection seulement la classe train_class\n",
      "iris = iris[iris[:,-1]==train_class]\n",
      "n_train = int(iris.shape[0]*0.90)\n",
      "\n",
      "inds = range(iris.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "\n",
      "train_data = iris[train_inds]\n",
      "test_data = iris[test_inds]\n",
      "train_data = train_data[:, train_cols]\n",
      "test_data = test_data[:, train_cols]\n",
      "\n",
      "print \"gaussienne isotropique\"\n",
      "mu, sigma_sq = gaussienne_isotropique(train_data)\n",
      "utilitaires3.distribution(train_data, test_data, lambda data: iso_log_probabilite(mu, sigma_sq, data))\n",
      "print \"gaussienne diagonale\"\n",
      "mu, sigma_sq = gaussienne_diagonale(train_data)\n",
      "utilitaires3.distribution(train_data, test_data, lambda data: diag_log_probabilite(mu, sigma_sq, data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Il y a encore place \u00e0 l'am\u00e9lioration, c'est pourquoi nous allons passer \u00e0 une gaussienne avec matrice de covariance. La totale quoi. :)\n",
      "\n",
      "La matrice de covariance se calcule selon l'\u00e9quation suivante\n",
      "\n",
      "$$\\Sigma = \\frac{1}{n}\\sum_{i=0}^n (\\vec{x_i}-\\vec{\\mu})(\\vec{x_i}-\\vec{\\mu})^T$$\n",
      "\n",
      "La log probabilit\u00e9 se calcule suivant\n",
      "\n",
      "$$\\log p(x) = \\left(- \\frac{d}{2}\\log 2\\pi - \\log|\\Sigma|\\right) - \\left(\\frac{1}{2}(x-\\mu)^T\\Sigma(x-\\mu)\\right)$$\n",
      "\n",
      "L'impl\u00e9mentation se corse un peu pour la log probabilit\u00e9 ici. Vous aurez besoin des functions `np.linalg.det` et `np.linalg.inv`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gaussienne_complete(train_data):\n",
      "    # calculer la moyenne\n",
      "    mu = np.mean(train_data, axis=0)\n",
      "    # calculer la variance\n",
      "    cov = ?\n",
      "    return mu, cov\n",
      "\n",
      "def comp_log_probabilite(mu, cov, test_data):\n",
      "    # n'essayez surtout pas de tout \u00e9crire sur seulement deux lignes!\n",
      "    c = ?\n",
      "            \n",
      "    log_prob = c - ?\n",
      "    \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Et finalement comparez les trois types de gaussiennes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# On fourni ici un exemple ou on ne decoupe pas les donnes en ensemble de train et test\n",
      "# a vous de le faire comme dans la demo 3\n",
      "iris=np.loadtxt('iris.txt')\n",
      "np.random.seed(3395)\n",
      "train_cols = [0,1]\n",
      "train_class = 1\n",
      "\n",
      "# selection seulement la classe train_class\n",
      "iris = iris[iris[:,-1]==train_class]\n",
      "n_train = int(iris.shape[0]*0.90)\n",
      "\n",
      "inds = range(iris.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "\n",
      "train_data = iris[train_inds]\n",
      "test_data = iris[test_inds]\n",
      "train_data = train_data[:, train_cols]\n",
      "test_data = test_data[:, train_cols]\n",
      "\n",
      "print \"gaussienne isotropique\"\n",
      "mu, sigma_sq = gaussienne_isotropique(train_data)\n",
      "utilitaires3.distribution(train_data, test_data, lambda data: iso_log_probabilite(mu, sigma_sq, data))\n",
      "print \"gaussienne diagonale\"\n",
      "mu, sigma_sq = gaussienne_diagonale(train_data)\n",
      "utilitaires3.distribution(train_data, test_data, lambda data: diag_log_probabilite(mu, sigma_sq, data))\n",
      "print \"gaussienne avec matrice de covariance\"\n",
      "mu, sigma_sq = gaussienne_complete(train_data)\n",
      "utilitaires3.distribution(train_data, test_data, lambda data: comp_log_probabilite(mu, sigma_sq, data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifieur de Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maintenant, on va construire un **classifieur multiclasse de Bayes**. \u00c7a veut dire qu'au lieu de mod\u00e9liser $p(\\mbox{classe}|\\mbox{exemple})$ (ou $p(y|x)$), on va plut\u00f4t utiliser l'\u00e9quation de Bayes \n",
      "\n",
      "$$p(\\mbox{classe}|\\mbox{exemple}) = \\frac{p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe})}{p(\\mbox{exemple})}$$\n",
      "\n",
      "et mod\u00e9liser les diff\u00e9rents morceaux. En fait, on a juste besoin de mod\u00e9liser $p(\\mbox{exemple}|\\mbox{classe})$ et $p(\\mbox{classe})$ (puisque $p(\\mbox{exemple})$ est \u00e9gal quel que soit la classe ($\\frac{1}{n}$)). \n",
      "\n",
      "Le terme $p(\\mbox{classe})$ repr\u00e9sente la probabilit\u00e9 \u00e0 priori d'une classe, c'est-\u00e0-dire notre croyance \u00e0 priori - avant d'avoir vu un exemple en particulier - sur la probabilit\u00e9 qu'un exemple inconnu appartienne \u00e0 cette classe). On va repr\u00e9senter cette croyance \u00e0 priori pour une classe par la fr\u00e9quence de cette derni\u00e8re dans les donn\u00e9es d'entra\u00eenement: $\\frac{n_c}{n}$ o\u00f9 $n_c$ = nombre d'exemple de la classe $c$, puis $n$ = nombre d'exemple d'entra\u00eenement. \n",
      "\n",
      "On va utiliser des **densit\u00e9s Gaussiennes multivari\u00e9es** pour mod\u00e9liser les diff\u00e9rents $p(\\mbox{exemple}|\\mbox{classe})$. Cela veut dire que pour chaque classe, on va supposer que la \"vrai\" distribution $p(\\mbox{exemple}|\\mbox{classe})$ poss\u00e8de la forme d'une Gaussienne multivari\u00e9e dont on va tenter d'apprendre les param\u00e8tres $\\mu$ et $\\Sigma$. En pratique, on va se limiter aujourd'hui \u00e0 un cas particulier de cette distribution: celui o\u00f9 l'on suppose que la matrice de covariance $\\Sigma$ de chaque Gaussienne est diagonale et que chaque \u00e9l\u00e9ment de cette diagonale est le m\u00eame, soit sigma_sq ( <=> \"sigma square\" <=> $\\sigma^2$ <=> la variance). On poss\u00e8de donc un seul param\u00e8tre pour contr\u00f4ler la forme de la covariance. C'est plus simple (pour nous et pour l'ordinateur) \u00e0 calculer, mais \u00e7a signifie aussi que notre mod\u00e8le est moins puissant. \n",
      "\n",
      "On a donc un mod\u00e8le param\u00e9trique tr\u00e8s simple. Les param\u00e8tres sont la moyenne $\\mu$ (un vecteur de dimension celle de l'entr\u00e9e du syst\u00e8me) et la variance $\\sigma^2$ (un seul scalaire dans notre mod\u00e8le simple, qui va multiplier la matrice identit\u00e9). L'apprentissage dans ce mod\u00e8le se fera aujourd'hui par l'application du **principe de maximum de vraisemblance** comme vue pr\u00e9demment. Pour chaque classe, on va trouver les valeurs des param\u00e8tres qui maximisent la log-vraisemblance des donn\u00e9es d'entra\u00eenement issus de cette classe.\n",
      "\n",
      "Ayant trouv\u00e9 les param\u00e8tres qui maximisent la vraisemblance pour chacune des classes, il nous est possible de calculer tous les $p(\\mbox{exemple}|\\mbox{classe})$. Il suffit maintenant d'appliquer la r\u00e8gle de Bayes afin de pouvoir classifier un nouvel exemple. Plus pr\u00e9cis\u00e9ment, on voudra choisir, pour un exemple, la classe qui maximise $p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe})$ ou encore $log(p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe}))$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vous avez ici une classe repr\u00e9sentant un mod\u00e8le d'apprentissage. Si vous n'\u00eates pas familier avec les classes en g\u00e9n\u00e9ral ou avec python, lisez le tutoriel officiel [fran\u00e7ais](http://www.afpy.org/doc/python/2.7/tutorial/classes.html) [anglais](https://docs.python.org/2/tutorial/classes.html). Prenez l'habitude, les mod\u00e8les que nous impl\u00e9menterons auront toujours les m\u00e9thodes __init__, train et compute_predictions comme c'est le cas ici."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class gauss_mv:\n",
      "\n",
      "    def __init__(self,n_dims,cov_type):\n",
      "        self.n_dims = n_dims\n",
      "        self.mu = np.zeros((1,n_dims))\n",
      "        self.cov_type = cov_type\n",
      "        if cov_type==\"isotropique\":\n",
      "            self.sigma_sq = 1.0\n",
      "        elif cov_type==\"diagonale\":\n",
      "            self.sigma_sq = numpy.ones(n_dims)\n",
      "        elif cov_type==\"full\":\n",
      "            self.cov = np.ones((n_dims,n_dims))\n",
      "\n",
      "    # Pour un ensemble d'entrainement, la fonction devrait calculer l'estimateur par MV de la moyenne et de la matrice de covariance\n",
      "    def train(self, train_data):\n",
      "        \n",
      "        if self.cov_type == \"isotropique\":\n",
      "            mu, sigma_sq = gaussienne_isotropique(train_data)\n",
      "        elif self.cov_type == \"diagonale\":\n",
      "            mu, sigma_sq = gaussienne_diagonale(train_data)\n",
      "        elif self.cov_type == \"full\":\n",
      "            mu, sigma_sq = gaussienne_complete(train_data)\n",
      "            \n",
      "        self.mu = mu\n",
      "        self.sigma_sq = sigma_sq\n",
      "        pass\n",
      "    \n",
      "    # Retourne un vecteur de taille nb. ex. de test contenant les log\n",
      "    # probabilit\u00e9s de chaque exemple de test sous le mod\u00e8le.    \n",
      "    def compute_predictions(self, test_data):\n",
      "\n",
      "        if self.cov_type == \"isotropique\":\n",
      "            log_prob = iso_log_probabilite(self.mu, self.sigma_sq, test_data)\n",
      "        elif self.cov_type == \"diagonale\":\n",
      "            log_prob = diag_log_probabilite(self.mu, self.sigma_sq, test_data)\n",
      "        elif self.cov_type == \"full\":\n",
      "            log_prob = comp_log_probabilite(self.mu, self.sigma_sq, test_data)\n",
      "            \n",
      "        return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Presque tout est fait \u00e0 votre place. Maintenant, impl\u00e9mentez la m\u00e9thode compute_predictions du classifieur de bayes. Avec la classe d\u00e9fini pr\u00e9c\u00e9demment, \u00e7a ne devrait pas \u00eatre compliqu\u00e9. Si vous ne comprenez pas, posez des questions!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pour faire taire les underflow\n",
      "np.seterr(under='ignore')\n",
      "\n",
      "class classif_bayes:\n",
      "\n",
      "    def __init__(self,modeles_mv, priors):\n",
      "        self.modeles_mv = modeles_mv\n",
      "        self.priors = priors\n",
      "        if len(self.modeles_mv) != len(self.priors):\n",
      "            print 'Le nombre de modeles MV doit etre egale au nombre de priors!'\n",
      "        \n",
      "        self.n_classes = len(self.modeles_mv)\n",
      "                                                            \n",
      "    # Retourne une matrice de taille nb. ex. de test x nombre de classes contenant les log\n",
      "    # probabilit\u00e9s de chaque exemple de test sous chaque mod\u00e8le MV. \n",
      "    def compute_predictions(self, test_data, eval_by_group=False):\n",
      "        \n",
      "        # \u00e0 completer\n",
      "\n",
      "        return log_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La cellule suivante vous permettra de tester votre m\u00e9thodes compute_predictions. Vous devez cependant construire le mod\u00e8le de classification bay\u00e9sien. Des indices trainent un peu partout pour vous aider. Une fois termin\u00e9, comparez les classifeurs de bayes avec trois types diff\u00e9rents de gaussiennes. Changez les valeurs de train_cols pour voir comment les classifieurs se comporte. Est-ce que les gaussiennes avec matrice de covariance donnent toujours de meilleur r\u00e9sultats? Pourquoi? Essayez notamment les valeurs `train_cols = [2,3]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris=np.loadtxt('iris.txt')\n",
      "train_cols = [2,3]\n",
      "target_cols = [-1]\n",
      "iris_train1=iris[0:50,train_cols]\n",
      "iris_train2=iris[50:100,train_cols]\n",
      "iris_train3=iris[100:150,train_cols]\n",
      "\n",
      "def experiment(cov_type):\n",
      "    # On cree un modele par classe (par maximum de vraissemblance)\n",
      "    \n",
      "    # Cr\u00e9ez les mod\u00e8les ici!\n",
      "    \n",
      "    # On cree une liste de tous nos modeles\n",
      "    # On fait la meme chose pour les priors\n",
      "    # Les priors sont calcules ici de facon exact car on connait le nombre \n",
      "    # de representants par classes. Un fois que vous aurez cree un\n",
      "    # ensemble de train/test, il va faloir les calculer de facon exacte\n",
      "    modele_mv=?\n",
      "    priors=[0.3333,0.3333,0.3333]\n",
      "\n",
      "    # On cree notre classifieur avec notre liste de modeles gaussien et nos priors\n",
      "    classifieur=?\n",
      "\n",
      "    #on peut maintenant calculer les logs-probabilites selon nos modeles\n",
      "    log_prob=?\n",
      "\n",
      "    # il reste maintenant a calculer le maximum par classe pour la classification\n",
      "    classesPred = ?\n",
      "\n",
      "    utilitaires3.teste(classesPred,iris[:,-1])\n",
      "\n",
      "    print \"Taux d'erreur %.2f%%\" % ((1-(classesPred==iris[:,-1]).mean())*100.0)\n",
      "    idx = range(0,iris.shape[0])\n",
      "    np.random.shuffle(idx)\n",
      "    train_set = iris[idx[:100]]\n",
      "    test_set = iris[idx[100:]]\n",
      "    train_set = train_set[:,train_cols+target_cols]\n",
      "    test_set = test_set[:,train_cols+target_cols]\n",
      "    utilitaires3.gridplot(classifieur,train_set,test_set,n_points=50)\n",
      "    \n",
      "for cov_type in [\"isotropique\", \"diagonale\", \"full\"]:\n",
      "    print \"gaussienne\", cov_type\n",
      "    experiment(cov_type)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
