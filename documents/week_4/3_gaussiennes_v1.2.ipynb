{
 "metadata": {
  "name": "",
  "signature": "sha256:1a13590f91b940ed4fddb4a269e921bfdaa9855e61226a25d235bee0bc1c0845"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gaussiennes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aujourd'hui, nous allons tenter de repr\u00e9senter la distribution des donn\u00e9es de l'ensemble `iris` avec des gaussiennes. \n",
      "\n",
      "Les param\u00e8tres $\\mu$ et $\\sigma^2$ des gaussiennes seront estim\u00e9s en maximisant leur log-vraisemblance suivant l'\u00e9quation:\n",
      "\n",
      "$$\\log\\prod_i^n p(X=x_i)$$\n",
      "\n",
      "Voici le [d\u00e9tail du calcul](http://www.iro.umontreal.ca/~memisevr/teaching/ift3395_2014/demos/maxvraisemblancegaussienne.pdf) dans le premier cas qui nous int\u00e9resse, soit une gaussienne isotropique.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Petit rappel Numpy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vous vous \u00eates un peu familiaris\u00e9 avec numpy \u00e0 la derni\u00e8re d\u00e9mo. Nous allons voir rapidement comment impl\u00e9menter des \u00e9quations de mani\u00e8re efficace avec numpy. Commencez par ex\u00e9cuter le code suivant qui s'occupera d'importer les libraires n\u00e9cessaire pour tout le reste de la d\u00e9mo. Vous devez t\u00e9l\u00e9charger le fichier [utilitaires3.py](http://www.iro.umontreal.ca/~memisevr/teaching/ift3395_2014/demos/utilitaires3.py) et le d\u00e9poser dans le m\u00eame dossier que vos fichier `ipython` (`.ipynb`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ce code ne fait que charger ce qui sera utile pour le reste de la d\u00e9mo\n",
      "%pylab inline\n",
      "import numpy as np\n",
      "import utilitaires3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Commen\u00e7ons par la m\u00e9thode `sum` en la testant sur une matrice remplie de 1 et de dimension (5,2)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.ones((5,2))\n",
      "\n",
      "print \"np.sum(X)\"\n",
      "print np.sum(X), \"\\n\"\n",
      "\n",
      "print \"np.sum(X, axis=0)\"\n",
      "print np.sum(X, axis=0), \"\\n\"\n",
      "\n",
      "print \"np.sum(X, axis=1)\"\n",
      "print np.sum(X, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comme vous le voyez, la fonction somme sur toute la matrice ou seulement selon un axe d\u00e9fini. Les appels pr\u00e9c\u00e9dents correspondent aux \u00e9quations suivantes:\n",
      "\n",
      "- `np.sum(X)` === $\\displaystyle \\sum_{i=1}^5\\sum_{j=1}^2 X_{i,j}$\n",
      "- `np.sum(X, axis=0)` === $\\displaystyle \\sum_{i=1}^5 X_{i}$\n",
      "- `np.sum(X, axis=1)` === $\\displaystyle \\sum_{j=1}^2 X_{j}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rien ne nous emp\u00eache de remplacer la matrice $X$ par un r\u00e9sultat interm\u00e9diare. On peut calculer une somme du genre $\\displaystyle \\sum_{i=1}^n X_i + Y_i$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.ones((5,2))\n",
      "\n",
      "Y = np.ones((5,2))\n",
      "\n",
      "print \"np.sum(X+Y, axis=0)\"\n",
      "print np.sum(X+Y, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Faites bien attention dans vos calcul \u00e0 ne pas faire de divisions enti\u00e8res! Si vous \u00e9crivez `1/10`, vous obtiendrez 0! Prenez toujours bien soins de mettre tout vos nombres en float, il suffit d'ajouter un point avec le nombre."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"1/10 =\", 1/10, \"\\n\"\n",
      "\n",
      "print \"1./10. =\", 1./10."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cela devient particuli\u00e8rement risqu\u00e9 si vous devez diviser 1 par la dimension d'un `array numpy`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.ones((5,2))\n",
      "\n",
      "print \"1/n =\", 1/X.shape[0], \"\\n\"\n",
      "\n",
      "print \"1./n =\", 1./X.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Impl\u00e9mentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous allons commencer par impl\u00e9menter une gaussienne isotropique, puis ensuite diagonale et finalement avec matrice de covariance. Vous serez \u00e0 m\u00eame de constater la diff\u00e9rence de capacit\u00e9 d'une gaussienne \u00e0 l'autre sur l'ensemble `iris`.\n",
      "\n",
      "Bref, voici les deux \u00e9quations qu'il faut retenir pour l'impl\u00e9mentation des param\u00e8tres de la gaussienne isotropique\n",
      "$$\\vec{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_{i}$$\n",
      "$$\\sigma^2 = \\frac{1}{nd}\\sum_{i=1}^n \\sum_{j=1}^d (X_{i,j}-\\vec{\\mu}_j)^2$$\n",
      "\n",
      "O\u00f9 $d$ est le nombre de dimensions et $n$ est le nombre d'exemples. $\\vec{\\mu}$ est un vecteur de dimension $d$ et $X$ est une matrice de dimension $(n,d)$. $X_i$ est donc un vecteur de dimension $d$. $\\sigma^2$ est bien un scalaire dans le cas isotropique. \n",
      "\n",
      "La log probabilit\u00e9 se calcule suivant:\n",
      "\n",
      "$$\\log p(X_i) = \\left(- \\frac{d}{2}\\log 2\\pi - d\\log\\sigma\\right) - \\left(\\frac{1}{2\\sigma^2}\\sum_{j=1}^d(X_{i,j}-\\vec{\\mu}_j)^2\\right)$$\n",
      "\n",
      "Notez qu'on calcule $\\log p(X_i)$ et non pas $\\log p(X)$ directement. Le r\u00e9sultat de $\\log p(X_i)$ est un scalaire alors que celui de $\\log p(X)$ est un vecteur de dimension $n$, soit, le nombre d'exemples (et donc de rang\u00e9es) dans la matrice $X$.\n",
      "\n",
      "Prenez le temps de bien penser comment impl\u00e9menter ces \u00e9quations avec `numpy`. Les fonctions `sum`, `log` et `prod` vous seront utiles. **N'utilisez surtout pas les fonctions `mean`, `std` et `cov` sans quoi cette d\u00e9mo ne sera pas utile pour vous. Le but est d'apprendre \u00e0 manipuler les fonctions de bases de `numpy`!** Vous devez aussi t\u00e9l\u00e9charger le fichier **[utilitaires3.py](http://www.iro.umontreal.ca/~memisevr/teaching/ift3395_2014/demos/utilitaires3.py)** et le d\u00e9poser dans le m\u00eame dossier que vos fichier `ipython` (`.ipynb`). **Veillez \u00e0 bien t\u00e9l\u00e9charger une version r\u00e9cente du fichier utilitaires3.py**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le squelette des fonctions a d\u00e9j\u00e0 \u00e9t\u00e9 impl\u00e9ment\u00e9 pour vous ici. Il ne vous reste qu'\u00e0 impl\u00e9menter le calcul de $\\mu$ et $\\sigma^2$. Testez votre impl\u00e9mentation en ex\u00e9cutant la cellule suivante."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gaussienne_isotropique(train_data):\n",
      "    \"\"\"\n",
      "    Calcule la moyenne mu et la variance sigma_sq\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    train_data : array\n",
      "        Matrice de dimension (n,d) o\u00f9 n est le nombre d'exemple et d le nombre de dimension.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : float\n",
      "        La variance, un scalaire.\n",
      "    \"\"\"\n",
      "    \n",
      "    # calcule la moyenne\n",
      "    mu = None # \u00e0 completer\n",
      "    \n",
      "    # calcule la variance\n",
      "    sigma_sq = None # \u00e0 completer\n",
      "    \n",
      "    return mu, sigma_sq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Les messages \"`moyenne bonne`\" et \"`variance bonne`\" devraient s'afficher si tout est bon. Sinon, regardez les r\u00e9sultats et essayez de comprendre d'o\u00f9 vient le probl\u00e8me. Laissez la valeur None \u00e0 sigma_sq pendant que vous testez mu, vous recevrez simplement le message \"`variance pas encore impl\u00e9ment\u00e9 (valeur re\u00e7u=None)`\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.normal(size=(5,2))\n",
      "\n",
      "mu, sigma_sq = gaussienne_isotropique(X)\n",
      "utilitaires3.compare(mu, X.mean(0), \"moyenne\")\n",
      "utilitaires3.compare(sigma_sq, (X-mu).std()**2 if mu is not None else 0, \"variance\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Avant d'impl\u00e9menter $\\log p(X)$, on va s'attaquer \u00e0 $\\log p(X_i)$ et regarder le temps de calcul avec une boucle. Completez la fonction p et ex\u00e9cutez la cellule."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def p(mu, sigma_sq, x):\n",
      "    \"\"\"\n",
      "    Calcule la log probabilit\u00e9 d'un seul exemple\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : float\n",
      "        La variance, un scalaire\n",
      "    x : array\n",
      "        vecteur de dimension d\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    log_prob : float\n",
      "        La log probabilit\u00e9 de l'exemple x en entr\u00e9e\n",
      "    \"\"\"\n",
      "    \n",
      "    # calcule le log du d\u00e9nominateur (premi\u00e8re grosse parenth\u00e8se)\n",
      "    denominateur = None # \u00e0 completer\n",
      "    \n",
      "    # calcule la log probabilit\u00e9 totale\n",
      "    log_prob = None # \u00e0 completer\n",
      "    \n",
      "    return log_prob\n",
      "\n",
      "X = np.random.normal(size=(10000,100))\n",
      "\n",
      "mu, sigma_sq = gaussienne_isotropique(X)\n",
      "\n",
      "log_probs = np.zeros(X.shape[0])\n",
      "\n",
      "import time\n",
      "start = time.clock()\n",
      "for i in xrange(X.shape[0]):\n",
      "    log_probs[i] = p(mu, sigma_sq, X[i])\n",
      "stop = time.clock()\n",
      "\n",
      "print \"Le calcul a pris\", stop-start, \"secondes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Il faut maintenant impl\u00e9menter la version qui calcule la log_prob de plusieurs exemple \u00e0 la fois. Comprenez bien que la fonction doit maintenant retourner un vecteur de $n$ log probabilit\u00e9s si on lui donne une matrice `test_data` de dimension $(n, d)$. Aussi, logiquement, le calcul pour chaque exemple est ind\u00e9pendant, il n'y a donc pas de raison qu'un seul calcul se fasse sur l'axe 0. \n",
      "\n",
      "Vous avez la r\u00e9ponse tout cuit dans le bec, allez-y, impl\u00e9mentez!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def iso_log_probabilite(mu, sigma_sq, test_data):\n",
      "    \"\"\"\n",
      "    Calcule la log probabilit\u00e9 d'un ensemble d'exemples\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : float\n",
      "        La variance, un scalaire\n",
      "    test_data : array\n",
      "        Matrice de dimension (n,d) o\u00f9 n est le nombre d'exemple et d le nombre de dimension.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    log_prob : array\n",
      "        Les log probabilit\u00e9s des n exemples contenus dans test_data. Un vecteur de dimension n\n",
      "    \"\"\"\n",
      "    # calcule le log du d\u00e9nominateur\n",
      "    denominateur = # \u00e0 completer\n",
      "    \n",
      "    # calcule la log probabilit\u00e9 totale\n",
      "    log_prob = denominateur - # \u00e0 completer\n",
      "    \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maintenant, testez la nouvelle fonction qui utilise les op\u00e9rations termes \u00e0 termes de `numpy` impl\u00e9ment\u00e9 de mani\u00e8re beaucoup plus efficace qu'une boucle python. Vous devriez voir que l'ex\u00e9cution se fait environ 10 fois plus vite!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.normal(size=(10000,100))\n",
      "\n",
      "mu, sigma_sq = gaussienne_isotropique(X)\n",
      "\n",
      "import time\n",
      "start = time.clock()\n",
      "log_probs = iso_log_probabilite(mu, sigma_sq, X)\n",
      "stop = time.clock()\n",
      "\n",
      "print \"Le calcul a pris\", stop-start, \"secondes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le code suivant va utilser les deux fonctions que vous avez compl\u00e9t\u00e9es et afficher la distribution apprises sur l'ensemble `iris`. Vous pouvez changer la classe en changeant la valeur de `train_class` pour 1,2 ou 3. \n",
      "\n",
      "Est-ce que la distribution de la gaussienne vous semble bien repr\u00e9senter la distribution des donn\u00e9es?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris=np.loadtxt('iris.txt')\n",
      "np.random.seed(3395)\n",
      "train_cols = [0,1]\n",
      "train_class = 1\n",
      "\n",
      "\n",
      "# selection seulement la classe train_class\n",
      "iris = iris[iris[:,-1]==train_class]\n",
      "n_train = int(iris.shape[0]*0.90)\n",
      "\n",
      "inds = range(iris.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "\n",
      "train_set = iris[train_inds]\n",
      "test_set = iris[test_inds]\n",
      "train_set = train_set[:, train_cols]\n",
      "test_set = test_set[:, train_cols]\n",
      "\n",
      "mu, sigma_sq = gaussienne_isotropique(train_set)\n",
      "\n",
      "print \"gaussienne isotropique\"\n",
      "utilitaires3.distribution(train_set, test_set, lambda data: iso_log_probabilite(mu, sigma_sq, data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Au cas o\u00f9 vous ne l'aviez pas devin\u00e9, la r\u00e9ponse \u00e9tait non! C'est pourquoi nous allons passer \u00e0 l'impl\u00e9mentation d'une gaussienne diagonale. Le calcul de la moyenne reste la m\u00eame, mais la variance se calcule maintenant selon l'\u00e9quation suivante:\n",
      "\n",
      "$$\\vec{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\vec{\\mu})^2$$\n",
      "\n",
      "La log probabilit\u00e9 se calcule suivant\n",
      "\n",
      "$$\\log p(X_i) = \\left(- \\frac{d}{2}\\log 2\\pi - \\log(\\prod_{j=1}^{d}\\vec{\\sigma}_i)\\right) - \\left(\\frac{1}{2}\\sum_{j=1}^d \\frac{(X_{i,j}-\\vec{\\mu}_j)^2}{\\vec{\\sigma}_j^2}\\right)$$\n",
      "\n",
      "\u00c0 noter que $\\vec{\\sigma}^2$ est maintenant un vecteur. Il ne vous reste qu'\u00e0 completer les deux fonctions suivantes comme vous venez de le faire pr\u00e9c\u00e9demment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gaussienne_diagonale(train_data):\n",
      "    \"\"\"\n",
      "    Calcule la moyenne mu et la variance sigma_sq\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    train_data : array\n",
      "        Matrice de dimension (n,d) o\u00f9 n est le nombre d'exemple et d le nombre de dimension.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : array\n",
      "        La variance, un vecteur de dimension d.\n",
      "    \"\"\"\n",
      "    # calcule la moyenne\n",
      "    mu = None # \u00e0 completer\n",
      "    \n",
      "    # calcule la variance\n",
      "    sigma_sq = None # \u00e0 completer\n",
      "\n",
      "    return mu, sigma_sq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.normal(size=(5,2))\n",
      "\n",
      "mu, sigma_sq = gaussienne_diagonale(X)\n",
      "utilitaires3.compare(mu, X.mean(0), \"moyenne\")\n",
      "utilitaires3.compare(sigma_sq, X.std(0)**2, \"variance\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def diag_log_probabilite(mu, sigma_sq, test_data):\n",
      "    \"\"\"\n",
      "    Calcule la log probabilit\u00e9 d'un ensemble d'exemples\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : array\n",
      "        La variance, un vecteur de dimension d\n",
      "    test_data : array\n",
      "        Matrice de dimension (n,d) o\u00f9 n est le nombre d'exemple et d le nombre de dimension.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    log_prob : array\n",
      "        Les log probabilit\u00e9s des n exemples contenus dans test_data. Un vecteur de dimension n\n",
      "    \"\"\"\n",
      "    # calculer la log probabilite\n",
      "    denominateur = # \u00e0 completer\n",
      "    \n",
      "    log_prob = denominateur - # \u00e0 completer\n",
      "    \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vous pouvez maintenant comparer les r\u00e9sultats des gaussiennes isotropiques et diagonales. **N'oubliez pas de changer la classe pour voir diff\u00e9rents exemples.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris=np.loadtxt('iris.txt')\n",
      "np.random.seed(3395)\n",
      "train_cols = [0,1]\n",
      "train_class = 3\n",
      "\n",
      "# selection seulement la classe train_class\n",
      "iris = iris[iris[:,-1]==train_class]\n",
      "n_train = int(iris.shape[0]*0.90)\n",
      "\n",
      "inds = range(iris.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "\n",
      "train_set = iris[train_inds]\n",
      "test_set = iris[test_inds]\n",
      "train_set = train_set[:, train_cols]\n",
      "test_set = test_set[:, train_cols]\n",
      "\n",
      "print \"gaussienne isotropique\"\n",
      "mu, sigma_sq = gaussienne_isotropique(train_set)\n",
      "utilitaires3.distribution(train_set, test_set, lambda data: iso_log_probabilite(mu, sigma_sq, data))\n",
      "print \"gaussienne diagonale\"\n",
      "mu, sigma_sq = gaussienne_diagonale(train_set)\n",
      "utilitaires3.distribution(train_set, test_set, lambda data: diag_log_probabilite(mu, sigma_sq, data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Il y a encore place \u00e0 l'am\u00e9lioration, c'est pourquoi nous allons passer \u00e0 une gaussienne avec matrice de covariance. La totale quoi. :)\n",
      "\n",
      "La matrice de covariance se calcule selon l'\u00e9quation suivante\n",
      "\n",
      "$$\\Sigma = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\vec{\\mu})^T(X_i-\\vec{\\mu})$$\n",
      "\n",
      "O\u00f9 $X_i$ est le $i$-\u00e8me vecteur ligne. Par d\u00e9finition, $\\displaystyle\\sum_{i=1}^n X_i^TX_i = X^TX$, on peut donc impl\u00e9menter de mani\u00e8re plus efficace la matrice de covariance, soit\n",
      "\n",
      "$$\\Sigma = \\frac{1}{n}(X-\\vec{\\mu})^T(X-\\vec{\\mu})$$\n",
      "\n",
      "La log probabilit\u00e9 se calcule suivant\n",
      "\n",
      "$$\\log p(X_i) = \\left(- \\frac{d}{2}\\log 2\\pi - \\log|\\Sigma|\\right) - \\left(\\frac{1}{2}(X_i-\\vec{\\mu})\\Sigma^{-1}(X_i-\\vec{\\mu})^T\\right)$$\n",
      "\n",
      "O\u00f9 $|\\Sigma|$ est le d\u00e9terminant de la matrice de covariance. L'impl\u00e9mentation se corse un peu pour la log probabilit\u00e9 ici. Vous aurez besoin des functions `np.linalg.det` et `np.linalg.inv`. \n",
      "\n",
      "**Attention!** Bien que la derni\u00e8re partie soit 2 produits matricielles pour un exemple $X_i$, soit $A = (X_i-\\vec{\\mu})^T\\Sigma^{-1}$, puis $A(X_i-\\vec{\\mu})$, ce n'est pas vrai pour une matrice $X$. Pour vous aider, je vous donne l'\u00e9quation pour une matrice $X$.\n",
      "\n",
      "$$\\log p(X) = \\left(- \\frac{d}{2}\\log 2\\pi - \\log|\\Sigma|\\right) - \\left(\\frac{1}{2}\\sum_{j=1}^d\\left(\\left((X-\\vec{\\mu})\\Sigma^{-1}\\right) \\circ (X_i-\\vec{\\mu})\\right)_j\\right)$$\n",
      "\n",
      "O\u00f9 $\\circ$ est un produit terme \u00e0 terme entre deux matrices et $\\displaystyle\\sum_{j=1}^d Y_j$ somme sur l'axe 1 d'une matrice. On obtient donc $\\log p(X)$, un vecteur de taille $n$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gaussienne_complete(train_data):\n",
      "    \"\"\"\n",
      "    Calcule la moyenne mu et la matrice de covariance\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    train_data : array\n",
      "        Matrice de dimension (n,d) o\u00f9 n est le nombre d'exemple et d le nombre de dimension.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : array\n",
      "        La matrice de covariance de dimension (d, d).\n",
      "    \"\"\"\n",
      "    \n",
      "    # calcule la moyenne\n",
      "    mu = None # \u00e0 completer\n",
      "    \n",
      "    # calcule la matrice de covariance\n",
      "    cov = None # \u00e0 completer\n",
      "    return mu, cov"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.normal(size=(5,2))\n",
      "\n",
      "mu, sigma_sq = gaussienne_complete(X)\n",
      "utilitaires3.compare(mu, X.mean(0), \"moyenne\")\n",
      "utilitaires3.compare(sigma_sq, np.cov(X.T, bias=1), \"variance\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def comp_log_probabilite(mu, cov, test_data):\n",
      "    \"\"\"\n",
      "    Calcule la log probabilit\u00e9 d'un ensemble d'exemples\n",
      "        \n",
      "    Parameters\n",
      "    ----------\n",
      "    mu : array\n",
      "        La moyenne, un vecteur de dimension d\n",
      "    sigma_sq : array\n",
      "        La matrice de covariance de dimension (d, d)\n",
      "    test_data : array\n",
      "        Matrice de dimension (n,d) o\u00f9 n est le nombre d'exemple et d le nombre de dimension.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    log_prob : array\n",
      "        Les log probabilit\u00e9s des n exemples contenus dans test_data. Un vecteur de dimension n\n",
      "    \"\"\"\n",
      "    \n",
      "    # \u00e0 completer\n",
      "    \n",
      "    return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Et finalement comparez les trois types de gaussiennes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris=np.loadtxt('iris.txt')\n",
      "np.random.seed(3395)\n",
      "train_cols = [0,1]\n",
      "train_class = 1\n",
      "\n",
      "# selection seulement la classe train_class\n",
      "iris = iris[iris[:,-1]==train_class]\n",
      "n_train = int(iris.shape[0]*0.90)\n",
      "\n",
      "inds = range(iris.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "\n",
      "train_set = iris[train_inds]\n",
      "test_set = iris[test_inds]\n",
      "train_set = train_set[:, train_cols]\n",
      "test_set = test_set[:, train_cols]\n",
      "\n",
      "print \"gaussienne isotropique\"\n",
      "mu, sigma_sq = gaussienne_isotropique(train_set)\n",
      "utilitaires3.distribution(train_set, test_set, lambda data: iso_log_probabilite(mu, sigma_sq, data))\n",
      "print \"gaussienne diagonale\"\n",
      "mu, sigma_sq = gaussienne_diagonale(train_set)\n",
      "utilitaires3.distribution(train_set, test_set, lambda data: diag_log_probabilite(mu, sigma_sq, data))\n",
      "print \"gaussienne avec matrice de covariance\"\n",
      "mu, sigma_sq = gaussienne_complete(train_set)\n",
      "utilitaires3.distribution(train_set, test_set, lambda data: comp_log_probabilite(mu, sigma_sq, data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifieur de Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maintenant, on va construire un **classifieur multiclasse de Bayes**. \u00c7a veut dire qu'au lieu de mod\u00e9liser $p(\\mbox{classe}|\\mbox{exemple})$ (ou $p(y|x)$), on va plut\u00f4t utiliser l'\u00e9quation de Bayes \n",
      "\n",
      "$$p(\\mbox{classe}|\\mbox{exemple}) = \\frac{p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe})}{p(\\mbox{exemple})}$$\n",
      "\n",
      "et mod\u00e9liser les diff\u00e9rents morceaux. En fait, on a juste besoin de mod\u00e9liser $p(\\mbox{exemple}|\\mbox{classe})$ et $p(\\mbox{classe})$ (puisque $p(\\mbox{exemple})$ est \u00e9gal quel que soit la classe ($\\frac{1}{n}$)). \n",
      "\n",
      "Le terme $p(\\mbox{classe})$ repr\u00e9sente la probabilit\u00e9 \u00e0 priori d'une classe, c'est-\u00e0-dire notre croyance \u00e0 priori - avant d'avoir vu un exemple en particulier - sur la probabilit\u00e9 qu'un exemple inconnu appartienne \u00e0 cette classe). On va repr\u00e9senter cette croyance \u00e0 priori pour une classe par la fr\u00e9quence de cette derni\u00e8re dans les donn\u00e9es d'entra\u00eenement: $\\frac{n_c}{n}$ o\u00f9 $n_c$ = nombre d'exemple de la classe $c$, puis $n$ = nombre d'exemple d'entra\u00eenement. \n",
      "\n",
      "On va utiliser des **densit\u00e9s Gaussiennes multivari\u00e9es** pour mod\u00e9liser les diff\u00e9rents $p(\\mbox{exemple}|\\mbox{classe})$. Cela veut dire que pour chaque classe, on va supposer que la \"vrai\" distribution $p(\\mbox{exemple}|\\mbox{classe})$ poss\u00e8de la forme d'une Gaussienne multivari\u00e9e dont on va tenter d'apprendre les param\u00e8tres $\\mu$ et $\\Sigma$. En pratique, on va se limiter aujourd'hui \u00e0 un cas particulier de cette distribution: celui o\u00f9 l'on suppose que la matrice de covariance $\\Sigma$ de chaque Gaussienne est diagonale et que chaque \u00e9l\u00e9ment de cette diagonale est le m\u00eame, soit sigma_sq ( <=> \"sigma square\" <=> $\\sigma^2$ <=> la variance). On poss\u00e8de donc un seul param\u00e8tre pour contr\u00f4ler la forme de la covariance. C'est plus simple (pour nous et pour l'ordinateur) \u00e0 calculer, mais \u00e7a signifie aussi que notre mod\u00e8le est moins puissant. \n",
      "\n",
      "On a donc un mod\u00e8le param\u00e9trique tr\u00e8s simple. Les param\u00e8tres sont la moyenne $\\mu$ (un vecteur de dimension celle de l'entr\u00e9e du syst\u00e8me) et la variance $\\sigma^2$ (un seul scalaire dans notre mod\u00e8le simple, qui va multiplier la matrice identit\u00e9). L'apprentissage dans ce mod\u00e8le se fera aujourd'hui par l'application du **principe de maximum de vraisemblance** comme vue pr\u00e9demment. Pour chaque classe, on va trouver les valeurs des param\u00e8tres qui maximisent la log-vraisemblance des donn\u00e9es d'entra\u00eenement issus de cette classe.\n",
      "\n",
      "Ayant trouv\u00e9 les param\u00e8tres qui maximisent la vraisemblance pour chacune des classes, il nous est possible de calculer tous les $p(\\mbox{exemple}|\\mbox{classe})$. Il suffit maintenant d'appliquer la r\u00e8gle de Bayes afin de pouvoir classifier un nouvel exemple. Plus pr\u00e9cis\u00e9ment, on voudra choisir, pour un exemple, la classe qui maximise $p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe})$ ou encore $log(p(\\mbox{exemple}|\\mbox{classe})p(\\mbox{classe}))$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vous avez ici une classe repr\u00e9sentant un mod\u00e8le d'apprentissage. Si vous n'\u00eates pas familier avec les classes en g\u00e9n\u00e9ral ou avec python, lisez le tutoriel officiel [fran\u00e7ais](http://www.afpy.org/doc/python/2.7/tutorial/classes.html) [anglais](https://docs.python.org/2/tutorial/classes.html). Prenez l'habitude, les mod\u00e8les que nous impl\u00e9menterons auront toujours les m\u00e9thodes __init__, train et compute_predictions comme c'est le cas ici."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class gauss_mv:\n",
      "    \"\"\"\n",
      "    Une distribution gaussienne\n",
      "    \n",
      "    Permet d'entrainer une gaussienne sur des donn\u00e9es et de calculer la log probabilit\u00e9s d'exemples.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_dims : int\n",
      "        le nombre de dimension des donn\u00e9es\n",
      "    cov_type : string\n",
      "        Le type de gaussienne : \n",
      "            - isotropique - variance scalaire\n",
      "            - diagonale - variance diagonale\n",
      "            - full - matrice de covariance\n",
      "            \n",
      "    Notes\n",
      "    -----\n",
      "        \n",
      "    Les fonctions gaussienne_isotropique, gaussienne_diagonale, \n",
      "    gaussienne_complete ainsi que les fonctions iso_log_probabilite\n",
      "    diag_log_probabilite et comp_log_probabilite sont ceux d\u00e9finies \n",
      "    plus haut dans ce fichier notebook.\n",
      "    \"\"\"\n",
      "    def __init__(self,n_dims,cov_type):\n",
      "        self.n_dims = n_dims\n",
      "        self.mu = np.zeros((1,n_dims))\n",
      "        self.cov_type = cov_type\n",
      "        if cov_type==\"isotropique\":\n",
      "            self.sigma_sq = 1.0\n",
      "        elif cov_type==\"diagonale\":\n",
      "            self.sigma_sq = numpy.ones(n_dims)\n",
      "        elif cov_type==\"full\":\n",
      "            self.cov = np.ones((n_dims, n_dims))\n",
      "\n",
      "    def train(self, train_data):\n",
      "        \"\"\"\n",
      "        M\u00e9thode d'entrainement de la gaussienne.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        train_data : array\n",
      "            Matrice de donn\u00e9es d'entrainement, de dimension (n, self.n_dims)\n",
      "        \"\"\"\n",
      "        if self.cov_type == \"isotropique\":\n",
      "            mu, sigma_sq = gaussienne_isotropique(train_data)\n",
      "        elif self.cov_type == \"diagonale\":\n",
      "            mu, sigma_sq = gaussienne_diagonale(train_data)\n",
      "        elif self.cov_type == \"full\":\n",
      "            mu, sigma_sq = gaussienne_complete(train_data)\n",
      "            \n",
      "        self.mu = mu\n",
      "        self.sigma_sq = sigma_sq\n",
      "        pass\n",
      "    \n",
      "    def compute_predictions(self, test_data):\n",
      "        \"\"\"\n",
      "        Calcule les log probabilit\u00e9s de plusieurs exemples.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        test_data : array\n",
      "            Matrice de donn\u00e9es de test, de dimension (n, self.n_dims)\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        log_prob : array\n",
      "            vecteur de log probabilit\u00e9s de dimension n\n",
      "        \"\"\"\n",
      "        if self.cov_type == \"isotropique\":\n",
      "            log_prob = iso_log_probabilite(self.mu, self.sigma_sq, test_data)\n",
      "        elif self.cov_type == \"diagonale\":\n",
      "            log_prob = diag_log_probabilite(self.mu, self.sigma_sq, test_data)\n",
      "        elif self.cov_type == \"full\":\n",
      "            log_prob = comp_log_probabilite(self.mu, self.sigma_sq, test_data)\n",
      "            \n",
      "        return log_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Presque tout est fait \u00e0 votre place. Maintenant, impl\u00e9mentez la m\u00e9thode compute_predictions du classifieur de bayes. Avec la classe d\u00e9fini pr\u00e9c\u00e9demment, \u00e7a ne devrait pas \u00eatre compliqu\u00e9. Si vous ne comprenez pas, posez des questions!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pour faire taire les underflow\n",
      "np.seterr(under='ignore')\n",
      "\n",
      "class classif_bayes:\n",
      "    \"\"\"\n",
      "    Classifieur de Bayes\n",
      "    \n",
      "    Permets de calculer la probabilit\u00e9 qu'un exemple appartienne \u00e0 une classe.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    modeles_mv : list of objects\n",
      "        Mod\u00e8les pr\u00e9alablement entrain\u00e9 et qui poss\u00e8de la m\u00e9thode compute_predictions. \n",
      "        Le nombre de mod\u00e8le doit correspondre au nombre de priors (nombre de classes)\n",
      "    priors : list of floats\n",
      "        Liste de float repr\u00e9sentant la probabilit\u00e9 \u00e0 priori de chaque classes.\n",
      "        Le nombre de prior doit correspondre au nombre de mod\u00e8les (nombre de classes)\n",
      "            \n",
      "    Notes\n",
      "    -----\n",
      "        \n",
      "    Les mod\u00e8les doivent \u00eatre entrain\u00e9 pr\u00e9alablement avant d'appeler la m\u00e9thode compute_predictions.\n",
      "    Le nombre de mod\u00e8le doit correspondre au nombre de priors (nombre de classes).\n",
      "    \"\"\"\n",
      "    def __init__(self,modeles_mv, priors):\n",
      "        self.modeles_mv = modeles_mv\n",
      "        self.priors = priors\n",
      "        \n",
      "        if len(self.modeles_mv) != len(self.priors):\n",
      "            print 'Le nombre de modeles MV doit etre egale au nombre de priors!'\n",
      "        \n",
      "        self.n_classes = len(self.modeles_mv)\n",
      "                                                            \n",
      "    def compute_predictions(self, test_data):\n",
      "        \"\"\"\n",
      "        Calcule les log probabilit\u00e9s de plusieurs exemples. \n",
      "        Pour chaque exemple, le nombre de log probabilit\u00e9 est \u00e9gal\n",
      "        au nombre de classes\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        test_data : array\n",
      "            Matrice de donn\u00e9es de test, de dimension (n, self.n_dims)\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        log_prob : array\n",
      "            matrice de log probabilit\u00e9s de dimension (n,c) o\u00f9 c est le nombre de classes\n",
      "        \"\"\"\n",
      "        \n",
      "        # commencez par initialiser une variable log_pred vide avec np.empty\n",
      "        # \u00e0 completer\n",
      "        \n",
      "        # puis calculer les log probabilit\u00e9 classe par classe\n",
      "        # Avec iris par exemple qui n'a que 3 classes, la boucle ne bouclera que 3 fois\n",
      "        for i in range(self.n_classes):\n",
      "            \n",
      "            # ici il va falloir utiliser modeles_mv[i] et priors pour remplir\n",
      "            # chaque colonne de log_pred. Faites un seul calcul pour toute la classe.\n",
      "            # Pensez bien \u00e0 comment indexer log_pred, tout peut tenir sur une seule ligne... mis \u00e0 part le commentaire -_-\n",
      "            \n",
      "            # \u00e0 completer\n",
      "\n",
      "        return log_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La cellule suivante vous permettra de tester votre m\u00e9thodes compute_predictions. Vous devez cependant construire le mod\u00e8le de classification bay\u00e9sien. Des indices trainent un peu partout pour vous aider. Une fois termin\u00e9, comparez les classifeurs de bayes avec trois types diff\u00e9rents de gaussiennes. **Changez les valeurs de train_cols pour voir comment les classifieurs se comporte.** Est-ce que les gaussiennes avec matrice de covariance donnent toujours de meilleur r\u00e9sultats? Pourquoi? Essayez notamment les valeurs `train_cols = [2,3]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris=np.loadtxt('iris.txt')\n",
      "train_cols = [0,2]\n",
      "target_cols = [-1]\n",
      "iris_train1=iris[0:50,train_cols]\n",
      "iris_train2=iris[50:100,train_cols]\n",
      "iris_train3=iris[100:150,train_cols]\n",
      "\n",
      "def experiment(cov_type):\n",
      "    # On cree un modele par classe (par maximum de vraissemblance)\n",
      "    # \u00e0 completer\n",
      "\n",
      "    # On cree une liste de tous nos modeles\n",
      "    # On fait la meme chose pour les priors\n",
      "    # Les priors sont calcules ici de facon exact car on connait le nombre \n",
      "    # de representants par classes. Un fois que vous aurez cree un\n",
      "    # ensemble de train/test, il va faloir les calculer de facon exacte\n",
      "    # \u00e0 completer\n",
      "\n",
      "    # On cree notre classifieur avec notre liste de modeles gaussien et nos priors\n",
      "    # \u00e0 completer\n",
      "\n",
      "    #on peut maintenant calculer les logs-probabilites selon nos modeles\n",
      "    # \u00e0 completer\n",
      "\n",
      "    # il reste maintenant a calculer le maximum par classe pour la classification\n",
      "    # \u00e0 completer\n",
      "\n",
      "    utilitaires3.teste(classesPred,iris[:,-1])\n",
      "\n",
      "    print \"Taux d'erreur %.2f%%\" % ((1-(classesPred==iris[:,-1]).mean())*100.0)\n",
      "    idx = range(0,iris.shape[0])\n",
      "    np.random.shuffle(idx)\n",
      "    train_set = iris[idx[:100]]\n",
      "    test_set = iris[idx[100:]]\n",
      "    train_set = train_set[:,train_cols+target_cols]\n",
      "    test_set = test_set[:,train_cols+target_cols]\n",
      "    utilitaires3.gridplot(classifieur,train_set,test_set,n_points=50)\n",
      "    \n",
      "for cov_type in [\"isotropique\", \"diagonale\", \"full\"]:\n",
      "    print \"gaussienne\", cov_type\n",
      "    experiment(cov_type)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}