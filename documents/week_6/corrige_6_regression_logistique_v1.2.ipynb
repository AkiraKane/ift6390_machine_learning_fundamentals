{
 "metadata": {
  "name": "",
  "signature": "sha256:9cc9f6ba3ac2f3a16ee89f4b08b7f021e27552b4f2e728189b5499ed042d7762"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "R\u00e9gression logistique multiclasse"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comme vue dans les cours, la r\u00e9gression logistique multiclasse se calcule de la mani\u00e8re suivante\n",
      "\n",
      "$$ y_k(X_n) = \\sum_i W_{i,k} X_{n,i}+b_k$$\n",
      "\n",
      "$$ h_k(X_n) = \\mbox{softmax}(y_k(X_n))$$\n",
      "\n",
      "Et on apprend les param\u00e8tres $W$ et $b$ en minimisant la log-vraisemblance n\u00e9gative. \n",
      "\n",
      "$$L = -\\ln\\prod_k h_k(X_n)^{t_{n,k}} = -\\sum_k t_{n,k}\\ln h_k(X_n) = -t_{n,c}\\ln h_c(X_n) = -\\ln h_c(X_n)$$\n",
      "\n",
      "o\u00f9 $c$ est la classe d'un exemple $n$, donc pour tout $k\\neq c$ $t_{n,k}=0$ et seulement $t_{n,k=c} = t_{n,c} = 1$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%pylab inline\n",
      "%aimport numpy\n",
      "np=numpy\n",
      "import time\n",
      "import utilitaires6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On se r\u00e9chauffe avec un exercice facile. Commencez par impl\u00e9menter la fonction softmax d'un matrice A\n",
      "\n",
      "$$\u00a0\\mbox{softmax}_k(a) = \\frac{\\exp(a_k)}{\\sum_jexp(a_j)}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(A):\n",
      "    \"\"\"\n",
      "    Fonction softmax \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    A: array\n",
      "        matrice de dimension n par d\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    array\n",
      "        matrice de dimension n par d\n",
      "    \"\"\"\n",
      "\n",
      "    return np.exp(A) / np.exp(A).sum(1)[:, None]\n",
      "\n",
      "X = np.random.normal(size=(20,10))\n",
      "\n",
      "print X.shape\n",
      "\n",
      "Y = X.sum(1)\n",
      "\n",
      "print Y.shape\n",
      "\n",
      "#Y = Y.reshape((Y.shape[0], 1))\n",
      "\n",
      "#print Y.shape\n",
      "\n",
      "Y = Y[:,None]\n",
      "Y = Y.reshape((Y.shape[0], 1))\n",
      "\n",
      "print Y.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calcul de la d\u00e9riv\u00e9"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maintenant calculons la d\u00e9riv\u00e9 de $L$ par rapport \u00e0 $W_{r,s}$.\n",
      "\n",
      "Par la r\u00e8gle de d\u00e9riv\u00e9 en cha\u00eene, on a\n",
      "\n",
      "$$\\frac{\\partial L}{\\partial W_{r,s}} = \\frac{\\partial L}{\\partial h_c}\\frac{\\partial h_c}{\\partial W_{r,s}}$$\n",
      "                                        \n",
      "On peut donc commencer par calculer simplement $\\frac{\\partial L}{\\partial h_c}$\n",
      "\n",
      "$$\\frac{\\partial L}{\\partial h_c} = \\frac{\\partial}{\\partial h_c} -\\ln h_c(X_n) = -\\frac{1}{h_c(X_n)}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "D\u00e9riv\u00e9 du softmax"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On peut ensuite calculer $\\frac{\\partial h_c}{\\partial W_{r,s}}$ encore une fois par la r\u00e8gle de d\u00e9riv\u00e9 en cha\u00eene, mais on doit faire bien attention car $h_c$ d\u00e9pent de $W_{r,s}$ \u00e0 travers diff\u00e9rent $y_s$. Alors\n",
      "\n",
      "$$\\frac{\\partial h_c}{\\partial W_{r,s}} = \\frac{\\partial h_c}{\\partial y_s}\u00a0\\frac{\\partial y_s}{\\partial W_{r,s}}$$\n",
      "\n",
      "Pourquoi par $y_c$? Parce que $W_{r,s}$ appara\u00eet toujours dans $y_s$ qui lui appara\u00eet toujours dans $h_c$. On doit donc d\u00e9riv\u00e9 pour deux cas diff\u00e9rent, soit $s=c$, soit $s\\neq c$.\n",
      "\n",
      "Si $s=c$\n",
      "\n",
      "$$\n",
      "\\begin{array}{rl}\n",
      "\\frac{\\partial h_c}{\\partial y_c} &= \\frac{\\partial}{\\partial y_c}\\frac{\\exp(y_c)}{\\sum_j\\exp(y_j)} \\\\\n",
      "                                  &= \\frac{\\exp(y_c)\\sum_j\\exp(y_j) - \\exp(y_c)\\exp(y_c)}{\\left(\\sum_j\\exp(y_j)\\right)^2} \\\\\n",
      "                                  &= \\frac{\\exp(y_c)}{\\sum_j\\exp(y_j)}\\left(1 - \\frac{\\exp(y_c)}{\\sum_j\\exp(y_j) }\\right)\\\\\n",
      "                                  &= h_c(x_n)(1 - h_c(x_n))\\\\\n",
      "\\end{array}\n",
      "$$\n",
      "\n",
      "Et si $s\\neq c$\n",
      "\n",
      "$$\n",
      "\\begin{array}{rl}\n",
      "\\frac{\\partial h_c}{\\partial y_s} &= \\frac{\\partial}{\\partial y_s}\\frac{\\exp(y_c)}{\\sum_j\\exp(y_j)} \\\\\n",
      "                                  &= 0 - \\frac{\\exp(y_c)\\exp(y_s)}{\\left(\\sum_j\\exp(y_j)\\right)^2} \\\\\n",
      "                                  &= - \\frac{\\exp(y_c)}{\\sum_j\\exp(y_j)}\\frac{\\exp(y_s)}{\\sum_j\\exp(y_j)}\\\\\n",
      "                                  &= - h_c(x_n)h_s(x_n)\\\\\n",
      "\\end{array}\n",
      "$$\n",
      "\n",
      "\n",
      "Avant de passer \u00e0 $\\frac{\\partial y_s}{\\partial W_{r,s}}$, on peut faire un peu de gymnastique avec $\\frac{\\partial L}{\\partial y_s}$ et annuler quelques termes.\n",
      "\n",
      "On avait \n",
      "\n",
      "$$\\frac{\\partial L}{\\partial y_s} = \\frac{\\partial L}{\\partial h_c}\\frac{\\partial h_c}{\\partial y_s}$$\n",
      "\n",
      "Donc, pour le cas $s=c$, on a\n",
      "\n",
      "$$\\frac{\\partial L}{\\partial y_s} = -\\frac{1}{h_c(X_n)}h_c(x_n)(1 - h_c(x_n)) = h_c(x_n) - 1$$\n",
      "\n",
      "Et pour le cas $s\\neq c$, on a \n",
      "\n",
      "$$\\frac{\\partial L}{\\partial y_s} = -\\frac{1}{h_c(X_n)}(- h_c(x_n)h_s(x_n)) = h_s(x_n)$$\n",
      "\n",
      "Autrement dit, si $s=c$, on soustrait 1 \u00e0 $h_s(x_n)$ sinon 0. On peut donc combiner les deux cas en soustrayant $t_{n,s}$!\n",
      "\n",
      "$$\\frac{\\partial L}{\\partial y_s} = h_s(x_n) - t_{n,s}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "D\u00e9riv\u00e9 de $y_s$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On termine enfin avec la d\u00e9riv\u00e9 de $\\frac{\\partial y_s}{\\partial W_{r,s}}$\n",
      "\n",
      "$$\n",
      "\\begin{array}{rl}\n",
      "\\frac{\\partial y_s}{\\partial W_{r,s}} &= \\frac{\\partial}{\\partial y_s} \\sum_i W_{i,s} X_{n,i}+b_s\\\\\n",
      "                                      &= X_{n,r}\\\\\n",
      "\\end{array}\n",
      "$$\n",
      "\n",
      "Car pour tout $i\\neq r$, la d\u00e9riv\u00e9 de $W_{i,s} = 0$\n",
      "\n",
      "On a donc la d\u00e9riv\u00e9 totale de $L$ par rapport \u00e0 $W_{r,s}$\n",
      "\n",
      "$$\\frac{\\partial L}{\\partial W_{r,s}} = (h_s(X_n)-t_{n,s})X_{n,r}$$\n",
      "\n",
      "Sous forme de calcul matriciel on se retrouve avec \n",
      "\n",
      "$$\\frac{\\partial L}{\\partial W} = \\left(h(X_n)-t_n\\right)^TX_n$$\n",
      "\n",
      "o\u00f9 la premi\u00e8re composante est un vecteur colonne et $X_n$ est un vecteur ligne\n",
      "\n",
      "Ainsi, la mise \u00e0 jour de $W$ se calcule selon\n",
      "\n",
      "$$W^{t+1} \\leftarrow W^t - \\mu \\left(h(X_n)-t_n)\\right)^TX_n$$\n",
      "\n",
      "La gradient pour $b$ est similaire, donnant le r\u00e9sultat\n",
      "\n",
      "$$b^{t+1} \\leftarrow b^t - \\mu \\left(h(X_n)-t_n)\\right)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression:\n",
      "\n",
      "    def __init__(self, mu, n_classes):\n",
      "        \"\"\"\n",
      "        Constructeur de la classe.\n",
      "        \n",
      "        Prend les param\u00e8tres donn\u00e9es \u00e0 la constuction de la classe et initialise ses attribues.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        mu: float\n",
      "            taux d'apprentissage\n",
      "        n_classes: int\n",
      "            nombre de classes\n",
      "        \"\"\"\n",
      "        # mu est le taux d'apprentissage.\n",
      "        self.mu = mu\n",
      "        self.n_classes = n_classes\n",
      "        \n",
      "    def compute_error_rate(self, data, labels):\n",
      "        \n",
      "        return (self.compute_predictions(data).argmax(1) != labels.argmax(1)).mean()\n",
      "    \n",
      "    def train(self, data, labels, max_epoque):\n",
      "        \"\"\"\n",
      "        Entraine le mod\u00e8le d'apprentissage\n",
      "        \n",
      "        Prend en entr\u00e9e une matrice de donn\u00e9es et entra\u00eene le mod\u00e8le. D'autre param\u00e8tres peuvent \u00eatre d\u00e9fini ici \n",
      "        tel que le nombre d'\u00e9poque d'entra\u00eenement par exemple.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array\n",
      "            matrice de dimension (n,d) o\u00f9 n est le nombre d'exemples et d le nombre de dimensions\n",
      "        labels : array\n",
      "            matrice one-hot de dimension (n,c) o\u00f9 n est le nombre d'exemples et c le nombre de classes\n",
      "            Les rang\u00e9es de la matrice doivent \u00eatre sous forme one-hot.\n",
      "        \"\"\"\n",
      "        # Cette fonction doit d\u00e9terminer les valeurs des parametres.\n",
      "        #   train_data: chaque ligne contient un exemple (traits et \u00e9tiquette).\n",
      "        \n",
      "        # 1) Initialisation des param\u00e8tres. \n",
      "        # Initialisez les poids \u00e0\u00a0de petites valeurs et le biais \u00e0 0.\n",
      "        \n",
      "        self.W = np.random.uniform(size=(data.shape[1],self.n_classes))\n",
      "        self.b = np.zeros(self.n_classes)\n",
      "        \n",
      "        # 2) Entrainement\n",
      "        i = 0\n",
      "        error_rate = 1 # on arr\u00eate si l'ensemble est lin\u00e9airement s\u00e9par\u00e9\n",
      "        epoque = 0\n",
      "        while error_rate > 0. and epoque < max_epoque:\n",
      "            \n",
      "            y = np.dot(data[i][None,:], self.W) + self.b\n",
      "            \n",
      "            self.W = self.W - self.mu * np.dot(data[i][:,None],(softmax(y) - labels[i][None,:]))\n",
      "            self.b = self.b - self.mu * (softmax(y) - labels[i][None,:])\n",
      "            \n",
      "            i = (i+1) % data.shape[0]\n",
      "            \n",
      "            if i==0:\n",
      "                error_rate = self.compute_error_rate(data, labels)\n",
      "                \n",
      "                print \"\u00e9poque %d : %2.1f%% d'erreur\" % (epoque, 100*error_rate)\n",
      "                \n",
      "                epoque += 1\n",
      "            \n",
      "    def compute_predictions(self, test_data):\n",
      "        \"\"\"\n",
      "        Calcule les pr\u00e9dictions du mod\u00e8le\n",
      "        \n",
      "        Calcule les pr\u00e9dictions \u00e0 partir d'une matrice de test. Donne en sortie une pr\u00e9dictions \n",
      "        pour chaque classe dans le cas d'une classification multiclasse. L'argmax n'est pas calcul\u00e9 dans cette m\u00e9thode.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        test_data : array\n",
      "            matrice de dimension (n,d) o\u00f9 n est le nombre d'exemples et d le nombre de dimensions\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        array\n",
      "            matrice de dimension (n,c) o\u00f9 n est le nombre d'exemples et c le nombre de classe\n",
      "        \"\"\"\n",
      "        \n",
      "        # A COMPL\u00c9TER!\n",
      "        # Cette fonction doit utiliser les param\u00e8tres appris pour calculer\n",
      "        # la valeur de sortie pour les exemples de test_data (un exemple\n",
      "        # par ligne, seulement les traits).\n",
      "  \n",
      "        # 1) Vous devez calculer la vraie valeur de sorties.\n",
      "        y = np.dot(test_data, self.W) + self.b\n",
      "        return softmax(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# On commence par charger iris\n",
      "iris = np.loadtxt('iris.txt')\n",
      "data = iris\n",
      "\n",
      "# On se limite au cas de la classification BINAIRE donc on va seulement garder \n",
      "# donn\u00e9es des 2 premi\u00e8res classes.\n",
      "# Ici on garde juste les exemples avec l'etiquette 1 et 2.\n",
      "data = data[data[:,-1]<3,:]\n",
      "# Ici on transforme chaque etiquette qui est egale a 2 en -1, pour avoir les \n",
      "# m\u00eames \u00e9tiquettes que dans la formulation standard du perceptron (1 et -1).\n",
      "data[data[:,-1]==2,-1] = 0\n",
      "\n",
      "# On se limite \u00e0 des donn\u00e9es dont la dimension est 2, de fa\u00e7on \u00e0 pouvoir visualiser\n",
      "# la fronti\u00e8re de decision avec la fonction gridplot.\n",
      "train_cols = [2,3]\n",
      "# Une variable pour contenir l'indice de la colonne correspondant aux \u00e9tiquettes.\n",
      "target_ind = [data.shape[1] - 1]\n",
      "\n",
      "# Nombre de classes\n",
      "n_classes = 2\n",
      "# Nombre de points d'entrainement\n",
      "n_train = 75\n",
      "# Taille de la grille = grid_size x grid_size\n",
      "grid_size = 50\n",
      "\n",
      "print \"On va entrainer un perceptron sur \", n_train, \" exemples d'entrainement\"\n",
      "\n",
      "# decommenter pour avoir des resultats non-deterministes \n",
      "random.seed(3395)\n",
      "\n",
      "# D\u00e9terminer au hasard des indices pour les exemples d'entrainement et de test\n",
      "inds = range(data.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "    \n",
      "# Separer les donnees dans les deux ensembles: entrainement et test.\n",
      "train_set = data[train_inds,:]\t# garder les bonnes lignes\n",
      "train_labels = (train_set[:, target_ind] == np.arange(n_classes)).astype(int)\n",
      "train_set = train_set[:,train_cols]  # garder les bonnes colonnes\n",
      "test_set = data[test_inds,:]\n",
      "test_labels = (test_set[:, target_ind] == np.arange(n_classes)).astype(int)\n",
      "test_set = test_set[:,train_cols]\n",
      "\n",
      "# Le taux d'apprentissage\n",
      "mu = 0.1\n",
      "\n",
      "# Cr\u00c3\u00a9er et entrainer le modele\n",
      "model = LogisticRegression(mu, 2)\n",
      "model.train(train_set, train_labels, 10)\n",
      "\n",
      "# Obtenir les sorties sur l'ensemble de test.\n",
      "t1 = time.clock()\n",
      "les_sorties = model.compute_predictions(test_set)\n",
      "t2 = time.clock()\n",
      "print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', test_set.shape[0],' points de test'\n",
      "\n",
      "# Convertir les sorties en classe. On prend le signe.\n",
      "classes_pred = les_sorties >= 0.5\n",
      "\n",
      "# Mesurer la performance.\n",
      "err = 1.0 - numpy.mean(test_labels==classes_pred)\n",
      "print \"L'erreur de test est de \", 100.0 * err,\"%\"\n",
      "\n",
      "# Affichage graphique\n",
      "if len(train_cols) == 2:\n",
      "    # Surface de decision\n",
      "    t1 = time.clock()\n",
      "    utilitaires6.gridplot(model,train_set,train_labels,test_set,test_labels,n_points = grid_size)\n",
      "    t2 = time.clock()\n",
      "    print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', grid_size * grid_size, ' points de la grille'\n",
      "    filename = 'grille_' + '_c1=' + str(train_cols[0]) + '_c2=' + str(train_cols[1])+'.png'\n",
      "    print 'On va sauvegarder la figure dans ', filename\n",
      "    pylab.savefig(filename,format='png')\n",
      "        \n",
      "else:\n",
      "    print 'Trop de dimensions (', len(train_cols),') pour pouvoir afficher la surface de decision'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tr\u00e8s courte introduction \u00e0 Theano"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Theano est une librairie, ou un language de compilation si vous pr\u00e9f\u00e9rez, fort utile pour l'apprentissage machine. Elle permet d'impl\u00e9menter le calcul sous forme analytique et de calculer automatiquement le gradient d'une \u00e9quation. Ce qui est, vous en conviendrez, fort utile!\n",
      "\n",
      "Suivez la d\u00e9monstration pour une coutre introduction \u00e0 Theano, sinon suivez le tutoriel en ligne [ici](http://deeplearning.net/software/theano/tutorial/index.html#tutorial). Cet outils est fort conseill\u00e9 aux \u00e9tudiants \u00e0 la maitrise qui devront faire un projet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function\n",
      "\n",
      "x = T.scalar('x')\n",
      "y = T.scalar('y')\n",
      "\n",
      "z = x + y\n",
      "\n",
      "k = z / (z + y)\n",
      "\n",
      "f = function([x,y], k)\n",
      "\n",
      "print f(x=1,y=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(10):\n",
      "    print f(i,i**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# T.scalar, T.dscalar, T.matrix, T.tensor3d, etc\n",
      "\n",
      "X = T.matrix('X')\n",
      "\n",
      "f = function([X], X+1)\n",
      "\n",
      "print f(np.zeros((10,400)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano import pp # pretty print\n",
      "\n",
      "x = T.col('x')\n",
      "y = T.col('y')\n",
      "\n",
      "z = x + y\n",
      "\n",
      "k = z / (z + y)\n",
      "\n",
      "z = T.nnet.softmax(k)\n",
      "\n",
      "print pp(z)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Matrices\n",
      "\n",
      "X = T.matrix('X')\n",
      "Y = T.matrix('Y')\n",
      "\n",
      "#Z = T.dot(X,Y)\n",
      "\n",
      "#f = function([X,Y], Z)\n",
      "\n",
      "x = np.zeros((20,10)) \n",
      "y = np.zeros((10, 5))\n",
      "\n",
      "#f(x,y)\n",
      "\n",
      "np.dot(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# multiple co\u00fbt\n",
      "\n",
      "x = T.matrix('x')\n",
      "y = T.matrix('y')\n",
      "\n",
      "\n",
      "f = function([x,y], [x+y, x**2+2*x*y+y**2, y**3])\n",
      "\n",
      "rval = f(np.ones((1,1)),np.ones((1,1))*2.)\n",
      "\n",
      "print type(rval)\n",
      "\n",
      "print rval[0].shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculer le gradient de y par rapport \u00e0 x\n",
      "\n",
      "x = T.scalar('x')\n",
      "\n",
      "z = x**2 + 1\n",
      "\n",
      "g = T.grad(z, wrt=x)\n",
      "\n",
      "f = function([x], g)\n",
      "\n",
      "print f(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# variable \u00abpartag\u00e9\u00bb\n",
      "import theano\n",
      "\n",
      "w = np.zeros(10)\n",
      "\n",
      "w = theano.shared(w, name=\"w\")\n",
      "\n",
      "print w.get_value()\n",
      "\n",
      "w.set_value(np.ones(10))\n",
      "\n",
      "print w.get_value()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# updates!\n",
      "\n",
      "w.set_value(np.ones(10))\n",
      "print w.get_value()\n",
      "\n",
      "f = function([], w, updates=((w,w+1),))\n",
      "\n",
      "print f()\n",
      "print f()\n",
      "print w.get_value()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "R\u00e9gression logistique avec Theano"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "L'architecture est toute faites pour vous, impl\u00e9mentez toutes les \u00e9quations \u00e0 l'int\u00e9rieur de la m\u00e9thode `LogisticRegression.__init__`. Les fonction de compilation sont \u00e9crites pour vous. \n",
      "\n",
      "L'entra\u00eenement se fera par mini-batch, un entre-deux entre la descente de gradient stochastique et la descente de gradient par batch. Retenez qu'ici, la valeur X qui sera donn\u00e9 \u00e0 self.update() sera une mini-batch de taille (batch_size, d). Pour calculer le co\u00fbt, on prend la moyenne sur la mini-batch. Ainsi, le gradient lors de la mise \u00e0 jour des param\u00e8tres est le gradient moyen sur la mini-batch.\n",
      "\n",
      "Utilisez ensuite la fonction \u00abmagique\u00bb `self.update` \u00e0 l'int\u00e9rieur de la m\u00e9thode `LogisticRegression.train` pour entra\u00eener le mod\u00e8le."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function\n",
      "import theano\n",
      "\n",
      "class TheanoLogisticRegression(object):\n",
      "\n",
      "    def __init__(self, learning_rate, d, n_classes):\n",
      "        \"\"\"\n",
      "        Constructeur de la classe.\n",
      "        \n",
      "        Prend les param\u00e8tres donn\u00e9es \u00e0 la constuction de la classe et initialise ses attribues.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        learning_rate: float\n",
      "            taux d'apprentissage\n",
      "        d: int\n",
      "            nombre de dimensions des exemples\n",
      "        n_classes: int\n",
      "            nombre de classes\n",
      "        \"\"\"\n",
      "        \n",
      "        # On initialise des variables Theanesques\n",
      "        X = T.matrix()\n",
      "        y = T.ivector('y')\n",
      "        \n",
      "        # On cr\u00e9e les param\u00e8tres sous forme de variables \u00abpartag\u00e9es\u00bb\n",
      "        self.W = theano.shared(\n",
      "            value=numpy.zeros((d, n_classes)),\n",
      "            name='W',\n",
      "        )\n",
      "        \n",
      "        self.b = theano.shared(\n",
      "            value=numpy.zeros(n_classes),\n",
      "            name='b',\n",
      "        )\n",
      "\n",
      "        p_y_given_x = T.nnet.softmax(T.dot(X, self.W) + self.b)\n",
      "\n",
      "        y_pred = T.argmax(p_y_given_x, axis=1)\n",
      "\n",
      "        # log-vraisemblance negative\n",
      "        cost = -T.mean(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "        # liste des mises \u00e0 jour \u00e0 faire lors de l'entrainement\n",
      "        updates = [\n",
      "            (self.W, self.W - learning_rate * T.grad(cost=cost, wrt=self.W)),\n",
      "            (self.b, self.b - learning_rate * T.grad(cost=cost, wrt=self.b))\n",
      "        ]\n",
      "        \n",
      "        # la fonction magique pour l'entrainement\n",
      "        self.update = function([X, y], y_pred, updates=updates)\n",
      "        \n",
      "        # une fonction qui renvoie les pr\u00e9dictions du mod\u00e8le\n",
      "        self.predict = function([X], y_pred)\n",
      "        \n",
      "        # une fonction qui renvoie le taux d'erreur du mod\u00e8le\n",
      "        self.compute_error_rate = function([X, y], T.mean(T.neq(y_pred, y)))\n",
      "\n",
      "        \n",
      "    def train(self, data, labels, batch_size, max_epoque):\n",
      "        \"\"\"\n",
      "        Entraine le mod\u00e8le d'apprentissage\n",
      "        \n",
      "        Prend en entr\u00e9e une matrice de donn\u00e9es et entra\u00eene le mod\u00e8le. D'autre param\u00e8tres peuvent \u00eatre d\u00e9fini ici \n",
      "        tel que le nombre d'\u00e9poque d'entra\u00eenement par exemple.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array\n",
      "            matrice de dimension (n,d) o\u00f9 n est le nombre d'exemples et d le nombre de dimensions\n",
      "        labels : array\n",
      "            matrice one-hot de dimension (n,c) o\u00f9 n est le nombre d'exemples et c le nombre de classes\n",
      "            Les rang\u00e9es de la matrice doivent \u00eatre sous forme one-hot.\n",
      "        batch_size: int\n",
      "            nombre d'exemple par mini-batch pour l'entra\u00eenement\n",
      "        max_epoque: int\n",
      "            nombre d'epoque d'entra\u00eenement\n",
      "        \"\"\"\n",
      "        \n",
      "        i = 0\n",
      "        precision = 0 # on arr\u00eate quand l'ensemble est lin\u00e9airement s\u00e9par\u00e9\n",
      "        epoque = 0\n",
      "        \n",
      "        X = data.astype('float32')\n",
      "        y = labels.astype('int32')\n",
      "        \n",
      "        for epoque in xrange(max_epoque):\n",
      "            \n",
      "            for mini_batch in xrange(0,data.shape[0],batch_size):\n",
      "                self.update(X[mini_batch:mini_batch+batch_size],\n",
      "                            y[mini_batch:mini_batch+batch_size])\n",
      "                \n",
      "            print \"\u00e9poque %d : %2.1f%% d'erreur\" % (epoque, 100*self.compute_error_rate(X,y))\n",
      "            \n",
      "    def compute_predictions(self, test_data):\n",
      "        \"\"\"\n",
      "        Calcule les pr\u00e9dictions du mod\u00e8le\n",
      "        \n",
      "        Calcule les pr\u00e9dictions \u00e0 partir d'une matrice de test. Donne en sortie une pr\u00e9dictions \n",
      "        pour chaque classe dans le cas d'une classification multiclasse. L'argmax n'est pas calcul\u00e9 dans cette m\u00e9thode.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        test_data : array\n",
      "            matrice de dimension (n,d) o\u00f9 n est le nombre d'exemples et d le nombre de dimensions\n",
      "            \n",
      "        Returns\n",
      "        -------\n",
      "        array\n",
      "            matrice de dimension (n,c) o\u00f9 n est le nombre d'exemples et c le nombre de classe\n",
      "        \"\"\"\n",
      "        \n",
      "        test_data = test_data.astype('float32')\n",
      "        \n",
      "        return self.predict(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# On commence par charger iris\n",
      "iris = np.loadtxt('iris.txt')\n",
      "data = iris\n",
      "\n",
      "# On se limite au cas de la classification BINAIRE donc on va seulement garder \n",
      "# donn\u00e9es des 2 premi\u00e8res classes.\n",
      "# Ici on garde juste les exemples avec l'etiquette 1 et 2.\n",
      "data = data[data[:,-1]<3,:]\n",
      "# Ici on transforme chaque etiquette qui est egale a 2 en -1, pour avoir les \n",
      "# m\u00eames \u00e9tiquettes que dans la formulation standard du perceptron (1 et -1).\n",
      "data[data[:,-1]==2,-1] = 0\n",
      "\n",
      "# On se limite \u00e0 des donn\u00e9es dont la dimension est 2, de fa\u00e7on \u00e0 pouvoir visualiser\n",
      "# la fronti\u00e8re de decision avec la fonction gridplot.\n",
      "train_cols = [2,3]\n",
      "# Une variable pour contenir l'indice de la colonne correspondant aux \u00e9tiquettes.\n",
      "target_ind = [data.shape[1] - 1]\n",
      "\n",
      "# Nombre de classes\n",
      "n_classes = 2\n",
      "# Nombre de points d'entrainement\n",
      "n_train = 75\n",
      "# Taille de la grille = grid_size x grid_size\n",
      "grid_size = 50\n",
      "\n",
      "print \"On va entrainer un perceptron sur \", n_train, \" exemples d'entrainement\"\n",
      "\n",
      "# decommenter pour avoir des resultats non-deterministes \n",
      "random.seed(3395)\n",
      "\n",
      "# D\u00e9terminer au hasard des indices pour les exemples d'entrainement et de test\n",
      "inds = range(data.shape[0])\n",
      "random.shuffle(inds)\n",
      "train_inds = inds[:n_train]\n",
      "test_inds = inds[n_train:]\n",
      "    \n",
      "# Separer les donnees dans les deux ensembles: entrainement et test.\n",
      "train_set = data[train_inds,:]\t# garder les bonnes lignes\n",
      "train_labels = (train_set[:, target_ind] == np.arange(n_classes)).astype(int)\n",
      "train_set = train_set[:,train_cols]  # garder les bonnes colonnes\n",
      "test_set = data[test_inds,:]\n",
      "test_labels = (test_set[:, target_ind] == np.arange(n_classes)).astype(int)\n",
      "test_set = test_set[:,train_cols]\n",
      "\n",
      "# Le taux d'apprentissage\n",
      "mu = 0.1\n",
      "\n",
      "# Cr\u00e9er et entrainer le modele\n",
      "model = TheanoLogisticRegression(mu, d=len(train_cols), n_classes=2)\n",
      "model.train(train_set, train_labels.argmax(1), batch_size=10, max_epoque=10)\n",
      "\n",
      "# Obtenir les sorties sur l'ensemble de test.\n",
      "t1 = time.clock()\n",
      "les_sorties = model.compute_predictions(test_set)\n",
      "t2 = time.clock()\n",
      "print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', test_set.shape[0],' points de test'\n",
      "\n",
      "# Convertir les sorties en classe. On prend le signe.\n",
      "classes_pred = les_sorties\n",
      "\n",
      "# Mesurer la performance.\n",
      "err = 1.0 - numpy.mean(test_labels.argmax(1)==classes_pred)\n",
      "print \"L'erreur de test est de \", 100.0 * err,\"%\"\n",
      "\n",
      "# Affichage graphique\n",
      "if len(train_cols) == 2:\n",
      "    # Surface de decision\n",
      "    t1 = time.clock()\n",
      "    utilitaires6.gridplot(model,train_set,train_labels,test_set,test_labels,n_points = grid_size)\n",
      "    t2 = time.clock()\n",
      "    print 'Ca nous a pris ', t2-t1, ' secondes pour calculer les predictions sur ', grid_size * grid_size, ' points de la grille'\n",
      "    filename = 'grille_' + '_c1=' + str(train_cols[0]) + '_c2=' + str(train_cols[1])+'.png'\n",
      "    print 'On va sauvegarder la figure dans ', filename\n",
      "    pylab.savefig(filename,format='png')\n",
      "        \n",
      "else:\n",
      "    print 'Trop de dimensions (', len(train_cols),') pour pouvoir afficher la surface de decision'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}