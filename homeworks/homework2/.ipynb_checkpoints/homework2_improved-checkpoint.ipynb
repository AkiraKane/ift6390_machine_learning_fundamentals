{
 "metadata": {
  "name": "",
  "signature": "sha256:7475301d40e938a4078166962b2f0eff831c94bd1c28fa7becc4c9c0d35489f7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "class MLP(object):\n",
      "    \"\"\"multilayer perceptron\"\"\"\n",
      "    def __init__(self, rng, input, n_input, n_hidden, n_output):\n",
      "\n",
      "        # hidden layer weights\n",
      "        self.W1 = theano.shared(\n",
      "        numpy.asarray(\n",
      "            rng.uniform(\n",
      "                        low=-numpy.sqrt(1. / n_input),\n",
      "                        high=numpy.sqrt(1. / n_input),\n",
      "                        size=(n_input, n_hidden)),\n",
      "            dtype=theano.config.floatX),\n",
      "            name='W1', borrow=True)\n",
      "        \n",
      "        # hidden layer biases\n",
      "        self.b1 = theano.shared(numpy.asarray(numpy.zeros(n_hidden,),\n",
      "                                dtype=theano.config.floatX),\n",
      "                                name='b1', borrow=True)\n",
      "        \n",
      "        # output layer weights\n",
      "        self.W2 = theano.shared(\n",
      "        numpy.asarray(\n",
      "            rng.uniform(\n",
      "                        low=-numpy.sqrt(1. / n_input),\n",
      "                        high=numpy.sqrt(1. / n_input),\n",
      "                        size=(n_hidden, n_output)),\n",
      "            dtype=theano.config.floatX),\n",
      "            name='W2', borrow=True)\n",
      "        \n",
      "        # output layer biases\n",
      "        self.b2 = theano.shared(numpy.asarray(numpy.zeros(n_output,),\n",
      "                                dtype=theano.config.floatX),\n",
      "                                name='b2', borrow=True)\n",
      "        \n",
      "        # prediction formula\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(T.tanh(T.dot(input, self.W1) + self.b1), self.W2) + self.b2)\n",
      "        \n",
      "        # predicted y from x\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        \n",
      "        # group all the parameters of the model\n",
      "        self.params = [self.W1, self.b1, self.W2, self.b2]\n",
      "        \n",
      "        # the penalty functions\n",
      "        self.L1 = abs(self.W1).sum() + abs(self.W2).sum()\n",
      "        self.L2 = (self.W1**2).sum() + (self.W2**2).sum()\n",
      "        \n",
      "\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      " \n",
      "    def errors(self, y):\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
      "            )\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data():\n",
      "    '''\n",
      "    Loads the data\n",
      "    '''\n",
      "\n",
      "    test_x = numpy.loadtxt('test_images.txt', delimiter=',')\n",
      "    test_y = numpy.argmax(numpy.loadtxt('test_labels.txt', delimiter=','), axis=1)\n",
      "    train_x = numpy.loadtxt('train_images.txt', delimiter=',')\n",
      "    train_y = numpy.argmax(numpy.loadtxt('train_labels.txt', delimiter=','), axis=1)\n",
      "\n",
      "\n",
      "    def shared_dataset(data_x, data_y, borrow=True):\n",
      "        \"\"\"\n",
      "        Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "    test_x, test_y = shared_dataset(test_x, test_y)\n",
      "    train_x, train_y = shared_dataset(train_x, train_y)\n",
      "\n",
      "    rval = (train_x, train_y, test_x, test_y)\n",
      "    return rval\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_test(n_epochs,\n",
      "             n_hidden,\n",
      "             learning_rate=0.01,\n",
      "             L1_reg=0.00,\n",
      "             L2_reg=0.0001,\n",
      "             batch_size=100,\n",
      "             check_gradients=False):\n",
      "    n_input = 28*28\n",
      "    n_output =10\n",
      "    \n",
      "    # only check the fist batch if debug\n",
      "    if check_gradients==True:\n",
      "        batch_size=1\n",
      "    \n",
      "    # load the data\n",
      "    datasets = load_data()\n",
      "    train_set_x, train_set_y, test_set_x, test_set_y = datasets\n",
      "\n",
      "    # compute number of minibatches for training and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    print '\\n... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a minibatch\n",
      "    x = T.matrix('x')    # the data is presented as rasterized images\n",
      "    y = T.ivector('y')   # the labels are presented as 1D vector of int labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(rng, x, n_input, n_hidden, n_output)\n",
      "\n",
      "    # minimize negative log likelihood of the model\n",
      "    # and the regularization terms (L1 and L2)\n",
      "    # cost is expressed symbolically\n",
      "    cost = (\n",
      "            classifier.negative_log_likelihood(y)\n",
      "            + L1_reg * classifier.L1\n",
      "            + L2_reg * classifier.L2\n",
      "            )\n",
      "\n",
      "    # returns the cost\n",
      "    ret_cost = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=cost,\n",
      "    givens={\n",
      "            x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "            }\n",
      "        )\n",
      "    # fit on train set\n",
      "    check_fit_train_set = theano.function(\n",
      "        inputs=[],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "                x: train_set_x,\n",
      "                y: train_set_y\n",
      "                }\n",
      "            )\n",
      "\n",
      "    # fit on test set\n",
      "    check_fit_test_set = theano.function(\n",
      "        inputs=[],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "                x: test_set_x,\n",
      "                y: test_set_y\n",
      "                }\n",
      "            )\n",
      "\n",
      "    # compile a Theano function that computes the mistakes that are made\n",
      "    # by the model on a minibatch of the train set (we'll see overfitting)\n",
      "    check_fit_batch = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "            x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "            }\n",
      "        )\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sorted in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gradient_w1 = T.grad(cost, classifier.W1)\n",
      "    gradient_b1 = T.grad(cost, classifier.b1)\n",
      "\n",
      "    gradient_w2 = T.grad(cost, classifier.W2)\n",
      "    gradient_b2 = T.grad(cost, classifier.b2)\n",
      "    \n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gradient)\n",
      "        for param, gradient in [(classifier.W1, gradient_w1),\n",
      "                                (classifier.b1, gradient_b1),\n",
      "                                (classifier.W2, gradient_w2),\n",
      "                                (classifier.b2, gradient_b2)]]\n",
      "    \n",
      "    values = [gradient_w1, gradient_b1, gradient_w2, gradient_b2]\n",
      "\n",
      "    ret_gradient = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=values,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    # updates the parameter of the model based on the rules defined in 'updates'\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=[],\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    print '... training'\n",
      "\n",
      "    best_iter = 0\n",
      "    test_scores = []\n",
      "    train_scores = []\n",
      "\n",
      "    epoch = 0\n",
      "    \n",
      "    epsilon = T.dscalar()\n",
      "    i = T.iscalar()\n",
      "    j = T.iscalar()\n",
      "    #(X, T.set_subtensor(X[i], T.inc_subtensor(X[i][j],epsilon)))\n",
      "    mod_w1 = theano.function([i,j,epsilon], updates=[(classifier.W1, T.set_subtensor(classifier.W1[i],T.inc_subtensor(classifier.W1[i][j], epsilon)))])\n",
      "    mod_b2 = theano.function([i, epsilon], updates=[(classifier.b1, T.inc_subtensor(classifier.b1[i], epsilon))])\n",
      "    #down_w1 = theano.function([i,j], updates=[(classifier.W1,[], T.inc_subtensor(classifier.W1[i][j], -2.*epsilon))])\n",
      "    #up_w2 = theano.function([i,j], updates=[(classifier.W2, T.inc_subtensor(classifier.W2[i][j], epsilon))])\n",
      "    #down_w2 = theano.function([i,j], updates=[(classifier.W2, T.inc_subtensor(classifier.W2[i][j], -2.*epsilon))])\n",
      "    \n",
      "    \n",
      "    while (epoch < n_epochs):\n",
      "\n",
      "        epoch = epoch + 1\n",
      "\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            train_model(minibatch_index)\n",
      "            if check_gradients==True:\n",
      "                symbolic_gradients = ret_gradient(minibatch_index)\n",
      "                \n",
      "                epsilon = 1E-4\n",
      "                a = numpy.zeros((n_input, n_hidden))\n",
      "                b = numpy.zeros((n_hidden,))\n",
      "                c = numpy.zeros((n_hidden, n_output))\n",
      "                d = numpy.zeros((n_output,))\n",
      "                grad=  [a,b,c,d]\n",
      "                \n",
      "                # b1\n",
      "                b1_vals = classifier.b1.get_value()\n",
      "                for i in range(len(grad[1])):\n",
      "                    # compute low\n",
      "                    b1_vals[i] -= epsilon\n",
      "                    classifier.b1.set_value(b1_vals, borrow=True)\n",
      "                    low= ret_cost(minibatch_index)\n",
      "                    \n",
      "                    # compute high\n",
      "                    b1_vals[i] += 2.*epsilon\n",
      "                    classifier.b1.set_value(b1_vals, borrow=True)\n",
      "                    high= ret_cost(minibatch_index)\n",
      "                    \n",
      "                    # reset the value\n",
      "                    b1_vals[i] -= epsilon\n",
      "                    # store the gradient\n",
      "                    grad[1][i] = (high - low) / (2.*epsilon)\n",
      "\n",
      "                # b2\n",
      "                b2_vals = classifier.b2.get_value()\n",
      "                for i in range(len(grad[3])):\n",
      "                    # compute low                    \n",
      "                    b2_vals[i] -= epsilon\n",
      "                    classifier.b2.set_value(b2_vals, borrow=True)\n",
      "                    low= ret_cost(minibatch_index)\n",
      "                    \n",
      "                    # compute high\n",
      "                    b2_vals[i] += 2.*epsilon\n",
      "                    classifier.b2.set_value(b2_vals, borrow=True)\n",
      "                    high= ret_cost(minibatch_index)\n",
      "                    \n",
      "                    # reset the value\n",
      "                    b2_vals[i] -= epsilon\n",
      "\n",
      "                    # store the gradient\n",
      "                    grad[3][i] = (high - low) / (2.*epsilon)\n",
      "          \n",
      "                \n",
      "                # W1\n",
      "                w1_vals = classifier.W1.get_value()\n",
      "                for i in range(len(grad[0])):\n",
      "                    for j in range(len(grad[0][0])):\n",
      "                        \n",
      "                        # compute low                    \n",
      "                        w1_vals[i][j] -= epsilon\n",
      "                        classifier.W1.set_value(w1_vals, borrow=True)\n",
      "                        low= ret_cost(minibatch_index)\n",
      "                    \n",
      "                        # compute high\n",
      "                        w1_vals[i][j] += 2.*epsilon\n",
      "                        classifier.W1.set_value(w1_vals, borrow=True)\n",
      "                        high= ret_cost(minibatch_index)\n",
      "                    \n",
      "                        # reset the value\n",
      "                        w1_vals[i][j] -= epsilon\n",
      "\n",
      "                        # store the gradient\n",
      "                        grad[0][i][j] = (high - low) / (2.*epsilon)\n",
      "                \n",
      "                # W2\n",
      "                w2_vals = classifier.W2.get_value()\n",
      "                for i in range(len(grad[2])):\n",
      "                    for j in range(len(grad[2][0])):\n",
      "                        \n",
      "                        # compute low                    \n",
      "                        w2_vals[i][j] -= epsilon\n",
      "                        classifier.W2.set_value(w2_vals, borrow=True)\n",
      "                        low= ret_cost(minibatch_index)\n",
      "                    \n",
      "                        # compute high\n",
      "                        w2_vals[i][j] += 2.*epsilon\n",
      "                        classifier.W2.set_value(w2_vals, borrow=True)\n",
      "                        high= ret_cost(minibatch_index)\n",
      "                    \n",
      "                        # reset the value\n",
      "                        w2_vals[i][j] -= epsilon\n",
      "\n",
      "                        # store the gradient\n",
      "                        grad[2][i][j] = (high - low) / (2.*epsilon)\n",
      "                        \n",
      "                return (symbolic_gradients, grad)\n",
      "\n",
      "        # verify the fit on the datasets\n",
      "        train_scores.append(check_fit_train_set())\n",
      "        test_scores.append(check_fit_test_set())\n",
      "            \n",
      "\n",
      "    print '... done\\n'\n",
      "    return (train_scores, test_scores)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = run_test(1, 50, check_gradients=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print a[0][0] / a[1][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.00000029  1.00000039  1.00000025 ...,  1.00000016  1.00000025\n",
        "   0.99998516]\n",
        " [ 0.99999995  1.00000035  1.00000026 ...,  0.99999982  1.00000018\n",
        "   0.99999983]\n",
        " [ 0.9999997   0.99999999  0.99999982 ...,  0.99999994  1.00000137\n",
        "   0.99999985]\n",
        " ..., \n",
        " [ 0.99999974  0.99999981  0.99999989 ...,  1.          0.99999992\n",
        "   1.00000001]\n",
        " [ 0.99999977  1.00000084  1.00000005 ...,  1.00000028  0.9999998\n",
        "   0.99999984]\n",
        " [ 0.99999988  1.00000001  1.00000151 ...,  1.00000019  1.00000012\n",
        "   1.00000023]]\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(((x.T \\dot ((SoftmaxGrad((AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}(fill(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), TensorConstant{0.0}), fill(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y), ((-fill((((-(Sum{acc_dtype=float64}(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])) + (TensorConstant{0.0} * (Sum{acc_dtype=float64}(|W1|) + Sum{acc_dtype=float64}(|W2|)))) + (TensorConstant{0.0001} * (Sum{acc_dtype=float64}((W1 ** TensorConstant{2})) + Sum{acc_dtype=float64}((W2 ** TensorConstant{2}))))), TensorConstant{1.0})) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y) / Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))) \\dot W2.T) * (TensorConstant{1} - sqr(tanh(((x \\dot W1) + b1)))))) + ((fill(|W1|, (fill((((-(Sum{acc_dtype=float64}(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])) + (TensorConstant{0.0} * (Sum{acc_dtype=float64}(|W1|) + Sum{acc_dtype=float64}(|W2|)))) + (TensorConstant{0.0001} * (Sum{acc_dtype=float64}((W1 ** TensorConstant{2})) + Sum{acc_dtype=float64}((W2 ** TensorConstant{2}))))), TensorConstant{1.0}) * TensorConstant{0.0})) * W1) / |W1|)) + ((fill((W1 ** TensorConstant{2}), (fill((((-(Sum{acc_dtype=float64}(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])) + (TensorConstant{0.0} * (Sum{acc_dtype=float64}(|W1|) + Sum{acc_dtype=float64}(|W2|)))) + (TensorConstant{0.0001} * (Sum{acc_dtype=float64}((W1 ** TensorConstant{2})) + Sum{acc_dtype=float64}((W2 ** TensorConstant{2}))))), TensorConstant{1.0}) * TensorConstant{0.0001})) * TensorConstant{2}) * (W1 ** (TensorConstant{2} - TensorConstant{1}))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.dscalar('x')\n",
      "y = x ** 2\n",
      "gy = T.grad(y, x)\n",
      "f = theano.function([x],gy)\n",
      "print theano.pp(gy)\n",
      "\n",
      "f(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "array(4.0)"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano import tensor as T\n",
      "from theano import function, shared\n",
      "import numpy\n",
      "\n",
      "X = shared(numpy.array([[0.,1,2,3,4],[5,6,7,8,9]]))\n",
      "epsilon = T.dscalar()\n",
      "i = T.iscalar()\n",
      "j = T.iscalar()\n",
      "X_update = (X, T.set_subtensor(X[i], T.inc_subtensor(X[i][j],epsilon)))\n",
      "f = function([i,j, epsilon], updates=[X_update])\n",
      "f(0,3, 2.4)\n",
      "print X.get_value() # [0 1 100 10 4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.   1.   2.   5.4  4. ]\n",
        " [ 5.   6.   7.   8.   9. ]]\n"
       ]
      }
     ],
     "prompt_number": 93
    }
   ],
   "metadata": {}
  }
 ]
}