{
 "metadata": {
  "name": "",
  "signature": "sha256:eb62ded6b697be9ef13cabbcd632d88212c92ccb9ab69bbaf30ed6c56ff084ce"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "class MLP(object):\n",
      "    \"\"\"multilayer perceptron\"\"\"\n",
      "    def __init__(self, rng, input, n_input, n_hidden, n_output):\n",
      "        # hidden layer weights\n",
      "        self.W1 = theano.shared(\n",
      "        numpy.asarray(\n",
      "            rng.uniform(\n",
      "                        low=-numpy.sqrt(1. / n_input),\n",
      "                        high=numpy.sqrt(1. / n_input),\n",
      "                        size=(n_input, n_hidden)),\n",
      "            dtype=theano.config.floatX),\n",
      "            name='W1', borrow=True)\n",
      "        \n",
      "        # hidden layer biases\n",
      "        self.b1 = theano.shared(numpy.asarray(numpy.zeros(n_hidden,),\n",
      "                                dtype=theano.config.floatX),\n",
      "                                name='b1', borrow=True)\n",
      "        \n",
      "        # output layer weights\n",
      "        self.W2 = theano.shared(\n",
      "        numpy.asarray(\n",
      "            rng.uniform(\n",
      "                        low=-numpy.sqrt(1. / n_input),\n",
      "                        high=numpy.sqrt(1. / n_input),\n",
      "                        size=(n_hidden, n_output)),\n",
      "            dtype=theano.config.floatX),\n",
      "            name='W2', borrow=True)\n",
      "        \n",
      "        # output layer biases\n",
      "        self.b2 = theano.shared(numpy.asarray(numpy.zeros(n_output,),\n",
      "                                dtype=theano.config.floatX),\n",
      "                                name='b2', borrow=True)\n",
      "        \n",
      "        # prediction formula\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(T.tanh(T.dot(input, self.W1) + self.b1), self.W2) + self.b2)\n",
      "        \n",
      "        # predicted y from x\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "        \n",
      "        # group all the parameters of the model\n",
      "        self.params = [self.W1, self.b1, self.W2, self.b2]\n",
      "        \n",
      "        # the penalty functions\n",
      "        self.L1 = abs(self.W1).sum() + abs(self.W2).sum()\n",
      "        self.L2 = (self.W1**2).sum() + (self.W2**2).sum()\n",
      "        \n",
      "\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      " \n",
      "    def errors(self, y):\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
      "            )\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data():\n",
      "    '''\n",
      "    Loads the data\n",
      "    '''\n",
      "\n",
      "    test_x = numpy.loadtxt('test_images.txt', delimiter=',')\n",
      "    test_y = numpy.argmax(numpy.loadtxt('test_labels.txt', delimiter=','), axis=1)\n",
      "    train_x = numpy.loadtxt('train_images.txt', delimiter=',')\n",
      "    train_y = numpy.argmax(numpy.loadtxt('train_labels.txt', delimiter=','), axis=1)\n",
      "\n",
      "\n",
      "    def shared_dataset(data_x, data_y, borrow=True):\n",
      "        \"\"\"\n",
      "        Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "    test_x, test_y = shared_dataset(test_x, test_y)\n",
      "    train_x, train_y = shared_dataset(train_x, train_y)\n",
      "\n",
      "    rval = (train_x, train_y, test_x, test_y)\n",
      "    return rval\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = load_data()\n",
      "print a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0), (<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0)]\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_test(n_epochs,\n",
      "             n_hidden,\n",
      "             learning_rate=0.01,\n",
      "             L1_reg=0.00,\n",
      "             L2_reg=0.0001,\n",
      "             batch_size=100,\n",
      "             debug=False):\n",
      "\n",
      "    # load the data\n",
      "    datasets = load_data()\n",
      "    train_set_x, train_set_y, test_set_x, test_set_y = datasets\n",
      "\n",
      "    # compute number of minibatches for training and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    print '\\n... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a minibatch\n",
      "    x = T.matrix('x')    # the data is presented as rasterized images\n",
      "    y = T.ivector('y')   # the labels are presented as 1D vector of int labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(rng, x, 28 * 28, n_hidden, 10)\n",
      "\n",
      "    # minimize negative log likelihood of the model\n",
      "    # and the regularization terms (L1 and L2)\n",
      "    # cost is expressed symbolically\n",
      "    cost = (\n",
      "            classifier.negative_log_likelihood(y)\n",
      "            + L1_reg * classifier.L1\n",
      "            + L2_reg * classifier.L2\n",
      "            )\n",
      "\n",
      "    # fit on train set\n",
      "    check_fit_train_set = theano.function(\n",
      "        inputs=[],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "                x: train_set_x,\n",
      "                y: train_set_y\n",
      "                }\n",
      "            )\n",
      "\n",
      "    # fit on test set\n",
      "    check_fit_test_set = theano.function(\n",
      "        inputs=[],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "                x: test_set_x,\n",
      "                y: test_set_y\n",
      "                }\n",
      "            )\n",
      "\n",
      "    # compile a Theano function that computes the mistakes that are made\n",
      "    # by the model on a minibatch of the train set (we'll see overfitting)\n",
      "    check_fit_batch = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "            x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "            }\n",
      "        )\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sorted in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gradient_w1 = T.grad(cost, classifier.W1)\n",
      "    gradient_b1 = T.grad(cost, classifier.b1)\n",
      "\n",
      "    gradient_w2 = T.grad(cost, classifier.W2)\n",
      "    gradient_b2 = T.grad(cost, classifier.b2)\n",
      "    \n",
      "    #x = T.dscalar('x')\n",
      "    #y = x ** 2\n",
      "    #gy = T.grad(y, x)\n",
      "    #f = theano.function([x],gy)\n",
      "    #print theano.pp(gy)\n",
      "    #f(2)\n",
      "    \n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gradient)\n",
      "        for param, gradient in [(classifier.W1, gradient_w1),\n",
      "                                (classifier.b1, gradient_b1),\n",
      "                                (classifier.W2, gradient_w2),\n",
      "                                (classifier.b2, gradient_b2)]\n",
      "    ]\n",
      "    \n",
      "    values = [gradient_w1, gradient_b1, gradient_w2, gradient_b2]\n",
      "\n",
      "    ret_gradient = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=values,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    # updates the parameter of the model based on the rules defined in 'updates'\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=[],\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    print '... training'\n",
      "\n",
      "    best_iter = 0\n",
      "    test_scores = []\n",
      "    train_scores = []\n",
      "\n",
      "    epoch = 0\n",
      "\n",
      "    while (epoch < n_epochs):\n",
      "\n",
      "        epoch = epoch + 1\n",
      "\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            train_model(minibatch_index)\n",
      "\n",
      "        # verify the fit on the datasets\n",
      "        train_scores.append(check_fit_train_set())\n",
      "        test_scores.append(check_fit_test_set())\n",
      "            \n",
      "\n",
      "    print '... done\\n'\n",
      "    return (train_scores, test_scores)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_test(300, 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 157,
       "text": [
        "([array(0.88),\n",
        "  array(0.848),\n",
        "  array(0.79),\n",
        "  array(0.743),\n",
        "  array(0.687),\n",
        "  array(0.651),\n",
        "  array(0.623),\n",
        "  array(0.593),\n",
        "  array(0.554),\n",
        "  array(0.532),\n",
        "  array(0.506),\n",
        "  array(0.484),\n",
        "  array(0.469),\n",
        "  array(0.454),\n",
        "  array(0.435),\n",
        "  array(0.426),\n",
        "  array(0.422),\n",
        "  array(0.419),\n",
        "  array(0.414),\n",
        "  array(0.402),\n",
        "  array(0.392),\n",
        "  array(0.388),\n",
        "  array(0.385),\n",
        "  array(0.379),\n",
        "  array(0.379),\n",
        "  array(0.375),\n",
        "  array(0.376),\n",
        "  array(0.372),\n",
        "  array(0.371),\n",
        "  array(0.364),\n",
        "  array(0.361),\n",
        "  array(0.363),\n",
        "  array(0.361),\n",
        "  array(0.362),\n",
        "  array(0.363),\n",
        "  array(0.365),\n",
        "  array(0.361),\n",
        "  array(0.363),\n",
        "  array(0.364),\n",
        "  array(0.362),\n",
        "  array(0.361),\n",
        "  array(0.358),\n",
        "  array(0.355),\n",
        "  array(0.352),\n",
        "  array(0.352),\n",
        "  array(0.351),\n",
        "  array(0.348),\n",
        "  array(0.343),\n",
        "  array(0.34),\n",
        "  array(0.339),\n",
        "  array(0.332),\n",
        "  array(0.329),\n",
        "  array(0.326),\n",
        "  array(0.326),\n",
        "  array(0.319),\n",
        "  array(0.314),\n",
        "  array(0.312),\n",
        "  array(0.309),\n",
        "  array(0.306),\n",
        "  array(0.3),\n",
        "  array(0.294),\n",
        "  array(0.291),\n",
        "  array(0.287),\n",
        "  array(0.285),\n",
        "  array(0.282),\n",
        "  array(0.276),\n",
        "  array(0.277),\n",
        "  array(0.275),\n",
        "  array(0.271),\n",
        "  array(0.268),\n",
        "  array(0.266),\n",
        "  array(0.263),\n",
        "  array(0.258),\n",
        "  array(0.25),\n",
        "  array(0.248),\n",
        "  array(0.241),\n",
        "  array(0.239),\n",
        "  array(0.236),\n",
        "  array(0.229),\n",
        "  array(0.227),\n",
        "  array(0.223),\n",
        "  array(0.22),\n",
        "  array(0.217),\n",
        "  array(0.215),\n",
        "  array(0.213),\n",
        "  array(0.21),\n",
        "  array(0.202),\n",
        "  array(0.199),\n",
        "  array(0.193),\n",
        "  array(0.191),\n",
        "  array(0.19),\n",
        "  array(0.187),\n",
        "  array(0.184),\n",
        "  array(0.177),\n",
        "  array(0.173),\n",
        "  array(0.172),\n",
        "  array(0.17),\n",
        "  array(0.17),\n",
        "  array(0.17),\n",
        "  array(0.169),\n",
        "  array(0.166),\n",
        "  array(0.165),\n",
        "  array(0.162),\n",
        "  array(0.162),\n",
        "  array(0.162),\n",
        "  array(0.159),\n",
        "  array(0.157),\n",
        "  array(0.155),\n",
        "  array(0.154),\n",
        "  array(0.154),\n",
        "  array(0.154),\n",
        "  array(0.153),\n",
        "  array(0.151),\n",
        "  array(0.15),\n",
        "  array(0.149),\n",
        "  array(0.146),\n",
        "  array(0.146),\n",
        "  array(0.146),\n",
        "  array(0.143),\n",
        "  array(0.142),\n",
        "  array(0.141),\n",
        "  array(0.141),\n",
        "  array(0.139),\n",
        "  array(0.138),\n",
        "  array(0.138),\n",
        "  array(0.137),\n",
        "  array(0.135),\n",
        "  array(0.134),\n",
        "  array(0.132),\n",
        "  array(0.13),\n",
        "  array(0.129),\n",
        "  array(0.127),\n",
        "  array(0.127),\n",
        "  array(0.127),\n",
        "  array(0.126),\n",
        "  array(0.125),\n",
        "  array(0.125),\n",
        "  array(0.125),\n",
        "  array(0.123),\n",
        "  array(0.123),\n",
        "  array(0.123),\n",
        "  array(0.123),\n",
        "  array(0.123),\n",
        "  array(0.121),\n",
        "  array(0.12),\n",
        "  array(0.12),\n",
        "  array(0.12),\n",
        "  array(0.119),\n",
        "  array(0.118),\n",
        "  array(0.118),\n",
        "  array(0.118),\n",
        "  array(0.116),\n",
        "  array(0.116),\n",
        "  array(0.116),\n",
        "  array(0.117),\n",
        "  array(0.117),\n",
        "  array(0.117),\n",
        "  array(0.116),\n",
        "  array(0.116),\n",
        "  array(0.116),\n",
        "  array(0.116),\n",
        "  array(0.116),\n",
        "  array(0.115),\n",
        "  array(0.115),\n",
        "  array(0.115),\n",
        "  array(0.114),\n",
        "  array(0.112),\n",
        "  array(0.112),\n",
        "  array(0.112),\n",
        "  array(0.112),\n",
        "  array(0.111),\n",
        "  array(0.11),\n",
        "  array(0.109),\n",
        "  array(0.109),\n",
        "  array(0.108),\n",
        "  array(0.107),\n",
        "  array(0.105),\n",
        "  array(0.105),\n",
        "  array(0.104),\n",
        "  array(0.104),\n",
        "  array(0.103),\n",
        "  array(0.102),\n",
        "  array(0.1),\n",
        "  array(0.1),\n",
        "  array(0.1),\n",
        "  array(0.1),\n",
        "  array(0.099),\n",
        "  array(0.098),\n",
        "  array(0.098),\n",
        "  array(0.098),\n",
        "  array(0.097),\n",
        "  array(0.096),\n",
        "  array(0.096),\n",
        "  array(0.096),\n",
        "  array(0.096),\n",
        "  array(0.096),\n",
        "  array(0.096),\n",
        "  array(0.096),\n",
        "  array(0.095),\n",
        "  array(0.094),\n",
        "  array(0.094),\n",
        "  array(0.094),\n",
        "  array(0.093),\n",
        "  array(0.093),\n",
        "  array(0.092),\n",
        "  array(0.092),\n",
        "  array(0.091),\n",
        "  array(0.09),\n",
        "  array(0.09),\n",
        "  array(0.088),\n",
        "  array(0.088),\n",
        "  array(0.088),\n",
        "  array(0.085),\n",
        "  array(0.082),\n",
        "  array(0.082),\n",
        "  array(0.082),\n",
        "  array(0.082),\n",
        "  array(0.081),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.08),\n",
        "  array(0.079),\n",
        "  array(0.079),\n",
        "  array(0.079),\n",
        "  array(0.077),\n",
        "  array(0.077),\n",
        "  array(0.076),\n",
        "  array(0.075),\n",
        "  array(0.075),\n",
        "  array(0.074),\n",
        "  array(0.073),\n",
        "  array(0.072),\n",
        "  array(0.072),\n",
        "  array(0.072),\n",
        "  array(0.072),\n",
        "  array(0.072),\n",
        "  array(0.071),\n",
        "  array(0.071),\n",
        "  array(0.07),\n",
        "  array(0.07),\n",
        "  array(0.07),\n",
        "  array(0.07),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.069),\n",
        "  array(0.068),\n",
        "  array(0.068),\n",
        "  array(0.067),\n",
        "  array(0.067),\n",
        "  array(0.067),\n",
        "  array(0.065),\n",
        "  array(0.065),\n",
        "  array(0.065),\n",
        "  array(0.064),\n",
        "  array(0.064),\n",
        "  array(0.062),\n",
        "  array(0.062),\n",
        "  array(0.062),\n",
        "  array(0.062),\n",
        "  array(0.062),\n",
        "  array(0.061),\n",
        "  array(0.06),\n",
        "  array(0.06),\n",
        "  array(0.06),\n",
        "  array(0.06),\n",
        "  array(0.06),\n",
        "  array(0.059),\n",
        "  array(0.059),\n",
        "  array(0.059),\n",
        "  array(0.058),\n",
        "  array(0.058),\n",
        "  array(0.057),\n",
        "  array(0.057),\n",
        "  array(0.057),\n",
        "  array(0.057),\n",
        "  array(0.057),\n",
        "  array(0.057),\n",
        "  array(0.057),\n",
        "  array(0.056),\n",
        "  array(0.056),\n",
        "  array(0.053),\n",
        "  array(0.052),\n",
        "  array(0.052),\n",
        "  array(0.052),\n",
        "  array(0.052)],\n",
        " [array(0.895),\n",
        "  array(0.86),\n",
        "  array(0.809),\n",
        "  array(0.766),\n",
        "  array(0.72),\n",
        "  array(0.686),\n",
        "  array(0.654),\n",
        "  array(0.625),\n",
        "  array(0.599),\n",
        "  array(0.566),\n",
        "  array(0.533),\n",
        "  array(0.51),\n",
        "  array(0.487),\n",
        "  array(0.468),\n",
        "  array(0.455),\n",
        "  array(0.448),\n",
        "  array(0.444),\n",
        "  array(0.433),\n",
        "  array(0.426),\n",
        "  array(0.42),\n",
        "  array(0.417),\n",
        "  array(0.411),\n",
        "  array(0.407),\n",
        "  array(0.401),\n",
        "  array(0.397),\n",
        "  array(0.394),\n",
        "  array(0.393),\n",
        "  array(0.385),\n",
        "  array(0.383),\n",
        "  array(0.383),\n",
        "  array(0.377),\n",
        "  array(0.374),\n",
        "  array(0.367),\n",
        "  array(0.367),\n",
        "  array(0.365),\n",
        "  array(0.364),\n",
        "  array(0.363),\n",
        "  array(0.365),\n",
        "  array(0.364),\n",
        "  array(0.363),\n",
        "  array(0.362),\n",
        "  array(0.363),\n",
        "  array(0.361),\n",
        "  array(0.359),\n",
        "  array(0.357),\n",
        "  array(0.352),\n",
        "  array(0.349),\n",
        "  array(0.346),\n",
        "  array(0.346),\n",
        "  array(0.346),\n",
        "  array(0.337),\n",
        "  array(0.338),\n",
        "  array(0.334),\n",
        "  array(0.329),\n",
        "  array(0.322),\n",
        "  array(0.317),\n",
        "  array(0.314),\n",
        "  array(0.311),\n",
        "  array(0.304),\n",
        "  array(0.302),\n",
        "  array(0.299),\n",
        "  array(0.297),\n",
        "  array(0.292),\n",
        "  array(0.291),\n",
        "  array(0.284),\n",
        "  array(0.282),\n",
        "  array(0.277),\n",
        "  array(0.273),\n",
        "  array(0.267),\n",
        "  array(0.266),\n",
        "  array(0.26),\n",
        "  array(0.259),\n",
        "  array(0.257),\n",
        "  array(0.255),\n",
        "  array(0.25),\n",
        "  array(0.249),\n",
        "  array(0.247),\n",
        "  array(0.243),\n",
        "  array(0.243),\n",
        "  array(0.237),\n",
        "  array(0.236),\n",
        "  array(0.229),\n",
        "  array(0.227),\n",
        "  array(0.226),\n",
        "  array(0.225),\n",
        "  array(0.224),\n",
        "  array(0.223),\n",
        "  array(0.221),\n",
        "  array(0.217),\n",
        "  array(0.215),\n",
        "  array(0.213),\n",
        "  array(0.212),\n",
        "  array(0.211),\n",
        "  array(0.208),\n",
        "  array(0.206),\n",
        "  array(0.207),\n",
        "  array(0.207),\n",
        "  array(0.207),\n",
        "  array(0.204),\n",
        "  array(0.202),\n",
        "  array(0.201),\n",
        "  array(0.2),\n",
        "  array(0.199),\n",
        "  array(0.198),\n",
        "  array(0.195),\n",
        "  array(0.194),\n",
        "  array(0.194),\n",
        "  array(0.19),\n",
        "  array(0.189),\n",
        "  array(0.189),\n",
        "  array(0.189),\n",
        "  array(0.189),\n",
        "  array(0.188),\n",
        "  array(0.187),\n",
        "  array(0.187),\n",
        "  array(0.185),\n",
        "  array(0.184),\n",
        "  array(0.184),\n",
        "  array(0.182),\n",
        "  array(0.182),\n",
        "  array(0.181),\n",
        "  array(0.182),\n",
        "  array(0.182),\n",
        "  array(0.182),\n",
        "  array(0.181),\n",
        "  array(0.181),\n",
        "  array(0.182),\n",
        "  array(0.182),\n",
        "  array(0.181),\n",
        "  array(0.181),\n",
        "  array(0.18),\n",
        "  array(0.179),\n",
        "  array(0.179),\n",
        "  array(0.178),\n",
        "  array(0.177),\n",
        "  array(0.176),\n",
        "  array(0.176),\n",
        "  array(0.176),\n",
        "  array(0.174),\n",
        "  array(0.174),\n",
        "  array(0.174),\n",
        "  array(0.173),\n",
        "  array(0.173),\n",
        "  array(0.174),\n",
        "  array(0.174),\n",
        "  array(0.173),\n",
        "  array(0.172),\n",
        "  array(0.172),\n",
        "  array(0.172),\n",
        "  array(0.171),\n",
        "  array(0.17),\n",
        "  array(0.17),\n",
        "  array(0.168),\n",
        "  array(0.169),\n",
        "  array(0.169),\n",
        "  array(0.169),\n",
        "  array(0.169),\n",
        "  array(0.169),\n",
        "  array(0.169),\n",
        "  array(0.169),\n",
        "  array(0.167),\n",
        "  array(0.166),\n",
        "  array(0.165),\n",
        "  array(0.165),\n",
        "  array(0.165),\n",
        "  array(0.165),\n",
        "  array(0.165),\n",
        "  array(0.164),\n",
        "  array(0.163),\n",
        "  array(0.163),\n",
        "  array(0.163),\n",
        "  array(0.164),\n",
        "  array(0.163),\n",
        "  array(0.163),\n",
        "  array(0.162),\n",
        "  array(0.162),\n",
        "  array(0.162),\n",
        "  array(0.161),\n",
        "  array(0.16),\n",
        "  array(0.16),\n",
        "  array(0.159),\n",
        "  array(0.158),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.158),\n",
        "  array(0.158),\n",
        "  array(0.158),\n",
        "  array(0.159),\n",
        "  array(0.159),\n",
        "  array(0.159),\n",
        "  array(0.158),\n",
        "  array(0.158),\n",
        "  array(0.158),\n",
        "  array(0.157),\n",
        "  array(0.157),\n",
        "  array(0.154),\n",
        "  array(0.154),\n",
        "  array(0.154),\n",
        "  array(0.154),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.153),\n",
        "  array(0.152),\n",
        "  array(0.152),\n",
        "  array(0.151),\n",
        "  array(0.151),\n",
        "  array(0.149),\n",
        "  array(0.149),\n",
        "  array(0.149),\n",
        "  array(0.148),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.147),\n",
        "  array(0.146),\n",
        "  array(0.145),\n",
        "  array(0.145),\n",
        "  array(0.144),\n",
        "  array(0.144),\n",
        "  array(0.141),\n",
        "  array(0.14),\n",
        "  array(0.14),\n",
        "  array(0.139),\n",
        "  array(0.139),\n",
        "  array(0.14),\n",
        "  array(0.14),\n",
        "  array(0.14),\n",
        "  array(0.14),\n",
        "  array(0.14),\n",
        "  array(0.14),\n",
        "  array(0.139),\n",
        "  array(0.139),\n",
        "  array(0.139),\n",
        "  array(0.138),\n",
        "  array(0.138),\n",
        "  array(0.138),\n",
        "  array(0.136),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.137),\n",
        "  array(0.136),\n",
        "  array(0.136),\n",
        "  array(0.136),\n",
        "  array(0.135),\n",
        "  array(0.135),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.134),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.133),\n",
        "  array(0.132),\n",
        "  array(0.132),\n",
        "  array(0.132),\n",
        "  array(0.131),\n",
        "  array(0.131),\n",
        "  array(0.131),\n",
        "  array(0.131),\n",
        "  array(0.131),\n",
        "  array(0.13),\n",
        "  array(0.13),\n",
        "  array(0.13),\n",
        "  array(0.13)])"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(((x.T \\dot ((SoftmaxGrad((AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}(fill(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), TensorConstant{0.0}), fill(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y), ((-fill((((-(Sum{acc_dtype=float64}(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])) + (TensorConstant{0.0} * (Sum{acc_dtype=float64}(|W1|) + Sum{acc_dtype=float64}(|W2|)))) + (TensorConstant{0.0001} * (Sum{acc_dtype=float64}((W1 ** TensorConstant{2})) + Sum{acc_dtype=float64}((W2 ** TensorConstant{2}))))), TensorConstant{1.0})) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y) / Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))) \\dot W2.T) * (TensorConstant{1} - sqr(tanh(((x \\dot W1) + b1)))))) + ((fill(|W1|, (fill((((-(Sum{acc_dtype=float64}(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])) + (TensorConstant{0.0} * (Sum{acc_dtype=float64}(|W1|) + Sum{acc_dtype=float64}(|W2|)))) + (TensorConstant{0.0001} * (Sum{acc_dtype=float64}((W1 ** TensorConstant{2})) + Sum{acc_dtype=float64}((W2 ** TensorConstant{2}))))), TensorConstant{1.0}) * TensorConstant{0.0})) * W1) / |W1|)) + ((fill((W1 ** TensorConstant{2}), (fill((((-(Sum{acc_dtype=float64}(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)) / Constant{0}[float64(Shape(AdvancedSubtensor(log(Softmax(((tanh(((x \\dot W1) + b1)) \\dot W2) + b2))), ARange(TensorConstant{0}, Constant{0}[Shape(y)], TensorConstant{1}), y)))])) + (TensorConstant{0.0} * (Sum{acc_dtype=float64}(|W1|) + Sum{acc_dtype=float64}(|W2|)))) + (TensorConstant{0.0001} * (Sum{acc_dtype=float64}((W1 ** TensorConstant{2})) + Sum{acc_dtype=float64}((W2 ** TensorConstant{2}))))), TensorConstant{1.0}) * TensorConstant{0.0001})) * TensorConstant{2}) * (W1 ** (TensorConstant{2} - TensorConstant{1}))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.dscalar('x')\n",
      "y = x ** 2\n",
      "gy = T.grad(y, x)\n",
      "f = theano.function([x],gy)\n",
      "print theano.pp(gy)\n",
      "\n",
      "f(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 117,
       "text": [
        "array(4.0)"
       ]
      }
     ],
     "prompt_number": 117
    }
   ],
   "metadata": {}
  }
 ]
}