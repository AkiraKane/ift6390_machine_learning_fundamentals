{
 "metadata": {
  "name": "",
  "signature": "sha256:77cb2a69ce64e82cc6e48d4462cc2b39227383e1b251b204bc2d30317935b208"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Homework 2: multilayer perceptron (single hidden layer)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## parameters\n",
      "\n",
      "- 3 layers (input -> hidden -> output)\n",
      "    - input: 784 neurons (1 for each pixel)\n",
      "    - hidden units: not fixed\n",
      "    - output: 10 neurons (1 for each digit)\n",
      "\n",
      "- tanh(sum(input)) for each layer\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 a)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the dimension of $b^{(1)}$ ?**\n",
      "\n",
      "$b^{(1)}$ is $d_h \\times 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula to calculate the vector of activations**\n",
      "**(i.e. before the non-linearity) of the neurons in the hidden layer,**\n",
      "**$h^a$ , given an input, x, at first in matrix expression.**\n",
      "\n",
      "$h^a_{d_h \\times 1} = b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} x_{d \\times 1} $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Element-by-element computation of the entries of $h^a$.**\n",
      "\n",
      "$h^a_i = b^{(1)}_i + \\sum\\limits_{j=1}^d w^{(1)}_{i, j}x_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the vector of outputs of the hidden layer neurons, $h^s$, in terms of the activations, $h^a$.**\n",
      "\n",
      "$h^s_{d_h \\times 1} = tanh(b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} x_{d \\times 1}) $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 b)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Let $W^{(2)}$ be the weight matrix from the hidden to output layer and $b^{(2)}$ be the**\n",
      "**vector of biases for the output layer. What are the dimensions of $W^{(2)}$ et $b^{(2)}$ ?**\n",
      "\n",
      "$b^{(2)}$ is $m \\times 1$\n",
      "\n",
      "$W^{(2)}$ is $m \\times d_h$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula describing the vector of activations of neurons in the output**\n",
      "**layer $o^a$ given $h^s$ in matrix form**\n",
      "\n",
      "$o^a_{m \\times 1} = b^{(2)}_{m \\times 1} + W^{(2)}_{m \\times d_h} h^s_{d_h \\times 1}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Then in detail element-wise form**\n",
      "\n",
      "$o^a_i = b^{(2)}_i + \\sum\\limits_{j=1}^{d_h} w^{(2)}_{i, j} h^s_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Q1 c)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is contained in the set of all network parameters, $\\theta$**\n",
      "\n",
      "- activation function (tanh, sigmoid, linear even)\n",
      "- number of hidden layer nodes\n",
      "- $W^{(1)}, W^{(2)}, b^{(1)}, b^{(2)}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the number $n_{\\theta}$ of parameters in $\\theta$ ?**\n",
      "\n",
      "- $W^{(1)}$ is $d_h \\times d$\n",
      "- $W^{(2)}$  is $m \\times d_h$\n",
      "- $b^{(1)}$  is $d_h \\times 1$\n",
      "- $b^{(2)}$  is $m \\times 1$\n",
      "\n",
      "$n_{\\theta}$ =  $d_h * (d+1 + m) + m$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 d)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Show that the gradients wrt. parameters $W^{(2)}$ and $b^{(2)}$ are given by:**\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T $\n",
      "\n",
      "and\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** (i) the dimensions**\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}}$ is $m \\times d_h$\n",
      "\n",
      "$\\dfrac {\\delta L} {\\delta o^a}$ is $m \\times 1$\n",
      "\n",
      "$(h^s)^T$ is $1 \\times d_h$\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}}$ is $m \\times 1$\n",
      "\n",
      "\n",
      "\n",
      "**(ii) the weights**\n",
      "\n",
      "\n",
      "$o^s = softmax(o^a) = softmax(W^{(2)}h^s + b^{(2)})$\n",
      "\n",
      "\n",
      "$f(g(x))' = f'(g(x)) * g'(x)$\n",
      "\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} * \\dfrac {\\delta (W^{(2)}h^s + b^{(2)})} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T $\n",
      "\n",
      "\n",
      "**(iii) the biases**\n",
      "\n",
      "same as for the weights\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} * \\dfrac {\\delta (W^{(2)}h^s + b^{(2)})} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 e)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the chain rule\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k} \\dfrac {\\delta o^a_k}\u00a0{\\delta h^s_j}$$\n",
      "\n",
      "show that the partial derivatives of the cost L with respect to the outputs of the neurons in the hidden layer are given by:\n",
      "\n",
      "$\\dfrac {\\delta L}\u00a0{\\delta h^s_j} = (W^{(2)})^T \\dfrac {\\delta L} {\\delta o^a}$\n",
      "\n",
      "\n",
      "We start from:\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k} \\dfrac {\\delta o^a_k}\u00a0{\\delta h^s_j}$$\n",
      "\n",
      "\n",
      "$$ o^a_k = W^{(2)}_k h^s_j + b^{(2)}_k$$\n",
      "\n",
      "$$ \\dfrac {\\delta o^a_k} {\\delta h^s_j} = W^{(2)}_k $$\n",
      "\n",
      "\n",
      "We substitute the derivative term for its value\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k}  W^{(2)}_k$$\n",
      "\n",
      "Which is equivalent in the matrix form to\n",
      "\n",
      "$$(W^{(2)})^T \\dfrac {\\delta L} {\\delta o^a}$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 f)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**First show that the derivative of $tahn(z) = 1 - tanh^2(z)$**\n",
      "\n",
      "You can see the derivation here:\n",
      "http://math.stackexchange.com/questions/741050/hyperbolic-functions-derivative-of-tanh-x .\n",
      "\n",
      "It's not really worth the copying."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Negative log-likelihood function\n",
      "\n",
      "L(**x**, *t*) = $ - log\\ o^t(x) $\n",
      "\n",
      "Training the neural network amounts to finding the parameters which minimize the\n",
      "value of the loss function for the training set.\n",
      "\n",
      "**Mention precisely what constitutes \u03b8 the set of all network parameters.**\n",
      "\n",
      "If we go into details for a MLP with one hidden layer:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 e)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Question 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# utility functions for the homework\n",
      "\n",
      "def load_data():\n",
      "    \"\"\" loads the image files used in the homework\"\"\"\n",
      "    test_set = numpy.loadtxt('test_images.txt', delimiter=',')\n",
      "    test_labels = numpy.loadtxt('test_labels.txt', delimiter=',')\n",
      "    train_set = numpy.loadtxt('train_images.txt', delimiter=',')\n",
      "    train_labels = numpy.loadtxt('train_labels.txt', delimiter=',')\n",
      "    return ((test_set, test_labels, train_set, train_labels))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# taken from http://deeplearning.net/tutorial/mlp.html\n",
      "# and http://deeplearning.net/tutorial/logreg.html\n",
      "\n",
      "\"\"\"\n",
      "A multilayer perceptron is a logistic regressor where\n",
      "instead of feeding the input to the logistic regression you insert a\n",
      "intermediate layer, called the hidden layer, that has a nonlinear\n",
      "activation function (tanh in this case) . One can use many such\n",
      "hidden layers making the architecture deep.\n",
      "\n",
      "    output(x) = softmax( b^{(2)} + W^{(2)}( tanh( b^{(1)} + W^{(1)} x) ) )\n",
      "\"\"\"\n",
      "\n",
      "import sys, os\n",
      "\n",
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "\n",
      "class LogisticRegression(object):\n",
      "    \"\"\"Output layer with softmax activation\"\"\"\n",
      "\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\"        \n",
      "        :param input: symbolic variable that describes the input\n",
      "         of the architecture (one minibatch)\n",
      "        \n",
      "        :param n_in: number of input units, the dimension of\n",
      "         the space in which the datapoints lie\n",
      "        \n",
      "        :param n_out: number of output units, the dimension of\n",
      "         the space in which the labels lie\n",
      "        \"\"\"\n",
      "\n",
      "        # zero initialized W\n",
      "        # dim(W) = (n_in, n_out)\n",
      "        W_values = numpy.asarray(\n",
      "            rng.uniform(\n",
      "                low=-numpy.sqrt(1. / n_in),\n",
      "                high=numpy.sqrt(1. / n_in),\n",
      "                size=(n_in, n_out)), dtype=theano.config.floatX)\n",
      "        \n",
      "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "        \n",
      "        # zero initialized b\n",
      "        # dim(b) = (n_out)\n",
      "        self.b = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_out,),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='b',\n",
      "            borrow=True)\n",
      "\n",
      "        # softmax activation function\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # predicted function (max softmax)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "\n",
      "        # all the parameters that need to be fitted\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"\n",
      "        Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"\n",
      "        Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type))\n",
      "\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \"\"\"\n",
      "        units are fully-connected\n",
      "        tanh activation function\n",
      "        weight matrix of shape (n_in,n_out)\n",
      "        bias vector b is of shape (n_out,)\n",
      "\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "        :param n_in: dimensionality of input\n",
      "        :param n_out: number of hidden units\n",
      "        :param activation: Non linearity to be applied in the hidden layer\n",
      "        \"\"\"\n",
      "\n",
      "        self.input = input\n",
      "\n",
      "        # `W` is initialized with uniformely sampled values\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      " \n",
      "        # initialize W\n",
      "        W_values = numpy.asarray(\n",
      "            rng.uniform(\n",
      "                low=-numpy.sqrt(1. / n_in),\n",
      "                high=numpy.sqrt(1. / n_in),\n",
      "                size=(n_in, n_out)), dtype=theano.config.floatX)\n",
      "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        # initialize b\n",
      "        b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "        self.b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        # output function\n",
      "        self.output = activation(T.dot(input, self.W) + self.b)\n",
      "        \n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "class MLP(object):\n",
      "    \"\"\"Multi-Layer Perceptron Class\n",
      "\n",
      "    A multilayer perceptron is a feedforward artificial neural network model\n",
      "    that has one layer or more of hidden units and nonlinear activations.\n",
      "    Intermediate layers usually have as activation function tanh or the\n",
      "    sigmoid function  while the top layer is a softmax layer.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :param input: symbolic variable that describes the input of\n",
      "         the architecture (one minibatch)\n",
      "\n",
      "        :param n_in: number of input units, the dimension of the space\n",
      "         in which the datapoints lie\n",
      "\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "         which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Hidden Layer -> Logistic Regression Layer\n",
      "        \n",
      "        # HiddenLayer with tanh layer\n",
      "        self.hiddenLayer = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # LogisticRegressionLayer with softmax activation\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "\n",
      "        # minimize L1 norm\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum() + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # minimize square of L2 norm\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum() + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "       \n",
      "\n",
      "        # log likelihood of the output of the model (last layer)\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "\n",
      "        # same holds for the function computing the number of errors\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # parameters are the parameters of the two layers\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
      "    \n",
      "    def finite_difference():\n",
      "        pass\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Must prove that the gradients are correctly computed using the finite difference method.\n",
      "# The ratio of your gradient calculated using back-prop to the gradient estimated by\n",
      "# finite differences should be situer somewhere between 0.99 and 1.01.\n",
      "\n",
      "def test_mlp(learning_rate=0.01,\n",
      "             L1_reg=0.00,\n",
      "             L2_reg=0.0001,\n",
      "             n_epochs=1000,\n",
      "             batch_size=100,\n",
      "             n_hidden=500):\n",
      "    \"\"\"\n",
      "    Stochastic gradient descent optimization for a multilayer perceptron\n",
      "\n",
      "    :param learning_rate: learning rate used (factor for the stochastic gradient descent)\n",
      "    :param L1_reg: L1-norm's weight when added to the cost (see regularization)\n",
      "    :param L2_reg: L2-norm's weight when added to the cost (see regularization)\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "   \"\"\"\n",
      "    print '... LOADING DATA'\n",
      "    test_set, test_labels, train_set, train_labels = load_data()\n",
      "\n",
      "    print '... INITIALIZING MLP'\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar() # index to a [mini]batch\n",
      "    x = T.matrix('x')   # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of [int] labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=28 * 28,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=10)\n",
      "\n",
      "    # the cost we minimize during training is the negative log likelihood of\n",
      "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
      "    # here symbolically\n",
      "    cost = (\n",
      "        classifier.negative_log_likelihood(y)\n",
      "        + L1_reg * classifier.L1\n",
      "        + L2_reg * classifier.L2_sqr)\n",
      "\n",
      "    # compiling a Theano function that computes the mistakes that are made\n",
      "    # by the model on a minibatch\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sorted in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    print '... TRAINING MODEL'\n",
      "\n",
      "    # early-stopping parameters\n",
      "    patience = 10000  # look as this many examples regardless\n",
      "    patience_increase = 2  # wait this much longer when a new best is\n",
      "                           # found\n",
      "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
      "                                   # considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_validation_loss = numpy.inf\n",
      "    best_iter = 0\n",
      "    test_score = 0.\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            # iteration number\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                # compute zero-one loss on validation set\n",
      "                validation_losses = [validate_model(i) for i\n",
      "                                     in xrange(n_valid_batches)]\n",
      "                this_validation_loss = numpy.mean(validation_losses)\n",
      "\n",
      "                print(\n",
      "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
      "                    (\n",
      "                        epoch,\n",
      "                        minibatch_index + 1,\n",
      "                        n_train_batches,\n",
      "                        this_validation_loss * 100.\n",
      "                    )\n",
      "                )\n",
      "\n",
      "                # if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "                    #improve patience if loss improvement is good enough\n",
      "                    if (\n",
      "                        this_validation_loss < best_validation_loss *\n",
      "                        improvement_threshold\n",
      "                    ):\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                    best_validation_loss = this_validation_loss\n",
      "                    best_iter = iter\n",
      "\n",
      "                    # test it on the test set\n",
      "                    test_losses = [test_model(i) for i\n",
      "                                   in xrange(n_test_batches)]\n",
      "                    test_score = numpy.mean(test_losses)\n",
      "\n",
      "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
      "                           'best model %f %%') %\n",
      "                          (epoch, minibatch_index + 1, n_train_batches,\n",
      "                           test_score * 100.))\n",
      "\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "    print '... DONE TRAINING'\n",
      "\n",
      "    end_time = time.clock()\n",
      "    print(('Optimization complete. Best validation score of %f %% '\n",
      "           'obtained at iteration %i, with test performance %f %%') %\n",
      "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
      "    print >> sys.stderr, ('The code for file ' +\n",
      "                          os.path.split(__file__)[1] +\n",
      "                          ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "\n",
      "test_mlp()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... LOADING DATA\n",
        "... INITIALIZING MLP"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "global name 'rng' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-5794875e8a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m                           ' ran for %.2fm' % ((end_time - start_time) / 60.))\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m \u001b[0mtest_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-7-5794875e8a8b>\u001b[0m in \u001b[0;36mtest_mlp\u001b[1;34m(learning_rate, L1_reg, L2_reg, n_epochs, batch_size, n_hidden)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mn_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m28\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mn_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         n_out=10)\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# the cost we minimize during training is the negative log likelihood of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-6-b2c47d31f1e6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rng, input, n_in, n_hidden, n_out)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhiddenLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mn_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mn_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         )\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-6-b2c47d31f1e6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input, n_in, n_out)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# dim(W) = (n_in, n_out)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         W_values = numpy.asarray(\n\u001b[1;32m---> 39\u001b[1;33m             rng.uniform(\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mlow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: global name 'rng' is not defined"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}