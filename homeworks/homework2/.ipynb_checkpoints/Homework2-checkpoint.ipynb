{
 "metadata": {
  "name": "",
  "signature": "sha256:5d6cb2b335d9d571d3a7831348d9576ea6e06b7bf356e4e477222e553a445ecc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##IFT6390\n",
      "\n",
      "##Homework 2: multilayer perceptron (single hidden layer)\n",
      "\n",
      "###Gabriel C-Parent C5912"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 a)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the dimension of $b^{(1)}$ ?**\n",
      "\n",
      "$b^{(1)}$ is $d_h \\times 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula to calculate the vector of activations**\n",
      "**(i.e. before the non-linearity) of the neurons in the hidden layer,**\n",
      "**$h^a$ , given an input, x, at first in matrix expression.**\n",
      "\n",
      "$h^a_{d_h \\times 1} = b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} x_{d \\times 1} $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Element-by-element computation of the entries of $h^a$.**\n",
      "\n",
      "$h^a_i = b^{(1)}_i + \\sum\\limits_{j=1}^d w^{(1)}_{i, j}x_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the vector of outputs of the hidden layer neurons, $h^s$, in terms of the activations, $h^a$.**\n",
      "\n",
      "$h^s_{d_h \\times 1} = tanh(b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} x_{d \\times 1}) $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 b)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Let $W^{(2)}$ be the weight matrix from the hidden to output layer and $b^{(2)}$ be the**\n",
      "**vector of biases for the output layer. What are the dimensions of $W^{(2)}$ et $b^{(2)}$ ?**\n",
      "\n",
      "$b^{(2)}$ is $m \\times 1$\n",
      "\n",
      "$W^{(2)}$ is $m \\times d_h$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula describing the vector of activations of neurons in the output**\n",
      "**layer $o^a$ given $h^s$ in matrix form**\n",
      "\n",
      "$o^a_{m \\times 1} = b^{(2)}_{m \\times 1} + W^{(2)}_{m \\times d_h} h^s_{d_h \\times 1}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Then in detail element-wise form**\n",
      "\n",
      "$o^a_i = b^{(2)}_i + \\sum\\limits_{j=1}^{d_h} w^{(2)}_{i, j} h^s_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Q1 c)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is contained in the set of all network parameters, $\\theta$**\n",
      "\n",
      "- activation function (tanh, sigmoid, linear even)\n",
      "- number of hidden layer nodes\n",
      "- $W^{(1)}, W^{(2)}, b^{(1)}, b^{(2)}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the number $n_{\\theta}$ of parameters in $\\theta$ ?**\n",
      "\n",
      "- $W^{(1)}$ is $d_h \\times d$\n",
      "- $W^{(2)}$  is $m \\times d_h$\n",
      "- $b^{(1)}$  is $d_h \\times 1$\n",
      "- $b^{(2)}$  is $m \\times 1$\n",
      "\n",
      "$n_{\\theta}$ =  $d_h * (d+1 + m) + m$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 d)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Show that the gradients wrt. parameters $W^{(2)}$ and $b^{(2)}$ are given by:**\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T $\n",
      "\n",
      "and\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** (i) the dimensions**\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}}$ is $m \\times d_h$\n",
      "\n",
      "$\\dfrac {\\delta L} {\\delta o^a}$ is $m \\times 1$\n",
      "\n",
      "$(h^s)^T$ is $1 \\times d_h$\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}}$ is $m \\times 1$\n",
      "\n",
      "\n",
      "\n",
      "**(ii) the weights**\n",
      "\n",
      "\n",
      "$o^s = softmax(o^a) = softmax(W^{(2)}h^s + b^{(2)})$\n",
      "\n",
      "\n",
      "$f(g(x))' = f'(g(x)) * g'(x)$\n",
      "\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} * \\dfrac {\\delta (W^{(2)}h^s + b^{(2)})} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T $\n",
      "\n",
      "\n",
      "**(iii) the biases**\n",
      "\n",
      "same as for the weights\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} * \\dfrac {\\delta (W^{(2)}h^s + b^{(2)})} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 e)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the chain rule\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k} \\dfrac {\\delta o^a_k}\u00a0{\\delta h^s_j}$$\n",
      "\n",
      "show that the partial derivatives of the cost L with respect to the outputs of the neurons in the hidden layer are given by:\n",
      "\n",
      "$$\\dfrac {\\delta L}\u00a0{\\delta h^s_j} = (W^{(2)})^T \\dfrac {\\delta L} {\\delta o^a}$$\n",
      "\n",
      "\n",
      "We start from:\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k} \\dfrac {\\delta o^a_k}\u00a0{\\delta h^s_j}$$\n",
      "\n",
      "\n",
      "$$ o^a_k = W^{(2)}_k h^s_j + b^{(2)}_k$$\n",
      "\n",
      "$$ \\dfrac {\\delta o^a_k} {\\delta h^s_j} = W^{(2)}_k $$\n",
      "\n",
      "\n",
      "We substitute the derivative term for its value\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k}  W^{(2)}_k$$\n",
      "\n",
      "Which is equivalent in the matrix form to\n",
      "\n",
      "$$\\dfrac {\\delta L}\u00a0{\\delta h^s} = (W^{(2)})^T \\dfrac {\\delta L} {\\delta o^a}$$\n",
      "\n",
      "$\\dfrac {\\delta L} {\\delta o^a}$ is $m \\times 1$ and $(W^{(2)})^T$ is $d_h \\times m$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 f)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**First show that the derivative of $tahn(z) = 1 - tanh^2(z)$**\n",
      "\n",
      "You can see the derivation here:\n",
      "http://math.stackexchange.com/questions/741050/hyperbolic-functions-derivative-of-tanh-x .\n",
      "\n",
      "It's not really worth the copying."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the derivative we are looking for is equal to:\n",
      "$$\\dfrac {\\delta L}\u00a0{\\delta h^a_j} = \\dfrac {\\delta L} {\\delta h^s_j} \\dfrac {\\delta h^s_j} {\\delta h^a_j}$$\n",
      "\n",
      "which is therefore equal to\n",
      "\n",
      "$$\\dfrac {\\delta L}\u00a0{\\delta h^a_j} = \\dfrac {\\delta L} {\\delta h^s_j} (1 - tanh^2(h^a_j))$$\n",
      "\n",
      "\n",
      "$\\dfrac {\\delta L} {\\delta h^s_j}$ is $d_h \\times 1$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 g)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\dfrac {\\delta L}\u00a0{\\delta W^{(1)}} = \\dfrac {\\delta L} {\\delta h^a} \\dfrac {\\delta h^a}\u00a0{\\delta W^{(1)}}$$\n",
      "\n",
      "$$\\dfrac {\\delta h^a}\u00a0{\\delta W^{(1)}} = \\dfrac {\\delta L} {\\delta h^a}  x^T$$\n",
      "\n",
      "and\n",
      "\n",
      "$$\\dfrac {\\delta h^a}\u00a0{\\delta b^{(1)}} = \\dfrac {\\delta L} {\\delta h^a}$$\n",
      "\n",
      "since $h^a = W^{(1)}x + b^{(1)}$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 h)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$L_{mod} = \\alpha W^{(1)} + \\alpha W^{(2)} + \\beta (W^{(1)})^2 + \\beta (W^{(2)})^2 +L(x, t)$ \n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T +\\alpha + 2\\beta W^{(1)}  $\n",
      "\n",
      "\n",
      "$\\dfrac {\\delta L}\u00a0{\\delta W^{(2)}} =  \\dfrac {\\delta L} {\\delta o^a} (h^s)^T +\\alpha + 2\\beta W^{(2)}  $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 i)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "$\\dfrac {\\delta h^s}\u00a0{\\delta h^a} = 1$ if $h^a>\\ 0,\\ 0$ otherwise\n",
      "\n",
      "therefore, only the derivatives depending on this term are affected: that is $\\dfrac {\\delta L}\u00a0{\\delta W^{(1)}}$ and $\\dfrac {\\delta L}\u00a0{\\delta b^{(1)}}$ "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}