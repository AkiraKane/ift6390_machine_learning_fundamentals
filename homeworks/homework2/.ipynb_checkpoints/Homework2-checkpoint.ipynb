{
 "metadata": {
  "name": "",
  "signature": "sha256:b5a9e30a827a608d10c1c199a8f33e5ac171879bbda9fb5d0b4c521815122edc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Homework 2: multilayer perceptron (single hidden layer)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## parameters\n",
      "\n",
      "- 3 layers (input -> hidden -> output)\n",
      "    - input: 784 neurons (1 for each pixel)\n",
      "    - hidden units: not fixed\n",
      "    - output: 10 neurons (1 for each digit)\n",
      "\n",
      "- tanh(sum(input)) for each layer\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 a)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the dimension of $b^{(1)}$ ?**\n",
      "\n",
      "$b^{(1)}$ is $d_h \\times 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula to calculate the vector of activations**\n",
      "**(i.e. before the non-linearity) of the neurons in the hidden layer,**\n",
      "**$h^a$ , given an input, x, at first in matrix expression.**\n",
      "\n",
      "$h^a_{d_h \\times 1} = b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} \\cdot x_{d \\times 1} $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Element-by-element computation of the entries of $h^a$.**\n",
      "\n",
      "$h^a_i = b^{(1)}_i + \\sum\\limits_{j=1}^d x_j * W^{(1)}_{i, j}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the vector of outputs of the hidden layer neurons, $h^s$, in terms of the activations, $h^a$.**\n",
      "\n",
      "$h^s_{d_h \\times 1} = tanh(b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} \\cdot x_{d \\times 1}) $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 b)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Let $W^{(2)}$ be the weight matrix from the hidden to output layer and $b^{(2)}$ be the**\n",
      "**vector of biases for the output layer. What are the dimensions of $W^{(2)}$ et $b^{(2)}$ ?**\n",
      "\n",
      "$b^{(2)}$ is $m \\times 1$\n",
      "\n",
      "$W^{(2)}$ is $m \\times d_h$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula describing the vector of activations of neurons in the output**\n",
      "**layer $o^a$ given $h^s$ in matrix form**\n",
      "\n",
      "$o^a_{m \\times 1} = b^{(2)}_{m \\times 1} + W^{(2)}_{m \\times d_h} \\cdot h^s_{d_h \\times 1}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Then in detail element-wise form**\n",
      "\n",
      "$o^a_i = b^{(2)}_i + \\sum\\limits_{j=1}^h h^s_j * W^{(2)}_{i, j}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Q1 c)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Output of the neurons in the output layer (softmax style)\n",
      "\n",
      "$o^s = softmax(o^a)$\n",
      "\n",
      "$o^s_k = \\dfrac{exp(o^s_k)}\u00a0{\\sum\\limits_{j=1}^M exp(o^s_j)}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 d)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Negative log-likelihood function\n",
      "\n",
      "L(**x**, *t*) = $ - log\\ o^t(x) $\n",
      "\n",
      "Training the neural network amounts to finding the parameters which minimize the\n",
      "value of the loss function for the training set.\n",
      "\n",
      "**Mention precisely what constitutes \u03b8 the set of all network parameters.**\n",
      "\n",
      "If we go into details for a MLP with one hidden layer:\n",
      "\n",
      "- activation function (tanh, sigmoid, linear even)\n",
      "- number of hidden layer nodes\n",
      "- $W^{(1)}, W^{(2)}, b^{(1)}, b^{(2)}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How many scalar parameters does \u03b8 contain?**\n",
      "\n",
      "- $W^{(1)}$ is $d_h \\times d$\n",
      "- $W^{(2)}$  is $m \\times d_h$\n",
      "- $b^{(1)}$  is $d_h \\times 1$\n",
      "- $b^{(2)}$  is $m \\times 1$\n",
      "\n",
      "total =  $d_h * (d+1 + m) + m$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 e)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Question 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}