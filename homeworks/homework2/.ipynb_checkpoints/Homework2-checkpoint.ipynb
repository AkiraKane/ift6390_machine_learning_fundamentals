{
 "metadata": {
  "name": "",
  "signature": "sha256:e9abe457046cf8cebf89d879c9ef1e2768828550c9ed1bc18f04e49088738ce7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Homework 2: multilayer perceptron (single hidden layer)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## parameters\n",
      "\n",
      "- 3 layers (input -> hidden -> output)\n",
      "    - input: 784 neurons (1 for each pixel)\n",
      "    - hidden units: not fixed\n",
      "    - output: 10 neurons (1 for each digit)\n",
      "\n",
      "- tanh(sum(input)) for each layer\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 a)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the dimension of $b^{(1)}$ ?**\n",
      "\n",
      "$b^{(1)}$ is $d_h \\times 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula to calculate the vector of activations**\n",
      "**(i.e. before the non-linearity) of the neurons in the hidden layer,**\n",
      "**$h^a$ , given an input, x, at first in matrix expression.**\n",
      "\n",
      "$h^a_{d_h \\times 1} = b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} x_{d \\times 1} $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Element-by-element computation of the entries of $h^a$.**\n",
      "\n",
      "$h^a_i = b^{(1)}_i + \\sum\\limits_{j=1}^d w^{(1)}_{i, j}x_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the vector of outputs of the hidden layer neurons, $h^s$, in terms of the activations, $h^a$.**\n",
      "\n",
      "$h^s_{d_h \\times 1} = tanh(b^{(1)}_{d_h \\times 1} + W^{(1)}_{d_h \\times d} x_{d \\times 1}) $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 b)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Let $W^{(2)}$ be the weight matrix from the hidden to output layer and $b^{(2)}$ be the**\n",
      "**vector of biases for the output layer. What are the dimensions of $W^{(2)}$ et $b^{(2)}$ ?**\n",
      "\n",
      "$b^{(2)}$ is $m \\times 1$\n",
      "\n",
      "$W^{(2)}$ is $m \\times d_h$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Write down the formula describing the vector of activations of neurons in the output**\n",
      "**layer $o^a$ given $h^s$ in matrix form**\n",
      "\n",
      "$o^a_{m \\times 1} = b^{(2)}_{m \\times 1} + W^{(2)}_{m \\times d_h} h^s_{d_h \\times 1}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Then in detail element-wise form**\n",
      "\n",
      "$o^a_i = b^{(2)}_i + \\sum\\limits_{j=1}^{d_h} w^{(2)}_{i, j} h^s_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Q1 c)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is contained in the set of all network parameters, $\\theta$**\n",
      "\n",
      "- activation function (tanh, sigmoid, linear even)\n",
      "- number of hidden layer nodes\n",
      "- $W^{(1)}, W^{(2)}, b^{(1)}, b^{(2)}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**What is the number $n_{\\theta}$ of parameters in $\\theta$ ?**\n",
      "\n",
      "- $W^{(1)}$ is $d_h \\times d$\n",
      "- $W^{(2)}$  is $m \\times d_h$\n",
      "- $b^{(1)}$  is $d_h \\times 1$\n",
      "- $b^{(2)}$  is $m \\times 1$\n",
      "\n",
      "$n_{\\theta}$ =  $d_h * (d+1 + m) + m$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 d)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Show that the gradients wrt. parameters $W^{(2)}$ and $b^{(2)}$ are given by:**\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T $\n",
      "\n",
      "and\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** (i) the dimensions**\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}}$ is $m \\times d_h$\n",
      "\n",
      "$\\dfrac {\\delta L} {\\delta o^a}$ is $m \\times 1$\n",
      "\n",
      "$(h^s)^T$ is $1 \\times d_h$\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}}$ is $m \\times 1$\n",
      "\n",
      "\n",
      "\n",
      "**(ii) the weights**\n",
      "\n",
      "\n",
      "$o^s = softmax(o^a) = softmax(W^{(2)}h^s + b^{(2)})$\n",
      "\n",
      "\n",
      "$f(g(x))' = f'(g(x)) * g'(x)$\n",
      "\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} * \\dfrac {\\delta (W^{(2)}h^s + b^{(2)})} {\\delta W^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} (h^s)^T $\n",
      "\n",
      "\n",
      "**(iii) the biases**\n",
      "\n",
      "same as for the weights\n",
      "\n",
      "$\\dfrac {\\delta  L} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} * \\dfrac {\\delta (W^{(2)}h^s + b^{(2)})} {\\delta b^{(2)}} = \\dfrac {\\delta L} {\\delta o^a} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 e)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the chain rule\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k} \\dfrac {\\delta o^a_k}\u00a0{\\delta h^s_j}$$\n",
      "\n",
      "show that the partial derivatives of the cost L with respect to the outputs of the neurons in the hidden layer are given by:\n",
      "\n",
      "$\\dfrac {\\delta L}\u00a0{\\delta h^s_j} = (W^{(2)})^T \\dfrac {\\delta L} {\\delta o^a}$\n",
      "\n",
      "\n",
      "We start from:\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k} \\dfrac {\\delta o^a_k}\u00a0{\\delta h^s_j}$$\n",
      "\n",
      "\n",
      "$$ o^a_k = W^{(2)}_k h^s_j + b^{(2)}_k$$\n",
      "\n",
      "$$ \\dfrac {\\delta o^a_k} {\\delta h^s_j} = W^{(2)}_k $$\n",
      "\n",
      "\n",
      "We substitute the derivative term for its value\n",
      "\n",
      "$$ \\dfrac {\\delta L}\u00a0{\\delta h^s_j} = \\sum \\limits_{k=1}^M \\dfrac {\\delta L} {\\delta o^a_k}  W^{(2)}_k$$\n",
      "\n",
      "Which is equivalent in the matrix form to\n",
      "\n",
      "$$(W^{(2)})^T \\dfrac {\\delta L} {\\delta o^a}$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 f)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**First show that the derivative of $tahn(z) = 1 - tanh^2(z)$**\n",
      "\n",
      "You can see the derivation here:\n",
      "http://math.stackexchange.com/questions/741050/hyperbolic-functions-derivative-of-tanh-x .\n",
      "\n",
      "It's not really worth the copying."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Negative log-likelihood function\n",
      "\n",
      "L(**x**, *t*) = $ - log\\ o^t(x) $\n",
      "\n",
      "Training the neural network amounts to finding the parameters which minimize the\n",
      "value of the loss function for the training set.\n",
      "\n",
      "**Mention precisely what constitutes \u03b8 the set of all network parameters.**\n",
      "\n",
      "If we go into details for a MLP with one hidden layer:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Q1 e)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Question 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data():\n",
      "    '''\n",
      "    Loads the data\n",
      "    '''\n",
      "\n",
      "    test_x = numpy.loadtxt('test_images.txt', delimiter=',')\n",
      "    test_y = numpy.argmax(numpy.loadtxt('test_labels.txt', delimiter=','), axis=1)\n",
      "    train_x = numpy.loadtxt('train_images.txt', delimiter=',')\n",
      "    train_y = numpy.argmax(numpy.loadtxt('train_labels.txt', delimiter=','), axis=1)\n",
      "\n",
      "\n",
      "    def shared_dataset(data_x, data_y, borrow=True):\n",
      "        \"\"\"\n",
      "        Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        shared_x = theano.shared(numpy.asarray(data_x,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        shared_y = theano.shared(numpy.asarray(data_y,\n",
      "                                               dtype=theano.config.floatX),\n",
      "                                 borrow=borrow)\n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "    test_x, test_y = shared_dataset(test_x, test_y)\n",
      "    train_x, train_y = shared_dataset(train_x, train_y)\n",
      "\n",
      "    rval = [(train_x, train_y),\n",
      "            (test_x, test_y)]\n",
      "    return rval\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "\n",
      "class LogisticRegression(object):\n",
      "    \"\"\"Output layer with softmax activation\"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_out):\n",
      "        \"\"\"\n",
      "        :param input: symbolic variable that describes the input\n",
      "         of the architecture (one minibatch)\n",
      "\n",
      "        :param n_in: number of input units, the dimension of\n",
      "         the space in which the datapoints lie\n",
      "\n",
      "        :param n_out: number of output units, the dimension of\n",
      "         the space in which the labels lie\n",
      "        \"\"\"\n",
      "\n",
      "        # zero initialized W\n",
      "        # dim(W) = (n_in, n_out)\n",
      "\n",
      "        W_values = numpy.asarray(\n",
      "            rng.uniform(\n",
      "                low=-numpy.sqrt(1. / n_in),\n",
      "                high=numpy.sqrt(1. / n_in),\n",
      "                size=(n_in, n_out)\n",
      "            ),\n",
      "            dtype=theano.config.floatX\n",
      "        )\n",
      "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        # zero initialized b\n",
      "        # dim(b) = (n_out)\n",
      "        self.b = theano.shared(\n",
      "            value=numpy.zeros(\n",
      "                (n_out,),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            name='b',\n",
      "            borrow=True\n",
      "        )\n",
      "\n",
      "        # softmax activation function\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # predicted function (max softmax)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "\n",
      "        # all the parameters that need to be fitted\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"\n",
      "        Return the mean of the negative log-likelihood of the prediction\n",
      "        of this model under a given target distribution.\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "\n",
      "        Note: we use the mean instead of the sum so that\n",
      "              the learning rate is less dependent on the batch size\n",
      "        \"\"\"\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "\n",
      "    def errors(self, y):\n",
      "        \"\"\"Return a float representing the number of errors in the minibatch\n",
      "        over the total number of examples of the minibatch ; zero one\n",
      "        loss over the size of the minibatch\n",
      "\n",
      "        :type y: theano.tensor.TensorType\n",
      "        :param y: corresponds to a vector that gives for each example the\n",
      "                  correct label\n",
      "        \"\"\"\n",
      "\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError(\n",
      "                'y should have the same shape as self.y_pred',\n",
      "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
      "            )\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "        \"\"\"\n",
      "        - units are fully-connected\n",
      "        - tanh activation function\n",
      "        - weight matrix of shape (n_in,n_out)\n",
      "        - bias vector b is of shape (n_out,)\n",
      "\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "        :param n_in: dimensionality of input\n",
      "        :param n_out: number of hidden units\n",
      "        :param activation: Non linearity to be applied in the hidden layer\n",
      "        \"\"\"\n",
      "\n",
      "        self.input = input\n",
      "\n",
      "        # `W` is initialized with uniformely sampled values\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      "        # Y Bengio, X. Glorot,\n",
      "        # Understanding the difficulty of training deep feedforward\n",
      "        # neural networks, AISTATS 2010\n",
      " \n",
      "        # initialize W\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(1. / n_in),\n",
      "                    high=numpy.sqrt(1. / n_in),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        # initialize b\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "\n",
      "        self.output = activation(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "class MLP(object):\n",
      "    \"\"\"Multi-Layer Perceptron Class\n",
      "\n",
      "    A multilayer perceptron is a feedforward artificial neural network model\n",
      "    that has one layer or more of hidden units and nonlinear activations.\n",
      "    Intermediate layers usually have as activation function tanh or the\n",
      "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
      "    top layer is a softamx layer (defined here by a ``LogisticRegression``\n",
      "    class).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # The logistic regression layer gets as input the hidden units\n",
      "        # of the hidden layer\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            rng=rng,\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "\n",
      "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
      "        # be small\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum()\n",
      "            + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # square of L2 norm ; one regularization option is to enforce\n",
      "        # square of L2 norm to be small\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum()\n",
      "            + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "\n",
      "        # negative log likelihood of the MLP is given by the negative\n",
      "        # log likelihood of the output of the model, computed in the\n",
      "        # logistic regression layer\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "        # same holds for the function computing the number of errors\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # the parameters of the model are the parameters of the two layer it is\n",
      "        # made out of\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
      "\n",
      "\n",
      "def test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
      "             batch_size=100, n_hidden=500):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    This is demonstrated on MNIST.\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type L1_reg: float\n",
      "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
      "    regularization)\n",
      "\n",
      "    :type L2_reg: float\n",
      "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
      "    regularization)\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "\n",
      "   \"\"\"\n",
      "    datasets = load_data()\n",
      "\n",
      "    train_set_x, train_set_y = datasets[0]\n",
      "    test_set_x, test_set_y = datasets[1]\n",
      "\n",
      "    # compute number of minibatches for training and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                        # [int] labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=28 * 28,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=10\n",
      "    )\n",
      "\n",
      "    # the cost we minimize during training is the negative log likelihood of\n",
      "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
      "    # here symbolically\n",
      "    cost = (\n",
      "        classifier.negative_log_likelihood(y)\n",
      "        + L1_reg * classifier.L1\n",
      "        + L2_reg * classifier.L2_sqr\n",
      "    )\n",
      "\n",
      "    # verifty the fit of the model on a test set minibatch\n",
      "    test_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    # verify the fit of the model on the train set\n",
      "    check_fit_train_set = theano.function(\n",
      "        inputs=[],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "            x: train_set_x,\n",
      "            y: train_set_y\n",
      "            }\n",
      "        )\n",
      "\n",
      "    # verify the fit of the model on the test set\n",
      "    check_fit_test_set = theano.function(\n",
      "        inputs=[],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "            x: test_set_x,\n",
      "            y: test_set_y\n",
      "        }\n",
      "    )\n",
      "\n",
      "\n",
      "\n",
      "    # compile a Theano funciton that computes the mistakes that are made\n",
      "    # by the model on a minibatch of the train set (we'll see overfitting)\n",
      "    check_fit = theano.function(\n",
      "    inputs=[index],\n",
      "    outputs=classifier.errors(y),\n",
      "    givens={\n",
      "        x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "        y: train_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "\n",
      "    print '... training'\n",
      "\n",
      "    best_iter = 0\n",
      "    test_scores = []\n",
      "    train_scores = []\n",
      "    test_score = 0.\n",
      "    train_score = 0.\n",
      "\n",
      "    epoch = 0\n",
      "\n",
      "    while (epoch < n_epochs):\n",
      "\n",
      "        epoch = epoch + 1\n",
      "        if epoch%5 == 0:\n",
      "          print \"{0}; {1}; {2}\".format(epoch, train_scores[-1], test_scores[-1])\n",
      "\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "\n",
      "        # verify the fit on the datasets\n",
      "        train_scores.append(check_fit_train_set())\n",
      "        test_scores.append(check_fit_test_set())\n",
      "\n",
      "\n",
      "    print 'Optimization complete. Best test performance {0}'.format(min(test_scores))\n",
      "    print 'Scores per iterations'\n",
      "\n",
      "    for i in range(len(train_scores)):\n",
      "      print \"{0}; {1}; {2}\".format(i, train_scores[i], test_scores[i])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_mlp()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5; 0.409; 0.424"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10; 0.283; 0.314"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15; 0.244; 0.282"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20; 0.228; 0.256"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25; 0.209; 0.226"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30; 0.191; 0.216"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35; 0.174; 0.202"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40; 0.16; 0.196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45; 0.146; 0.19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50; 0.141; 0.185"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "55; 0.138; 0.179"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60; 0.13; 0.173"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "65; 0.126; 0.164"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70; 0.12; 0.161"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "75; 0.118; 0.158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80; 0.119; 0.154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "85; 0.113; 0.153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90; 0.11; 0.15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "95; 0.104; 0.146"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100; 0.102; 0.143"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "105; 0.098; 0.143"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "110; 0.092; 0.142"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "115; 0.09; 0.141"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "120; 0.087; 0.139"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "125; 0.086; 0.136"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "130; 0.084; 0.136"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "135; 0.081; 0.136"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "140; 0.079; 0.136"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "145; 0.076; 0.134"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "150; 0.076; 0.133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "155; 0.072; 0.133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "160; 0.071; 0.133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "165; 0.067; 0.133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "170; 0.066; 0.132"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "175; 0.065; 0.132"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "180; 0.062; 0.132"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "185; 0.062; 0.131"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "190; 0.057; 0.13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "195; 0.055; 0.127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "200; 0.053; 0.129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "205; 0.048; 0.129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "210; 0.048; 0.129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "215; 0.048; 0.129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "220; 0.048; 0.13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "225; 0.047; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "230; 0.047; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "235; 0.047; 0.129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "240; 0.046; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "245; 0.046; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "250; 0.044; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "255; 0.041; 0.129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "260; 0.041; 0.127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "265; 0.04; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "270; 0.039; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "275; 0.037; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "280; 0.037; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "285; 0.036; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "290; 0.036; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "295; 0.035; 0.128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "300; 0.035; 0.127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "305; 0.033; 0.127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "310; 0.033; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "315; 0.033; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "320; 0.031; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "325; 0.031; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "330; 0.031; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "335; 0.029; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "340; 0.027; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "345; 0.026; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "350; 0.024; 0.122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "355; 0.024; 0.122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "360; 0.024; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "365; 0.021; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "370; 0.02; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "375; 0.019; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "380; 0.018; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "385; 0.018; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "390; 0.018; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "395; 0.017; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "400; 0.017; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "405; 0.017; 0.123"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "410; 0.017; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "415; 0.015; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "420; 0.014; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "425; 0.014; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "430; 0.014; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "435; 0.014; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "440; 0.014; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "445; 0.013; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "450; 0.013; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "455; 0.013; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "460; 0.012; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "465; 0.011; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "470; 0.011; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "475; 0.011; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "480; 0.011; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "485; 0.011; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "490; 0.009; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "495; 0.009; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "500; 0.008; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "505; 0.008; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "510; 0.008; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "515; 0.008; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "520; 0.008; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "525; 0.008; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "530; 0.007; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "535; 0.006; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "540; 0.006; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "545; 0.005; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "550; 0.005; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "555; 0.005; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "560; 0.005; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "565; 0.004; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "570; 0.003; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "575; 0.003; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "580; 0.002; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "585; 0.002; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "590; 0.002; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "595; 0.002; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "600; 0.002; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "605; 0.002; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "610; 0.002; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "615; 0.002; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "620; 0.001; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "625; 0.001; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "630; 0.001; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "635; 0.001; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "640; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "645; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "650; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "655; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "660; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "665; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "670; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "675; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "680; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "685; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "690; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "695; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "700; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "705; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "710; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "715; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "720; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "725; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "730; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "735; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "740; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "745; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "750; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "755; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "760; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "765; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "770; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "775; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "780; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "785; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "790; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "795; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "800; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "805; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "810; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "815; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "820; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "825; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "830; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "835; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "840; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "845; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "850; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "855; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "860; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "865; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "870; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "875; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "880; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "885; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "890; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "895; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "900; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "905; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "910; 0.0; 0.126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "915; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "920; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "925; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "930; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "935; 0.0; 0.125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "940; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "945; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "950; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "955; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "960; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "965; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "970; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "975; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "980; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "985; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "990; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "995; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000; 0.0; 0.124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Optimization complete. Best test performance 0.122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Scores per iterations\n",
        "0; 0.746; 0.761\n",
        "1; 0.568; 0.572\n",
        "2; 0.466; 0.484\n",
        "3; 0.409; 0.424\n",
        "4; 0.37; 0.381\n",
        "5; 0.338; 0.35\n",
        "6; 0.314; 0.334\n",
        "7; 0.298; 0.326\n",
        "8; 0.283; 0.314\n",
        "9; 0.275; 0.309\n",
        "10; 0.268; 0.3\n",
        "11; 0.259; 0.294\n",
        "12; 0.254; 0.289\n",
        "13; 0.244; 0.282\n",
        "14; 0.241; 0.276\n",
        "15; 0.239; 0.273\n",
        "16; 0.237; 0.266\n",
        "17; 0.231; 0.259\n",
        "18; 0.228; 0.256\n",
        "19; 0.225; 0.249\n",
        "20; 0.221; 0.242\n",
        "21; 0.217; 0.235\n",
        "22; 0.213; 0.232\n",
        "23; 0.209; 0.226\n",
        "24; 0.207; 0.223\n",
        "25; 0.203; 0.22\n",
        "26; 0.2; 0.22\n",
        "27; 0.195; 0.219\n",
        "28; 0.191; 0.216\n",
        "29; 0.187; 0.213\n",
        "30; 0.183; 0.21\n",
        "31; 0.177; 0.207\n",
        "32; 0.175; 0.203\n",
        "33; 0.174; 0.202\n",
        "34; 0.17; 0.202\n",
        "35; 0.167; 0.202\n",
        "36; 0.163; 0.201\n",
        "37; 0.161; 0.198\n",
        "38; 0.16; 0.196\n",
        "39; 0.158; 0.195\n",
        "40; 0.156; 0.194\n",
        "41; 0.153; 0.194\n",
        "42; 0.15; 0.194\n",
        "43; 0.146; 0.19\n",
        "44; 0.145; 0.189\n",
        "45; 0.144; 0.188\n",
        "46; 0.142; 0.186\n",
        "47; 0.141; 0.186\n",
        "48; 0.141; 0.185\n",
        "49; 0.141; 0.183\n",
        "50; 0.14; 0.182\n",
        "51; 0.139; 0.178\n",
        "52; 0.138; 0.179\n",
        "53; 0.138; 0.179\n",
        "54; 0.135; 0.176\n",
        "55; 0.135; 0.175\n",
        "56; 0.134; 0.177\n",
        "57; 0.133; 0.175\n",
        "58; 0.13; 0.173\n",
        "59; 0.129; 0.173\n",
        "60; 0.129; 0.172\n",
        "61; 0.129; 0.169\n",
        "62; 0.128; 0.167\n",
        "63; 0.126; 0.164\n",
        "64; 0.126; 0.164\n",
        "65; 0.124; 0.164\n",
        "66; 0.122; 0.162\n",
        "67; 0.12; 0.161\n",
        "68; 0.12; 0.161\n",
        "69; 0.12; 0.16\n",
        "70; 0.119; 0.16\n",
        "71; 0.119; 0.159\n",
        "72; 0.118; 0.158\n",
        "73; 0.118; 0.158\n",
        "74; 0.118; 0.157\n",
        "75; 0.118; 0.157\n",
        "76; 0.118; 0.158\n",
        "77; 0.118; 0.156\n",
        "78; 0.119; 0.154\n",
        "79; 0.117; 0.154\n",
        "80; 0.115; 0.154\n",
        "81; 0.115; 0.154\n",
        "82; 0.115; 0.152\n",
        "83; 0.113; 0.153\n",
        "84; 0.113; 0.153\n",
        "85; 0.112; 0.152\n",
        "86; 0.111; 0.152\n",
        "87; 0.111; 0.151\n",
        "88; 0.11; 0.15\n",
        "89; 0.11; 0.149\n",
        "90; 0.108; 0.148\n",
        "91; 0.108; 0.147\n",
        "92; 0.105; 0.147\n",
        "93; 0.104; 0.146\n",
        "94; 0.103; 0.145\n",
        "95; 0.103; 0.145\n",
        "96; 0.102; 0.144\n",
        "97; 0.102; 0.144\n",
        "98; 0.102; 0.143\n",
        "99; 0.102; 0.142\n",
        "100; 0.102; 0.142\n",
        "101; 0.099; 0.143\n",
        "102; 0.098; 0.144\n",
        "103; 0.098; 0.143\n",
        "104; 0.096; 0.143\n",
        "105; 0.093; 0.143\n",
        "106; 0.093; 0.143\n",
        "107; 0.092; 0.143\n",
        "108; 0.092; 0.142\n",
        "109; 0.091; 0.142\n",
        "110; 0.091; 0.141\n",
        "111; 0.09; 0.141\n",
        "112; 0.09; 0.141\n",
        "113; 0.09; 0.141\n",
        "114; 0.09; 0.141\n",
        "115; 0.09; 0.141\n",
        "116; 0.089; 0.14\n",
        "117; 0.088; 0.14\n",
        "118; 0.087; 0.139\n",
        "119; 0.087; 0.139\n",
        "120; 0.087; 0.138\n",
        "121; 0.087; 0.137\n",
        "122; 0.087; 0.136\n",
        "123; 0.086; 0.136\n",
        "124; 0.085; 0.136\n",
        "125; 0.085; 0.136\n",
        "126; 0.085; 0.136\n",
        "127; 0.085; 0.136\n",
        "128; 0.084; 0.136\n",
        "129; 0.081; 0.136\n",
        "130; 0.081; 0.136\n",
        "131; 0.081; 0.136\n",
        "132; 0.081; 0.136\n",
        "133; 0.081; 0.136\n",
        "134; 0.08; 0.135\n",
        "135; 0.08; 0.135\n",
        "136; 0.08; 0.135\n",
        "137; 0.08; 0.135\n",
        "138; 0.079; 0.136\n",
        "139; 0.079; 0.136\n",
        "140; 0.079; 0.136\n",
        "141; 0.079; 0.136\n",
        "142; 0.077; 0.136\n",
        "143; 0.076; 0.134\n",
        "144; 0.076; 0.135\n",
        "145; 0.076; 0.134\n",
        "146; 0.076; 0.133\n",
        "147; 0.076; 0.133\n",
        "148; 0.076; 0.133\n",
        "149; 0.075; 0.133\n",
        "150; 0.074; 0.133\n",
        "151; 0.073; 0.133\n",
        "152; 0.073; 0.133\n",
        "153; 0.072; 0.133\n",
        "154; 0.072; 0.133\n",
        "155; 0.072; 0.133\n",
        "156; 0.071; 0.133\n",
        "157; 0.071; 0.133\n",
        "158; 0.071; 0.133\n",
        "159; 0.069; 0.133\n",
        "160; 0.068; 0.133\n",
        "161; 0.068; 0.133\n",
        "162; 0.067; 0.133\n",
        "163; 0.067; 0.133\n",
        "164; 0.067; 0.133\n",
        "165; 0.066; 0.132\n",
        "166; 0.066; 0.132\n",
        "167; 0.066; 0.132\n",
        "168; 0.066; 0.132\n",
        "169; 0.066; 0.132\n",
        "170; 0.065; 0.132\n",
        "171; 0.065; 0.132\n",
        "172; 0.065; 0.132\n",
        "173; 0.065; 0.132\n",
        "174; 0.064; 0.132\n",
        "175; 0.064; 0.131\n",
        "176; 0.063; 0.131\n",
        "177; 0.063; 0.131\n",
        "178; 0.062; 0.132\n",
        "179; 0.062; 0.131\n",
        "180; 0.062; 0.131\n",
        "181; 0.062; 0.131\n",
        "182; 0.062; 0.131\n",
        "183; 0.062; 0.131\n",
        "184; 0.062; 0.131\n",
        "185; 0.059; 0.13\n",
        "186; 0.058; 0.13\n",
        "187; 0.058; 0.13\n",
        "188; 0.057; 0.13\n",
        "189; 0.057; 0.13\n",
        "190; 0.056; 0.129\n",
        "191; 0.055; 0.129\n",
        "192; 0.055; 0.128\n",
        "193; 0.055; 0.127\n",
        "194; 0.055; 0.128\n",
        "195; 0.055; 0.129\n",
        "196; 0.055; 0.129\n",
        "197; 0.054; 0.129\n",
        "198; 0.053; 0.129\n",
        "199; 0.053; 0.129\n",
        "200; 0.053; 0.129\n",
        "201; 0.051; 0.129\n",
        "202; 0.05; 0.129\n",
        "203; 0.048; 0.129\n",
        "204; 0.048; 0.129\n",
        "205; 0.048; 0.129\n",
        "206; 0.048; 0.129\n",
        "207; 0.048; 0.129\n",
        "208; 0.048; 0.129\n",
        "209; 0.048; 0.129\n",
        "210; 0.048; 0.129\n",
        "211; 0.048; 0.129\n",
        "212; 0.048; 0.129\n",
        "213; 0.048; 0.129\n",
        "214; 0.048; 0.129\n",
        "215; 0.048; 0.129\n",
        "216; 0.048; 0.129\n",
        "217; 0.048; 0.129\n",
        "218; 0.048; 0.13\n",
        "219; 0.048; 0.129\n",
        "220; 0.048; 0.129\n",
        "221; 0.047; 0.129\n",
        "222; 0.047; 0.128\n",
        "223; 0.047; 0.128\n",
        "224; 0.047; 0.128\n",
        "225; 0.047; 0.128\n",
        "226; 0.047; 0.128\n",
        "227; 0.047; 0.128\n",
        "228; 0.047; 0.128\n",
        "229; 0.047; 0.129\n",
        "230; 0.047; 0.129\n",
        "231; 0.047; 0.129\n",
        "232; 0.047; 0.129\n",
        "233; 0.047; 0.129\n",
        "234; 0.047; 0.129\n",
        "235; 0.047; 0.128\n",
        "236; 0.047; 0.128\n",
        "237; 0.047; 0.128\n",
        "238; 0.046; 0.128\n",
        "239; 0.046; 0.128\n",
        "240; 0.046; 0.128\n",
        "241; 0.046; 0.128\n",
        "242; 0.046; 0.128\n",
        "243; 0.046; 0.128\n",
        "244; 0.046; 0.128\n",
        "245; 0.046; 0.128\n",
        "246; 0.046; 0.128\n",
        "247; 0.045; 0.128\n",
        "248; 0.044; 0.128\n",
        "249; 0.042; 0.129\n",
        "250; 0.042; 0.129\n",
        "251; 0.042; 0.129\n",
        "252; 0.042; 0.129\n",
        "253; 0.041; 0.129\n",
        "254; 0.041; 0.129\n",
        "255; 0.041; 0.129\n",
        "256; 0.041; 0.128\n",
        "257; 0.041; 0.127\n",
        "258; 0.041; 0.127\n",
        "259; 0.04; 0.127\n",
        "260; 0.04; 0.127\n",
        "261; 0.04; 0.128\n",
        "262; 0.04; 0.128\n",
        "263; 0.04; 0.128\n",
        "264; 0.04; 0.128\n",
        "265; 0.04; 0.128\n",
        "266; 0.04; 0.128\n",
        "267; 0.039; 0.129\n",
        "268; 0.039; 0.128\n",
        "269; 0.038; 0.128\n",
        "270; 0.038; 0.129\n",
        "271; 0.038; 0.129\n",
        "272; 0.037; 0.128\n",
        "273; 0.037; 0.128\n",
        "274; 0.037; 0.128\n",
        "275; 0.037; 0.128\n",
        "276; 0.037; 0.128\n",
        "277; 0.037; 0.128\n",
        "278; 0.037; 0.128\n",
        "279; 0.037; 0.128\n",
        "280; 0.037; 0.128\n",
        "281; 0.036; 0.128\n",
        "282; 0.036; 0.128\n",
        "283; 0.036; 0.128\n",
        "284; 0.036; 0.128\n",
        "285; 0.036; 0.128\n",
        "286; 0.036; 0.128\n",
        "287; 0.036; 0.128\n",
        "288; 0.036; 0.128\n",
        "289; 0.036; 0.128\n",
        "290; 0.036; 0.128\n",
        "291; 0.035; 0.128\n",
        "292; 0.035; 0.128\n",
        "293; 0.035; 0.128\n",
        "294; 0.035; 0.128\n",
        "295; 0.035; 0.128\n",
        "296; 0.035; 0.128\n",
        "297; 0.035; 0.127\n",
        "298; 0.035; 0.127\n",
        "299; 0.035; 0.127\n",
        "300; 0.035; 0.126\n",
        "301; 0.034; 0.127\n",
        "302; 0.034; 0.127\n",
        "303; 0.033; 0.127\n",
        "304; 0.033; 0.126\n",
        "305; 0.033; 0.125\n",
        "306; 0.033; 0.125\n",
        "307; 0.033; 0.125\n",
        "308; 0.033; 0.125\n",
        "309; 0.033; 0.125\n",
        "310; 0.033; 0.125\n",
        "311; 0.033; 0.125\n",
        "312; 0.033; 0.125\n",
        "313; 0.033; 0.125\n",
        "314; 0.033; 0.125\n",
        "315; 0.033; 0.124\n",
        "316; 0.033; 0.124\n",
        "317; 0.032; 0.124\n",
        "318; 0.031; 0.124\n",
        "319; 0.031; 0.124\n",
        "320; 0.031; 0.124\n",
        "321; 0.031; 0.124\n",
        "322; 0.031; 0.124\n",
        "323; 0.031; 0.124\n",
        "324; 0.031; 0.124\n",
        "325; 0.031; 0.123\n",
        "326; 0.031; 0.123\n",
        "327; 0.031; 0.123\n",
        "328; 0.031; 0.123\n",
        "329; 0.03; 0.123\n",
        "330; 0.03; 0.123\n",
        "331; 0.029; 0.123\n",
        "332; 0.029; 0.123\n",
        "333; 0.029; 0.123\n",
        "334; 0.028; 0.123\n",
        "335; 0.028; 0.123\n",
        "336; 0.028; 0.123\n",
        "337; 0.027; 0.123\n",
        "338; 0.027; 0.123\n",
        "339; 0.027; 0.123\n",
        "340; 0.027; 0.123\n",
        "341; 0.027; 0.123\n",
        "342; 0.027; 0.123\n",
        "343; 0.026; 0.123\n",
        "344; 0.024; 0.123\n",
        "345; 0.024; 0.123\n",
        "346; 0.024; 0.123\n",
        "347; 0.024; 0.123\n",
        "348; 0.024; 0.122\n",
        "349; 0.024; 0.122\n",
        "350; 0.024; 0.122\n",
        "351; 0.024; 0.122\n",
        "352; 0.024; 0.122\n",
        "353; 0.024; 0.122\n",
        "354; 0.024; 0.123\n",
        "355; 0.024; 0.123\n",
        "356; 0.024; 0.123\n",
        "357; 0.024; 0.123\n",
        "358; 0.024; 0.123\n",
        "359; 0.024; 0.123\n",
        "360; 0.024; 0.123\n",
        "361; 0.023; 0.123\n",
        "362; 0.022; 0.123\n",
        "363; 0.021; 0.123\n",
        "364; 0.02; 0.123\n",
        "365; 0.02; 0.123\n",
        "366; 0.02; 0.123\n",
        "367; 0.02; 0.123\n",
        "368; 0.02; 0.123\n",
        "369; 0.02; 0.123\n",
        "370; 0.02; 0.123\n",
        "371; 0.02; 0.123\n",
        "372; 0.02; 0.123\n",
        "373; 0.019; 0.123\n",
        "374; 0.018; 0.123\n",
        "375; 0.018; 0.123\n",
        "376; 0.018; 0.123\n",
        "377; 0.018; 0.123\n",
        "378; 0.018; 0.123\n",
        "379; 0.018; 0.123\n",
        "380; 0.018; 0.123\n",
        "381; 0.018; 0.123\n",
        "382; 0.018; 0.123\n",
        "383; 0.018; 0.123\n",
        "384; 0.018; 0.123\n",
        "385; 0.018; 0.123\n",
        "386; 0.018; 0.123\n",
        "387; 0.018; 0.123\n",
        "388; 0.018; 0.123\n",
        "389; 0.018; 0.123\n",
        "390; 0.018; 0.123\n",
        "391; 0.018; 0.123\n",
        "392; 0.017; 0.123\n",
        "393; 0.017; 0.123\n",
        "394; 0.017; 0.123\n",
        "395; 0.017; 0.123\n",
        "396; 0.017; 0.123\n",
        "397; 0.017; 0.123\n",
        "398; 0.017; 0.123\n",
        "399; 0.017; 0.123\n",
        "400; 0.017; 0.123\n",
        "401; 0.017; 0.123\n",
        "402; 0.017; 0.123\n",
        "403; 0.017; 0.123\n",
        "404; 0.017; 0.123\n",
        "405; 0.017; 0.123\n",
        "406; 0.017; 0.123\n",
        "407; 0.017; 0.123\n",
        "408; 0.017; 0.124\n",
        "409; 0.017; 0.124\n",
        "410; 0.017; 0.124\n",
        "411; 0.017; 0.124\n",
        "412; 0.016; 0.124\n",
        "413; 0.015; 0.124\n",
        "414; 0.015; 0.124\n",
        "415; 0.015; 0.124\n",
        "416; 0.015; 0.124\n",
        "417; 0.014; 0.124\n",
        "418; 0.014; 0.124\n",
        "419; 0.014; 0.125\n",
        "420; 0.014; 0.125\n",
        "421; 0.014; 0.125\n",
        "422; 0.014; 0.125\n",
        "423; 0.014; 0.125\n",
        "424; 0.014; 0.125\n",
        "425; 0.014; 0.125\n",
        "426; 0.014; 0.125\n",
        "427; 0.014; 0.125\n",
        "428; 0.014; 0.125\n",
        "429; 0.014; 0.125\n",
        "430; 0.014; 0.125\n",
        "431; 0.014; 0.125\n",
        "432; 0.014; 0.125\n",
        "433; 0.014; 0.125\n",
        "434; 0.014; 0.125\n",
        "435; 0.014; 0.125\n",
        "436; 0.014; 0.125\n",
        "437; 0.014; 0.125\n",
        "438; 0.014; 0.125\n",
        "439; 0.013; 0.125\n",
        "440; 0.013; 0.125\n",
        "441; 0.013; 0.125\n",
        "442; 0.013; 0.125\n",
        "443; 0.013; 0.125\n",
        "444; 0.013; 0.125\n",
        "445; 0.013; 0.125\n",
        "446; 0.013; 0.125\n",
        "447; 0.013; 0.125\n",
        "448; 0.013; 0.126\n",
        "449; 0.013; 0.125\n",
        "450; 0.013; 0.125\n",
        "451; 0.013; 0.125\n",
        "452; 0.013; 0.125\n",
        "453; 0.013; 0.125\n",
        "454; 0.013; 0.125\n",
        "455; 0.012; 0.125\n",
        "456; 0.012; 0.124\n",
        "457; 0.012; 0.124\n",
        "458; 0.012; 0.124\n",
        "459; 0.012; 0.124\n",
        "460; 0.012; 0.124\n",
        "461; 0.012; 0.124\n",
        "462; 0.011; 0.124\n",
        "463; 0.011; 0.124\n",
        "464; 0.011; 0.124\n",
        "465; 0.011; 0.124\n",
        "466; 0.011; 0.124\n",
        "467; 0.011; 0.124\n",
        "468; 0.011; 0.124\n",
        "469; 0.011; 0.124\n",
        "470; 0.011; 0.124\n",
        "471; 0.011; 0.124\n",
        "472; 0.011; 0.124\n",
        "473; 0.011; 0.124\n",
        "474; 0.011; 0.124\n",
        "475; 0.011; 0.124\n",
        "476; 0.011; 0.124\n",
        "477; 0.011; 0.125\n",
        "478; 0.011; 0.125\n",
        "479; 0.011; 0.125\n",
        "480; 0.011; 0.125\n",
        "481; 0.011; 0.125\n",
        "482; 0.011; 0.125\n",
        "483; 0.011; 0.125\n",
        "484; 0.011; 0.125\n",
        "485; 0.011; 0.125\n",
        "486; 0.01; 0.126\n",
        "487; 0.01; 0.126\n",
        "488; 0.009; 0.126\n",
        "489; 0.009; 0.126\n",
        "490; 0.009; 0.126\n",
        "491; 0.009; 0.126\n",
        "492; 0.009; 0.126\n",
        "493; 0.009; 0.126\n",
        "494; 0.009; 0.126\n",
        "495; 0.008; 0.126\n",
        "496; 0.008; 0.126\n",
        "497; 0.008; 0.126\n",
        "498; 0.008; 0.126\n",
        "499; 0.008; 0.126\n",
        "500; 0.008; 0.126\n",
        "501; 0.008; 0.125\n",
        "502; 0.008; 0.124\n",
        "503; 0.008; 0.124\n",
        "504; 0.008; 0.124\n",
        "505; 0.008; 0.124\n",
        "506; 0.008; 0.124\n",
        "507; 0.008; 0.124\n",
        "508; 0.008; 0.124\n",
        "509; 0.008; 0.124\n",
        "510; 0.008; 0.124\n",
        "511; 0.008; 0.124\n",
        "512; 0.008; 0.125\n",
        "513; 0.008; 0.125\n",
        "514; 0.008; 0.125\n",
        "515; 0.008; 0.125\n",
        "516; 0.008; 0.125\n",
        "517; 0.008; 0.125\n",
        "518; 0.008; 0.125\n",
        "519; 0.008; 0.125\n",
        "520; 0.008; 0.125\n",
        "521; 0.008; 0.125\n",
        "522; 0.008; 0.125\n",
        "523; 0.008; 0.125\n",
        "524; 0.007; 0.125\n",
        "525; 0.007; 0.125\n",
        "526; 0.007; 0.125\n",
        "527; 0.007; 0.125\n",
        "528; 0.007; 0.125\n",
        "529; 0.007; 0.125\n",
        "530; 0.006; 0.125\n",
        "531; 0.006; 0.125\n",
        "532; 0.006; 0.125\n",
        "533; 0.006; 0.125\n",
        "534; 0.006; 0.125\n",
        "535; 0.006; 0.125\n",
        "536; 0.006; 0.124\n",
        "537; 0.006; 0.124\n",
        "538; 0.006; 0.124\n",
        "539; 0.005; 0.124\n",
        "540; 0.005; 0.124\n",
        "541; 0.005; 0.124\n",
        "542; 0.005; 0.124\n",
        "543; 0.005; 0.124\n",
        "544; 0.005; 0.124\n",
        "545; 0.005; 0.124\n",
        "546; 0.005; 0.124\n",
        "547; 0.005; 0.124\n",
        "548; 0.005; 0.124\n",
        "549; 0.005; 0.124\n",
        "550; 0.005; 0.124\n",
        "551; 0.005; 0.124\n",
        "552; 0.005; 0.124\n",
        "553; 0.005; 0.124\n",
        "554; 0.005; 0.124\n",
        "555; 0.005; 0.124\n",
        "556; 0.005; 0.124\n",
        "557; 0.005; 0.124\n",
        "558; 0.005; 0.124\n",
        "559; 0.005; 0.124\n",
        "560; 0.005; 0.124\n",
        "561; 0.005; 0.124\n",
        "562; 0.004; 0.124\n",
        "563; 0.004; 0.124\n",
        "564; 0.003; 0.124\n",
        "565; 0.003; 0.124\n",
        "566; 0.003; 0.125\n",
        "567; 0.003; 0.125\n",
        "568; 0.003; 0.125\n",
        "569; 0.003; 0.125\n",
        "570; 0.003; 0.125\n",
        "571; 0.003; 0.125\n",
        "572; 0.003; 0.125\n",
        "573; 0.003; 0.125\n",
        "574; 0.003; 0.125\n",
        "575; 0.003; 0.125\n",
        "576; 0.003; 0.125\n",
        "577; 0.002; 0.125\n",
        "578; 0.002; 0.125\n",
        "579; 0.002; 0.124\n",
        "580; 0.002; 0.124\n",
        "581; 0.002; 0.124\n",
        "582; 0.002; 0.124\n",
        "583; 0.002; 0.124\n",
        "584; 0.002; 0.124\n",
        "585; 0.002; 0.124\n",
        "586; 0.002; 0.124\n",
        "587; 0.002; 0.124\n",
        "588; 0.002; 0.124\n",
        "589; 0.002; 0.124\n",
        "590; 0.002; 0.124\n",
        "591; 0.002; 0.124\n",
        "592; 0.002; 0.124\n",
        "593; 0.002; 0.124\n",
        "594; 0.002; 0.124\n",
        "595; 0.002; 0.124\n",
        "596; 0.002; 0.124\n",
        "597; 0.002; 0.124\n",
        "598; 0.002; 0.125\n",
        "599; 0.002; 0.125\n",
        "600; 0.002; 0.125\n",
        "601; 0.002; 0.125\n",
        "602; 0.002; 0.125\n",
        "603; 0.002; 0.125\n",
        "604; 0.002; 0.125\n",
        "605; 0.002; 0.125\n",
        "606; 0.002; 0.125\n",
        "607; 0.002; 0.125\n",
        "608; 0.002; 0.125\n",
        "609; 0.002; 0.125\n",
        "610; 0.002; 0.125\n",
        "611; 0.002; 0.125\n",
        "612; 0.002; 0.125\n",
        "613; 0.002; 0.125\n",
        "614; 0.002; 0.125\n",
        "615; 0.002; 0.125\n",
        "616; 0.002; 0.125\n",
        "617; 0.001; 0.125\n",
        "618; 0.001; 0.125\n",
        "619; 0.001; 0.125\n",
        "620; 0.001; 0.125\n",
        "621; 0.001; 0.125\n",
        "622; 0.001; 0.125\n",
        "623; 0.001; 0.125\n",
        "624; 0.001; 0.125\n",
        "625; 0.001; 0.125\n",
        "626; 0.001; 0.125\n",
        "627; 0.001; 0.125\n",
        "628; 0.001; 0.125\n",
        "629; 0.001; 0.124\n",
        "630; 0.001; 0.124\n",
        "631; 0.001; 0.124\n",
        "632; 0.001; 0.124\n",
        "633; 0.001; 0.124\n",
        "634; 0.001; 0.124\n",
        "635; 0.001; 0.124\n",
        "636; 0.001; 0.124\n",
        "637; 0.0; 0.124\n",
        "638; 0.0; 0.124\n",
        "639; 0.0; 0.124\n",
        "640; 0.0; 0.124\n",
        "641; 0.0; 0.124\n",
        "642; 0.0; 0.124\n",
        "643; 0.0; 0.124\n",
        "644; 0.0; 0.124\n",
        "645; 0.0; 0.124\n",
        "646; 0.0; 0.124\n",
        "647; 0.0; 0.124\n",
        "648; 0.0; 0.124\n",
        "649; 0.0; 0.124\n",
        "650; 0.0; 0.124\n",
        "651; 0.0; 0.124\n",
        "652; 0.0; 0.124\n",
        "653; 0.0; 0.124\n",
        "654; 0.0; 0.124\n",
        "655; 0.0; 0.124\n",
        "656; 0.0; 0.124\n",
        "657; 0.0; 0.124\n",
        "658; 0.0; 0.124\n",
        "659; 0.0; 0.124\n",
        "660; 0.0; 0.125\n",
        "661; 0.0; 0.125\n",
        "662; 0.0; 0.125\n",
        "663; 0.0; 0.126\n",
        "664; 0.0; 0.126\n",
        "665; 0.0; 0.126\n",
        "666; 0.0; 0.126\n",
        "667; 0.0; 0.126\n",
        "668; 0.0; 0.126\n",
        "669; 0.0; 0.126\n",
        "670; 0.0; 0.126\n",
        "671; 0.0; 0.126\n",
        "672; 0.0; 0.126\n",
        "673; 0.0; 0.126\n",
        "674; 0.0; 0.126\n",
        "675; 0.0; 0.126\n",
        "676; 0.0; 0.125\n",
        "677; 0.0; 0.125\n",
        "678; 0.0; 0.125\n",
        "679; 0.0; 0.125\n",
        "680; 0.0; 0.125\n",
        "681; 0.0; 0.125\n",
        "682; 0.0; 0.125\n",
        "683; 0.0; 0.125\n",
        "684; 0.0; 0.125\n",
        "685; 0.0; 0.125\n",
        "686; 0.0; 0.125\n",
        "687; 0.0; 0.125\n",
        "688; 0.0; 0.125\n",
        "689; 0.0; 0.125\n",
        "690; 0.0; 0.125\n",
        "691; 0.0; 0.125\n",
        "692; 0.0; 0.125\n",
        "693; 0.0; 0.125\n",
        "694; 0.0; 0.125\n",
        "695; 0.0; 0.125\n",
        "696; 0.0; 0.125\n",
        "697; 0.0; 0.125\n",
        "698; 0.0; 0.125\n",
        "699; 0.0; 0.125\n",
        "700; 0.0; 0.125\n",
        "701; 0.0; 0.125\n",
        "702; 0.0; 0.125\n",
        "703; 0.0; 0.125\n",
        "704; 0.0; 0.125\n",
        "705; 0.0; 0.125\n",
        "706; 0.0; 0.125\n",
        "707; 0.0; 0.125\n",
        "708; 0.0; 0.125\n",
        "709; 0.0; 0.125\n",
        "710; 0.0; 0.125\n",
        "711; 0.0; 0.125\n",
        "712; 0.0; 0.125\n",
        "713; 0.0; 0.125\n",
        "714; 0.0; 0.125\n",
        "715; 0.0; 0.125\n",
        "716; 0.0; 0.125\n",
        "717; 0.0; 0.125\n",
        "718; 0.0; 0.125\n",
        "719; 0.0; 0.125\n",
        "720; 0.0; 0.125\n",
        "721; 0.0; 0.125\n",
        "722; 0.0; 0.125\n",
        "723; 0.0; 0.125\n",
        "724; 0.0; 0.125\n",
        "725; 0.0; 0.125\n",
        "726; 0.0; 0.125\n",
        "727; 0.0; 0.125\n",
        "728; 0.0; 0.125\n",
        "729; 0.0; 0.125\n",
        "730; 0.0; 0.125\n",
        "731; 0.0; 0.125\n",
        "732; 0.0; 0.125\n",
        "733; 0.0; 0.125\n",
        "734; 0.0; 0.125\n",
        "735; 0.0; 0.125\n",
        "736; 0.0; 0.125\n",
        "737; 0.0; 0.125\n",
        "738; 0.0; 0.125\n",
        "739; 0.0; 0.125\n",
        "740; 0.0; 0.125\n",
        "741; 0.0; 0.125\n",
        "742; 0.0; 0.125\n",
        "743; 0.0; 0.125\n",
        "744; 0.0; 0.125\n",
        "745; 0.0; 0.125\n",
        "746; 0.0; 0.126\n",
        "747; 0.0; 0.126\n",
        "748; 0.0; 0.126\n",
        "749; 0.0; 0.126\n",
        "750; 0.0; 0.126\n",
        "751; 0.0; 0.126\n",
        "752; 0.0; 0.126\n",
        "753; 0.0; 0.126\n",
        "754; 0.0; 0.126\n",
        "755; 0.0; 0.126\n",
        "756; 0.0; 0.126\n",
        "757; 0.0; 0.126\n",
        "758; 0.0; 0.126\n",
        "759; 0.0; 0.126\n",
        "760; 0.0; 0.126\n",
        "761; 0.0; 0.126\n",
        "762; 0.0; 0.125\n",
        "763; 0.0; 0.125\n",
        "764; 0.0; 0.125\n",
        "765; 0.0; 0.125\n",
        "766; 0.0; 0.125\n",
        "767; 0.0; 0.125\n",
        "768; 0.0; 0.125\n",
        "769; 0.0; 0.125\n",
        "770; 0.0; 0.125\n",
        "771; 0.0; 0.125\n",
        "772; 0.0; 0.125\n",
        "773; 0.0; 0.125\n",
        "774; 0.0; 0.125\n",
        "775; 0.0; 0.125\n",
        "776; 0.0; 0.125\n",
        "777; 0.0; 0.124\n",
        "778; 0.0; 0.124\n",
        "779; 0.0; 0.124\n",
        "780; 0.0; 0.124\n",
        "781; 0.0; 0.124\n",
        "782; 0.0; 0.124\n",
        "783; 0.0; 0.124\n",
        "784; 0.0; 0.124\n",
        "785; 0.0; 0.124\n",
        "786; 0.0; 0.124\n",
        "787; 0.0; 0.124\n",
        "788; 0.0; 0.124\n",
        "789; 0.0; 0.124\n",
        "790; 0.0; 0.124\n",
        "791; 0.0; 0.124\n",
        "792; 0.0; 0.124\n",
        "793; 0.0; 0.124\n",
        "794; 0.0; 0.124\n",
        "795; 0.0; 0.124\n",
        "796; 0.0; 0.124\n",
        "797; 0.0; 0.124\n",
        "798; 0.0; 0.124\n",
        "799; 0.0; 0.125\n",
        "800; 0.0; 0.125\n",
        "801; 0.0; 0.125\n",
        "802; 0.0; 0.125\n",
        "803; 0.0; 0.125\n",
        "804; 0.0; 0.125\n",
        "805; 0.0; 0.125\n",
        "806; 0.0; 0.125\n",
        "807; 0.0; 0.125\n",
        "808; 0.0; 0.125\n",
        "809; 0.0; 0.125\n",
        "810; 0.0; 0.125\n",
        "811; 0.0; 0.125\n",
        "812; 0.0; 0.125\n",
        "813; 0.0; 0.125\n",
        "814; 0.0; 0.125\n",
        "815; 0.0; 0.125\n",
        "816; 0.0; 0.125\n",
        "817; 0.0; 0.125\n",
        "818; 0.0; 0.125\n",
        "819; 0.0; 0.125\n",
        "820; 0.0; 0.125\n",
        "821; 0.0; 0.125\n",
        "822; 0.0; 0.125\n",
        "823; 0.0; 0.125\n",
        "824; 0.0; 0.125\n",
        "825; 0.0; 0.125\n",
        "826; 0.0; 0.125\n",
        "827; 0.0; 0.125\n",
        "828; 0.0; 0.125\n",
        "829; 0.0; 0.125\n",
        "830; 0.0; 0.125\n",
        "831; 0.0; 0.125\n",
        "832; 0.0; 0.125\n",
        "833; 0.0; 0.125\n",
        "834; 0.0; 0.125\n",
        "835; 0.0; 0.125\n",
        "836; 0.0; 0.125\n",
        "837; 0.0; 0.125\n",
        "838; 0.0; 0.125\n",
        "839; 0.0; 0.125\n",
        "840; 0.0; 0.125\n",
        "841; 0.0; 0.125\n",
        "842; 0.0; 0.125\n",
        "843; 0.0; 0.125\n",
        "844; 0.0; 0.125\n",
        "845; 0.0; 0.125\n",
        "846; 0.0; 0.125\n",
        "847; 0.0; 0.125\n",
        "848; 0.0; 0.125\n",
        "849; 0.0; 0.125\n",
        "850; 0.0; 0.125\n",
        "851; 0.0; 0.125\n",
        "852; 0.0; 0.125\n",
        "853; 0.0; 0.125\n",
        "854; 0.0; 0.125\n",
        "855; 0.0; 0.125\n",
        "856; 0.0; 0.125\n",
        "857; 0.0; 0.124\n",
        "858; 0.0; 0.124\n",
        "859; 0.0; 0.124\n",
        "860; 0.0; 0.124\n",
        "861; 0.0; 0.124\n",
        "862; 0.0; 0.124\n",
        "863; 0.0; 0.124\n",
        "864; 0.0; 0.124\n",
        "865; 0.0; 0.124\n",
        "866; 0.0; 0.125\n",
        "867; 0.0; 0.125\n",
        "868; 0.0; 0.125\n",
        "869; 0.0; 0.125\n",
        "870; 0.0; 0.125\n",
        "871; 0.0; 0.125\n",
        "872; 0.0; 0.125\n",
        "873; 0.0; 0.125\n",
        "874; 0.0; 0.125\n",
        "875; 0.0; 0.125\n",
        "876; 0.0; 0.125\n",
        "877; 0.0; 0.125\n",
        "878; 0.0; 0.125\n",
        "879; 0.0; 0.125\n",
        "880; 0.0; 0.125\n",
        "881; 0.0; 0.125\n",
        "882; 0.0; 0.125\n",
        "883; 0.0; 0.125\n",
        "884; 0.0; 0.125\n",
        "885; 0.0; 0.125\n",
        "886; 0.0; 0.125\n",
        "887; 0.0; 0.125\n",
        "888; 0.0; 0.125\n",
        "889; 0.0; 0.125\n",
        "890; 0.0; 0.125\n",
        "891; 0.0; 0.125\n",
        "892; 0.0; 0.125\n",
        "893; 0.0; 0.125\n",
        "894; 0.0; 0.125\n",
        "895; 0.0; 0.125\n",
        "896; 0.0; 0.125\n",
        "897; 0.0; 0.126\n",
        "898; 0.0; 0.126\n",
        "899; 0.0; 0.126\n",
        "900; 0.0; 0.126\n",
        "901; 0.0; 0.126\n",
        "902; 0.0; 0.126\n",
        "903; 0.0; 0.126\n",
        "904; 0.0; 0.126\n",
        "905; 0.0; 0.126\n",
        "906; 0.0; 0.126\n",
        "907; 0.0; 0.126\n",
        "908; 0.0; 0.126\n",
        "909; 0.0; 0.126\n",
        "910; 0.0; 0.125\n",
        "911; 0.0; 0.125\n",
        "912; 0.0; 0.125\n",
        "913; 0.0; 0.125\n",
        "914; 0.0; 0.125\n",
        "915; 0.0; 0.125\n",
        "916; 0.0; 0.125\n",
        "917; 0.0; 0.125\n",
        "918; 0.0; 0.125\n",
        "919; 0.0; 0.125\n",
        "920; 0.0; 0.125\n",
        "921; 0.0; 0.125\n",
        "922; 0.0; 0.125\n",
        "923; 0.0; 0.125\n",
        "924; 0.0; 0.125\n",
        "925; 0.0; 0.125\n",
        "926; 0.0; 0.125\n",
        "927; 0.0; 0.125\n",
        "928; 0.0; 0.125\n",
        "929; 0.0; 0.125\n",
        "930; 0.0; 0.125\n",
        "931; 0.0; 0.125\n",
        "932; 0.0; 0.125\n",
        "933; 0.0; 0.125\n",
        "934; 0.0; 0.125\n",
        "935; 0.0; 0.125\n",
        "936; 0.0; 0.125\n",
        "937; 0.0; 0.125\n",
        "938; 0.0; 0.124\n",
        "939; 0.0; 0.124\n",
        "940; 0.0; 0.124\n",
        "941; 0.0; 0.124\n",
        "942; 0.0; 0.124\n",
        "943; 0.0; 0.124\n",
        "944; 0.0; 0.124\n",
        "945; 0.0; 0.124\n",
        "946; 0.0; 0.124\n",
        "947; 0.0; 0.124\n",
        "948; 0.0; 0.124\n",
        "949; 0.0; 0.124\n",
        "950; 0.0; 0.124\n",
        "951; 0.0; 0.124\n",
        "952; 0.0; 0.124\n",
        "953; 0.0; 0.124\n",
        "954; 0.0; 0.124\n",
        "955; 0.0; 0.124\n",
        "956; 0.0; 0.124\n",
        "957; 0.0; 0.124\n",
        "958; 0.0; 0.124\n",
        "959; 0.0; 0.124\n",
        "960; 0.0; 0.124\n",
        "961; 0.0; 0.124\n",
        "962; 0.0; 0.124\n",
        "963; 0.0; 0.124\n",
        "964; 0.0; 0.124\n",
        "965; 0.0; 0.124\n",
        "966; 0.0; 0.124\n",
        "967; 0.0; 0.124\n",
        "968; 0.0; 0.124\n",
        "969; 0.0; 0.124\n",
        "970; 0.0; 0.124\n",
        "971; 0.0; 0.124\n",
        "972; 0.0; 0.124\n",
        "973; 0.0; 0.124\n",
        "974; 0.0; 0.124\n",
        "975; 0.0; 0.124\n",
        "976; 0.0; 0.124\n",
        "977; 0.0; 0.124\n",
        "978; 0.0; 0.124\n",
        "979; 0.0; 0.124\n",
        "980; 0.0; 0.124\n",
        "981; 0.0; 0.124\n",
        "982; 0.0; 0.124\n",
        "983; 0.0; 0.124\n",
        "984; 0.0; 0.124\n",
        "985; 0.0; 0.124\n",
        "986; 0.0; 0.124\n",
        "987; 0.0; 0.124\n",
        "988; 0.0; 0.124\n",
        "989; 0.0; 0.124\n",
        "990; 0.0; 0.124\n",
        "991; 0.0; 0.124\n",
        "992; 0.0; 0.124\n",
        "993; 0.0; 0.124\n",
        "994; 0.0; 0.124\n",
        "995; 0.0; 0.124\n",
        "996; 0.0; 0.124\n",
        "997; 0.0; 0.124\n",
        "998; 0.0; 0.124\n",
        "999; 0.0; 0.124\n"
       ]
      }
     ],
     "prompt_number": 59
    }
   ],
   "metadata": {}
  }
 ]
}