{
 "metadata": {
  "name": "",
  "signature": "sha256:162664e450a1e24bd20544961df354edf8a4af2ce6d43871ecd04dba4b1ef157"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function\n",
      "\n",
      "# define variables to add\n",
      "# dscalar is a theano type:\n",
      "# - scalar = 0-dmensional array\n",
      "# - double precision\n",
      "x = T.dscalar('x')\n",
      "y = T.dscalar('y')\n",
      "\n",
      "# \n",
      "z = x + y\n",
      "\n",
      "# f will be compiled to C code\n",
      "f = function([x,y], z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f(2,3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array(5.0)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "theano.tensor.var.TensorVariable"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Matrix additions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.dmatrix('x')\n",
      "y = T.dmatrix('y')\n",
      "z = x + y\n",
      "f = function([x, y], z)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f([[1,2],[3,4]], [[5,6],[7,8]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([[  6.,   8.],\n",
        "       [ 10.,  12.]])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = T.vector()\n",
      "b = T.vector()\n",
      "out = (a ** 2) + (b ** 2) + (2 * a * b)\n",
      "f = function([a, b], out)\n",
      "f([1,2], [4,5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([ 25.,  49.])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.dmatrix('x')\n",
      "# this is the equation of the logistic equation\n",
      "s = 1 / (1 + T.exp(-x))\n",
      "logistic = function([x], s)\n",
      "logistic([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([[ 0.5       ,  0.73105858],\n",
        "       [ 0.26894142,  0.11920292]])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# apparently, logistic equation is equivalent to this one\n",
      "s2 = (1 + T.tanh(x / 2)) / 2\n",
      "logistic2 = function([x], s2)\n",
      "logistic2([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([[ 0.5       ,  0.73105858],\n",
        "       [ 0.26894142,  0.11920292]])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's try matrix abs diff and squared diff\n",
      "\n",
      "x, y = T.dmatrices('x', 'y')\n",
      "\n",
      "abs_diff = abs(x - y)\n",
      "squared_diff = (x - y) * (x - y)\n",
      "f = function([x,y], [abs_diff, squared_diff])\n",
      "\n",
      "f([[1,2],[3,4]], [[4,3],[2,1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[array([[ 3.,  1.],\n",
        "        [ 1.,  3.]]), array([[ 9.,  1.],\n",
        "        [ 1.,  9.]])]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Default values and named arguments"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set default values in the function declaration\n",
      "from theano import Param\n",
      "x, y = T.dscalars('x', 'y')\n",
      "z = x + (y/2-12)\n",
      "f = function([Param(x, name='x'), Param(y, default=42, name='y')], z)\n",
      "print f(33)\n",
      "print f(33, 42)\n",
      "print(f(y=42, x=33))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "42.0\n",
        "42.0\n",
        "42.0\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Shared variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# used to make an accumulator for example, could be thought as a class variable in Java I think\n",
      "from theano import shared\n",
      "state = shared(0)\n",
      "inc = T.iscalar('inc') # integer scalar\n",
      "accumulator = function([inc], state, updates=[(state, state+inc)])\n",
      "\n",
      "accumulator(0)\n",
      "accumulator(200)\n",
      "state.get_value()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array(200)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random numbers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# random stream interface as in ssj\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from theano import function\n",
      "srng = RandomStreams()\n",
      "\n",
      "rv_u = srng.uniform((2,2))\n",
      "rv_n = srng.normal((2,2))\n",
      "f = function([], rv_u)\n",
      "g = function([], rv_n, no_default_updates=True) # no update to rv_n.rng\n",
      "nearly_zeros = function([], rv_u + rv_u - 2 * rv_u)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "rng = numpy.random\n",
      "\n",
      "N = 400\n",
      "feats = 784\n",
      "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
      "training_steps = 10000\n",
      "\n",
      "# declare theano symbolic variables\n",
      "# observations (x,y), it's supervised learning...\n",
      "x = T.matrix('x')\n",
      "y = T.vector('y')\n",
      "# weights and biases\n",
      "w = theano.shared(rng.randn(feats), name='w')\n",
      "b = theano.shared(0., name=\"b\")\n",
      "\n",
      "#print \"Initial model:\"\n",
      "#print w.get_value(), b.get_value()\n",
      "\n",
      "# construct theano expression graph (will have to dig deeper on the meaning of expresison graph)\n",
      "# could be simply an AST?\n",
      "# actually, it's the way mathematical expressions are symbolically represented internally...\n",
      "\n",
      "# Construct Theano expression graph\n",
      "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
      "prediction = p_1 > 0.5                    # The prediction thresholded\n",
      "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
      "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
      "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
      "                                          # (we shall return to this in a\n",
      "                                          # following section of this tutorial)\n",
      "\n",
      "# Compile\n",
      "train = theano.function(\n",
      "          inputs=[x,y],\n",
      "          outputs=[prediction, xent],\n",
      "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))\n",
      "predict = theano.function(inputs=[x], outputs=prediction)\n",
      "\n",
      "# Train\n",
      "for i in range(training_steps):\n",
      "    pred, err = train(D[0], D[1])\n",
      "\n",
      "#print \"Final model:\"\n",
      "#print w.get_value(), b.get_value()\n",
      "#print \"target values for D:\", D[1]\n",
      "#print \"prediction on D:\", predict(D[0])\n",
      "\n",
      "print sum(D[1] == predict(D[0]))\n",
      "print D[1] == predict(D[0])\n",
      "print D[1]\n",
      "print predict(D[0])\n",
      "print sum(D[1] == predict(D[0])) / len(D)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "400\n",
        "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True  True  True  True  True  True  True  True  True\n",
        "  True  True  True  True]\n",
        "[0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
        " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0\n",
        " 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1\n",
        " 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
        " 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1\n",
        " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 0\n",
        " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n",
        " 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
        " 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0\n",
        " 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
        " 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1]\n",
        "[0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
        " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0\n",
        " 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1\n",
        " 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
        " 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1\n",
        " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 0\n",
        " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n",
        " 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
        " 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0\n",
        " 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
        " 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1]\n",
        "200\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano import pp\n",
      "x = T.dscalar('x')\n",
      "y = x ** 2\n",
      "gy = T.grad(y, x)\n",
      "print pp(gy)\n",
      "\n",
      "f = function([x], gy)\n",
      "print f(4)\n",
      "print f(94.2)\n",
      "\n",
      "logistic  = T.sum(1 / (1 + T.exp(-x)))\n",
      "gs2 = T.grad(logistic, x)\n",
      "dlogistic = function([x], gs2)\n",
      "dlogistic(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))\n",
        "8.0\n",
        "188.4\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array(0.25)"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scan function, elementwise application"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "import theano.tensor as T\n",
      "import numpy as np\n",
      "\n",
      "# defining the tensor variables\n",
      "X = T.matrix('X')\n",
      "W = T.matrix('W')\n",
      "b_sym = T.vector('b_sym')\n",
      "\n",
      "results, updates = theano.scan(lambda v: T.tanh(T.dot(v, W) + b_sym), sequences=X)\n",
      "compute_elementwise = theano.function(inputs=[X, W, b_sym], outputs=[results])\n",
      "\n",
      "#test values\n",
      "x = np.eye(2, dtype=theano.config.floatX)\n",
      "w = np.ones((2, 2), dtype=theano.config.floatX)\n",
      "b = np.ones((2), dtype=theano.config.floatX)\n",
      "b[1] = 2\n",
      "\n",
      "print compute_elementwise(x, w, b)[0]\n",
      "print np.tanh(x.dot(w) + b)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.96402758  0.99505475]\n",
        " [ 0.96402758  0.99505475]]\n",
        "[[ 0.96402758  0.99505475]\n",
        " [ 0.96402758  0.99505475]]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/gab/Downloads/Theano-Theano-7fb9005/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
        "  from scan_perform.scan_perform import *\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from theano import pp, function\n",
      "x = T.dscalar('x')\n",
      "y = x ** 2\n",
      "gy = T.grad(y, x)\n",
      "print pp(gy)\n",
      "\n",
      "f = function([x], gy)\n",
      "print f(4)\n",
      "print f(94.2)\n",
      "\n",
      "softmax = T.nnet.softmax\n",
      "\n",
      "logistic  = T.sum(1 / (1 + T.exp(-x)))\n",
      "gs2 = T.grad(softmax, x)\n",
      "dlogistic = function([x], gs2)\n",
      "dlogistic(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))\n",
        "8.0\n",
        "188.4\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'Softmax' object has no attribute 'type'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-15862101e736>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlogistic\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgs2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdlogistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdlogistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/gab/Downloads/Theano-Theano-7fb9005/theano/gradient.pyc\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mknown_grads\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         raise ValueError(\"Can't differentiate a NaN cost.\"\n\u001b[0;32m    423\u001b[0m                          \u001b[1;34m\"cost is NaN because \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'Softmax' object has no attribute 'type'"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}